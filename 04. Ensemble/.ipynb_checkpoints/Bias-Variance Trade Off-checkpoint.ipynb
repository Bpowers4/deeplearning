{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irreducible Error\n",
    "- Data-generating processes are noisy\n",
    "- Noise is by definition random(not deterministic)\n",
    "- Can't predict its values, only its statistics(like mean & variance)\n",
    "- Example:\n",
    "- we are in charge of the data-generating process\n",
    "- f(x) = 2x +1 \n",
    "- linear regression\n",
    "- If a machine learning guy is working on our data, we can give him this function, his work is done\n",
    "- but linear regression model is: y = ax+b + $\\epsilon$\n",
    "- $\\epsilon \\sim N(0,\\sigma^2)$ \n",
    "- $\\hat{f}(x)$ = 2x + 1 doesn't achieve 0 error on y = 2x +1+ $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias \n",
    "- Bias here refers to the delta between your average model and the true f(x)\n",
    "- some sources refer to the square of this as bias, we won't\n",
    "$$\n",
    "bias = E[f(x) - \\hat{f}(x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "- Variance from statistics = how much a random variable deviates from its mean in squared units\n",
    "- Variance in the context of bias-variance trade-off is more specific\n",
    "- Variance = statistical variance of predictor over all possible training sets\n",
    "- Suppose we have a model that overfits- gets perfect for any training set\n",
    "- Then the models for each training set are probably very different from each other\n",
    "- Has nothing to do with accuracy\n",
    "- variance just measures how inconsistent a predictor is, over different training sets\n",
    "- remember: goal is not to achieve lowest possible error\n",
    "- goal is to find true f(x)\n",
    "- being close to training points is only a proxy solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Complexity\n",
    "- Variance is a proxy for model complexity\n",
    "- Complexity is a malleable term\n",
    "- can mean different things for different classifiers\n",
    "- ex. deep learning tree = complex, shallow decision tree = not complex\n",
    "- ex.K-nearest neighbor: K = 1complex, K =50 = not complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off\n",
    "- In ML we strive to minimize error\n",
    "- Overall error is a combination of\n",
    "    - Bias\n",
    "    - Variance\n",
    "    - Irreducible error\n",
    "- Goal is then to make bias and variance as small as possible\n",
    "- It's a tradeoff\n",
    "- we need to balance these\n",
    "- when we lower one, the other increases\n",
    "- Overfit: bias goes down, variance goes up\n",
    "- Underfit: bias goes up, variance goes down\n",
    "\n",
    "![](https://cn.bing.com/th?id=OIP.XRW2556DfOJz2EIz31RoOwHaFu&pid=Api&w=1056&h=816&rs=1)\n",
    "![](https://cn.bing.com/th?id=OIP.-pwwSpcPJcxzDaRH40w95QAAAA&pid=Api&rs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias -Variance Decomposition\n",
    "- Expected error = $bias^2$ + variance + irreducible error\n",
    "- Use mean-squared error for derivation for both regression and classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon\\\\\n",
    "\\epsilon \\sim N(0,\\sigma^2)\\\\\n",
    "\\hat{f(x)} = \\text{estimate of }f(x)\\\\\n",
    "err = E[(y-\\hat{f(x)})^2]\\\\\n",
    "\\bar{f(x)} = E[\\hat{f(x)}]\\\\\n",
    "= E[(f(x)+\\epsilon -\\hat{f(x)}+\\bar{f(x)}-\\bar{f(x)})^2]\\\\\n",
    "= E[\\epsilon^2]+E[\\epsilon(f(x)-\\bar{f(x)} - (\\hat{f(x)}-\\bar{f(x)}) )]\\\\\n",
    "E[\\epsilon] = 0\\\\\n",
    "E[\\epsilon^2] = \\sigma_{\\epsilon}^2+(E[\\epsilon])^2 = \\sigma_{\\epsilon}^2\\\\\n",
    "E[\\hat{f(x)} - \\bar{f(x)}] = E[\\hat{f(x)}] - E[\\hat{f(x)}] = 0\\\\\n",
    "= [f(x) - \\bar{f(x)}]^2 + E[(\\hat{f(x)} - \\bar{f(x)})^2]+E[\\epsilon^2]\\\\\n",
    "= bias^2 + variance + \\sigma_{\\epsilon}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- expected error is a combination of bias,variance, and irreducible error\n",
    "- this is not just the error between the true f(x) and f_hat(x)\n",
    "- we never observe f(x), we can only observe y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "NUM_DATASETS = 50\n",
    "NOISE_VARIANCE = 0.5\n",
    "MAX_POLY = 12\n",
    "N = 25\n",
    "Ntrain = int(0.9*N)\n",
    "\n",
    "np.random.seed(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# make a dataset with x^D, x^(D-1), ..., x^0\n",
    "def make_poly(x, D):\n",
    "    N = len(x)\n",
    "    X = np.empty((N, D+1))\n",
    "    for d in range(D+1):\n",
    "        X[:,d] = x**d\n",
    "        print(\"x[:,d] {}\".format(X[:,d].shape))\n",
    "        print(\"x {} X[:,d] {}\".format(x,X[:,d]))\n",
    "        if d > 1:\n",
    "            X[:,d] = (X[:,d] - X[:,d].mean()) / X[:,d].std()\n",
    "    return X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "X = np.linspace(-np.pi, np.pi, N)\n",
    "X\n",
    "Xpoly = make_poly(X, MAX_POLY)\n",
    "\n",
    "array([-3.14159265, -2.87979327, -2.61799388, -2.35619449, -2.0943951 ,\n",
    "       -1.83259571, -1.57079633, -1.30899694, -1.04719755, -0.78539816,\n",
    "       -0.52359878, -0.26179939,  0.        ,  0.26179939,  0.52359878,\n",
    "        0.78539816,  1.04719755,  1.30899694,  1.57079633,  1.83259571,\n",
    "        2.0943951 ,  2.35619449,  2.61799388,  2.87979327,  3.14159265])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Xpoly.shape\n",
    "(25,13)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def f(X):\n",
    "    return np.sin(X)\n",
    "\n",
    "\n",
    "x_axis = np.linspace(-np.pi, np.pi, 100)\n",
    "y_axis = f(x_axis)\n",
    "\n",
    "((100,), (100,))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "train_scores = np.zeros((NUM_DATASETS, MAX_POLY))\n",
    "test_scores = np.zeros((NUM_DATASETS, MAX_POLY))\n",
    "# squared_biases = np.zeros((NUM_DATASETS, MAX_POLY))\n",
    "# test_predictions = np.zeros((N - Ntrain, NUM_DATASETS, MAX_POLY))\n",
    "train_predictions = np.zeros((Ntrain, NUM_DATASETS, MAX_POLY))\n",
    "prediction_curves = np.zeros((100, NUM_DATASETS, MAX_POLY))\n",
    "\n",
    "train_scores.shape,train_predictions.shape\n",
    "((50, 12), (22, 50, 12))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "for k in range(NUM_DATASETS):\n",
    "  Y = f_X + np.random.randn(N)*NOISE_VARIANCE\n",
    "\n",
    "  Xtrain = Xpoly[:Ntrain]\n",
    "  Ytrain = Y[:Ntrain]\n",
    "\n",
    "  Xtest = Xpoly[Ntrain:]\n",
    "  Ytest = Y[Ntrain:]\n",
    "\n",
    "  for d in range(MAX_POLY):\n",
    "    model.fit(Xtrain[:,:d+2], Ytrain)\n",
    "    predictions = model.predict(Xpoly[:,:d+2])\n",
    "\n",
    "    # debug\n",
    "    x_axis_poly = make_poly(x_axis, d+1)\n",
    "    prediction_axis = model.predict(x_axis_poly)\n",
    "    # plt.plot(x_axis, prediction_axis)\n",
    "    # plt.show()\n",
    "\n",
    "    prediction_curves[:,k,d] = prediction_axis\n",
    "\n",
    "    train_prediction = predictions[:Ntrain]\n",
    "    test_prediction = predictions[Ntrain:]\n",
    "\n",
    "    train_predictions[:,k,d] = train_prediction # use this to calculate bias/variance later\n",
    "\n",
    "    train_score = mse(train_prediction, Ytrain)\n",
    "    test_score = mse(test_prediction, Ytest)\n",
    "\n",
    "    train_scores[k,d] = train_score\n",
    "    test_scores[k,d] = test_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= [f(x) - \\bar{f(x)}]^2 + E[(\\hat{f(x)} - \\bar{f(x)})^2]+E[\\epsilon^2]\\\\\n",
    "= bias^2 + variance + \\sigma_{\\epsilon}^2\n",
    "$$\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# calculate the squared bias\n",
    "avg_train_prediction = np.zeros((Ntrain, MAX_POLY))\n",
    "squared_bias = np.zeros(MAX_POLY)\n",
    "f_Xtrain = f_X[:Ntrain]\n",
    "for d in range(MAX_POLY):\n",
    "  for i in range(Ntrain):\n",
    "    avg_train_prediction[i,d] = train_predictions[i,:,d].mean()\n",
    "  squared_bias[d] = ((avg_train_prediction[:,d] - f_Xtrain)**2).mean()\n",
    "\n",
    "# calculate the variance\n",
    "variances = np.zeros((Ntrain, MAX_POLY))\n",
    "for d in range(MAX_POLY):\n",
    "  for i in range(Ntrain):\n",
    "    delta = train_predictions[i,:,d] - avg_train_prediction[i,d]\n",
    "    variances[i,d] = delta.dot(delta) / N\n",
    "variance = variances.mean(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
