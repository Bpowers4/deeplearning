{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "- AdaBoost, very powerful, one of the best off the shelf / plug and play algorithms\n",
    "- Different from bagging/random forests: we wanted low bias, high variance models\n",
    "- now we want high bias models\n",
    "- In AdaBoost nomenclature: we want weak learners\n",
    "- 50-60% accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hypothesis is that combining many weak learners can yield a string learner\n",
    "- as with stacking,  we want to weight each learner\n",
    "\n",
    "$$\n",
    "F(x) = \\sum_{m=1}^{M}\\alpha_{m}f_{m}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Learners\n",
    "- How do we get weak learners?\n",
    "- Decision Tree with max_depth = 1(decision stump)\n",
    "- This is only splits the space in half\n",
    "- Remarkable that a combination of these can yield a strong learner\n",
    "- another weak learner : logistic regression\n",
    "- advantage: weak learners train fast, can easily train 1000s of weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "- we will use targets{+1,-1}, not{0,1}\n",
    "- you'll see why later\n",
    "- we can modify existing algorithms like so\n",
    "\n",
    "$$\n",
    "\\text{Modified Logistic Output(x)} = 2LogisticOutput(x) -1\n",
    "$$\n",
    "\n",
    "- Decision boundary is 0\n",
    "- Usually use $\\alpha$ as model weights\n",
    "\n",
    "$$\n",
    "F_{M}(x) = sign(\\sum_{m=1}^{M}\\alpha_{m}f_{m}(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://mblogthumb-phinf.pstatic.net/20160330_250/dic1224_1459323262712kPNVH_JPEG/boosting.png?type=w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://mblogthumb-phinf.pstatic.net/20160330_221/dic1224_1459323510463sJSv7_JPEG/adaboost1.png?type=w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "- add one base model at a time(called additive modeling)\n",
    "- Train base model on all data(no resampling/bootstrapping)\n",
    "- Instead, we'll weight how important each sample is with w[i] , i = 1...N\n",
    "- Modify w[i] on each round\n",
    "- if we get(x[i],y[i]) wrong, then increase w[i], else decrease w[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to modify base model so it takes into account the weights of each sample\n",
    "- for logistic regression\n",
    "\n",
    "$$\n",
    "J_{WEIGHTED} = \\sum_{i=1}^{N}w_{i}[t_{i}log(y_{i}) + (1-t_{i})log(1-y_{i})]\n",
    "$$\n",
    "\n",
    "- for decision tree, SKlearn gives us an API to pass in sample weights,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train base model on all data(X,Y) with weights w[i] = 1,...N\n",
    "- rest of the algorithm\n",
    "- calculate error weighted by w[i]\n",
    "- calculate $\\alpha_{m}$ as a function of error\n",
    "    - more accurate(less error) -> $\\alpha_{m}$ should be bigger\n",
    "- store $\\alpha_{m}$ and $f_{m}$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "\n",
    "```python\n",
    "Initialize w[i] = 1/N for i = 1...N #uniform distribution\n",
    "for m = 1...M:\n",
    "    Fit f_m(x) with sample weights w[i]\n",
    "```   \n",
    "$$\n",
    "\\epsilon_{m} = \\frac{\\sum_{i=1}^{N}w_{i}I(y_{i}\\neq f_{m}(x))}{\\sum_{i=1}^{N}w_{i}}\\\\\n",
    "\\alpha_{m} = \\frac{1}{2}log[\\frac{1-\\epsilon_{m}}{\\epsilon_{m}}]\\\\\n",
    "w_{i} = w_{i}exp[-\\alpha_{m}y_{i}f_{m}(x_{i})],i=1,...N\\\\\n",
    "w_{i} = w_{i}/\\sum_{j=1}^{N}w_{i} \\text{# normalize w}\\\\\n",
    "$$\n",
    "\n",
    "- save $\\alpha_{m},f_{m}(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- very specific to binary classification, requiring labels{-1,+1}\n",
    "- can be extended to multiclass classification and regression\n",
    "- use scikit learn\n",
    "- our goal is to capture the main idea\n",
    "- like random forest,authors recommended treee as base model\n",
    "- we will see linear classifier as base model again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive modeling\n",
    "\n",
    "- forward stagewise additive modeling\n",
    "- adaboost is an instance of this general algorithm\n",
    "\n",
    "- L(y,f(x)) is the loss/cost function given target y and model f(x)\n",
    "- F = full model, f = base model\n",
    "\n",
    "$$\n",
    "\\text{Initialize }F_{0}(x) = 0\\\\\n",
    "\\text{for m = 1,...M:}\\\\\n",
    "\\text{    }(\\alpha_{m}^*,\\theta_{m}^*)= argmin_{\\alpha_{m},\\theta_{m}}\\sum_{i=1}^{N}L(y_{i},F_{m-1}(x_{i})+\\alpha_{m}f_{m}(x_{i};\\theta_{m}))\\\\\n",
    "F_{m}(x) = F_{m-1}(x) + \\alpha_{m}^*f_{m}(x;\\theta_{m}^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Loss Function: exponential Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "- Binary Cross Entropy (binary classification)\n",
    "- Multiclass cross entropy (multiclass classification)\n",
    "- Squeared error(regression)\n",
    "- Absolute error(repression)\n",
    "- Accuracy(perceptron)\n",
    "- AdaBoost-> Exponential Loss\n",
    "- {+1,-1} doesn't make sense for cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Loss\n",
    "\n",
    "$$\n",
    "L(y,f(x)) = exp(-yf(x))\n",
    "$$\n",
    "\n",
    "- when y & f(x) same sign -> 0\n",
    "- when y & f(x) opposite sign -> inf\n",
    "- has the same asymptotic effect as cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\text{Initialize }F_{0}(x) = 0\\\\\n",
    "\\text{for m = 1,...M:}\\\\\n",
    "\\text{    }(\\alpha_{m}^*,\\theta_{m}^*)= argmin_{\\alpha_{m},\\theta_{m}}\\sum_{i=1}^{N}L(y_{i},F_{m-1}(x_{i})+\\alpha_{m}f_{m}(x_{i};\\theta_{m}))\\\\\n",
    "F_{m}(x) = F_{m-1}(x) + \\alpha_{m}^*f_{m}(x;\\theta_{m}^*)\n",
    "$$\n",
    "\n",
    "- replace L \n",
    "\n",
    "$$\n",
    "\\text{    }(\\alpha_{m}^*,f_{m}^*)= argmin_{\\alpha_{m},f_{m}}\\sum_{i=1}^{N}exp\\big\\{ -y_{i}[F_{m-1}(x_{i})+ \\alpha_{m}f_{m}(x_{i})] \\big\\}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expand exponential\n",
    "\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{N}exp\\big\\{ -y_{i}[F_{m-1}(x_{i})+ \\alpha_{m}f_{m}(x_{i})] \\big\\}\\\\\n",
    "J = \\sum_{i=1}^{N}exp\\big\\{ -y_{i}F_{m-1}(x_{i}) \\big\\} exp\\big\\{ -y_{i}\\alpha_{m}f_{m}(x_{i}) \\big\\}\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the first exponential is just w\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{N}w_{i}^{(m)} exp\\big\\{ -y_{i}\\alpha_{m}f_{m}(x_{i}) \\big\\}\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- returning to the cose\n",
    "\n",
    "- y $\\cdot$f will always be +/-1\n",
    "- +1 if correct, -1 if incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split into sum of corrects, sum of incorrects\n",
    "\n",
    "$$\n",
    "J = e^{-\\alpha_{m}}\\sum_{y_{i}=f_{m}(x)}^{}w_{i}^{(m)}+e^{\\alpha_{m}}\\sum_{y_{i}\\neq f_{m}(x)}^{}w_{i}^{(m)}\n",
    "$$\n",
    "\n",
    "- simplify these symbols,drop m, first sum = A, second sum = B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- substitute:\n",
    "\n",
    "$$\n",
    "J = e^{-\\alpha}A + e^{\\alpha}B\n",
    "$$\n",
    "\n",
    "- differentiate and set to 0, solve for $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial \\alpha} = - e^{-\\alpha}A + e^{\\alpha}B = 0\\\\\n",
    "\\alpha_{m} = \\frac{1}{2}ln(\\frac{A}{B}) = \\frac{1}{2}ln(\\frac{\\text{number of weighted correct}}{\\text{number of weighted incorrect}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A and B represented as error rates(between 0 and 1) in original algorithm,\n",
    "- but they are both divided by sum of w's on top and bottom, thus canceling out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Additive modeling doesn't specify loss function/other details\n",
    "- AdaBoost does, so we replace L with exponential loss\n",
    "- manipulate loss to recover w[i]\n",
    "- optimize loss wrt alpha to recover alpha = func(weighted err)\n",
    "- last step(derivative of J wrt alpha =0,solve alpha) is the most intellectually satisfying because it fits w/in the framwork of what we do w /logistic regression, neural network, perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://postfiles.pstatic.net/MjAxNzA0MTJfNjYg/MDAxNDkxOTg3MzE2NTEx.gAwXXAUqr0NHz1nqNPipkIFF0-ZY2C2GyQXqFUM3E04g.5Twv-UUVGoWvw_uOrOh3o817bgsKxxZ0FcAFcMBUB4wg.JPEG.dic1224/adaboost1.png?type=w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://postfiles.pstatic.net/MjAxNzA0MTJfMjA5/MDAxNDkxOTg3MzgwODM2.PAsV4jR9iEIGnv3gpXVGTFUz9N7-Vco-0EoLgh68YmQg.nGqvr0KN3PXAbfTUT-WNsduWr-l33ohGlMi9G_NZqnMg.JPEG.dic1224/adaboost2.png?type=w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://postfiles.pstatic.net/MjAxNzA0MjFfMjgz/MDAxNDkyNzc3NTQwNzM0.NMl-Qt7vwOb-_0OdAjmaC7uBV0VJPsuXz3W243ZUjVUg.f5_1dsr-4uYhhuc-S1dfYWKiWwapKXrtkwqDeBT-QpAg.JPEG.dic1224/adaboost_flowchart.jpg?type=w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[adaboost blog](https://m.blog.naver.com/dic1224/220669575477)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./data/\") \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from rf_classification import get_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaBoost:\n",
    "  def __init__(self, M):\n",
    "    self.M = M\n",
    "\n",
    "  def fit(self, X, Y):\n",
    "    self.models = []\n",
    "    self.alphas = []\n",
    "\n",
    "    N, _ = X.shape\n",
    "    W = np.ones(N) / N\n",
    "\n",
    "    for m in range(self.M):\n",
    "      tree = DecisionTreeClassifier(max_depth=1)\n",
    "      tree.fit(X, Y, sample_weight=W)\n",
    "      P = tree.predict(X)\n",
    "\n",
    "      err = W.dot(P != Y)\n",
    "      alpha = 0.5*(np.log(1 - err) - np.log(err))\n",
    "\n",
    "      W = W*np.exp(-alpha*Y*P) # vectorized form\n",
    "      W = W / W.sum() # normalize so it sums to 1\n",
    "\n",
    "      self.models.append(tree)\n",
    "      self.alphas.append(alpha)\n",
    "\n",
    "  def predict(self, X):\n",
    "    # NOT like SKLearn API\n",
    "    # we want accuracy and exponential loss for plotting purposes\n",
    "    N, _ = X.shape\n",
    "    FX = np.zeros(N)\n",
    "    for alpha, tree in zip(self.alphas, self.models):\n",
    "      FX += alpha*tree.predict(X)\n",
    "    return np.sign(FX), FX\n",
    "\n",
    "  def score(self, X, Y):\n",
    "    # NOT like SKLearn API\n",
    "    # we want accuracy and exponential loss for plotting purposes\n",
    "    P, FX = self.predict(X)\n",
    "    L = np.exp(-Y*FX).mean()\n",
    "    return np.mean(P == Y), L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensionality: 139\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "final train error: 0.0\n",
      "final test error: 0.0\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_data()\n",
    "Y[Y == 0] = -1 # make the targets -1,+1\n",
    "Ntrain = int(0.8*len(X))\n",
    "Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "\n",
    "T = 200\n",
    "train_errors = np.empty(T)\n",
    "test_losses = np.empty(T)\n",
    "test_errors = np.empty(T)\n",
    "for num_trees in range(T):\n",
    "    if num_trees == 0:\n",
    "      train_errors[num_trees] = None\n",
    "      test_errors[num_trees] = None\n",
    "      test_losses[num_trees] = None\n",
    "      continue\n",
    "    if num_trees % 20 == 0:\n",
    "      print(num_trees)\n",
    "\n",
    "    model = AdaBoost(num_trees)\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    acc, loss = model.score(Xtest, Ytest)\n",
    "    acc_train, _ = model.score(Xtrain, Ytrain)\n",
    "    train_errors[num_trees] = 1 - acc_train\n",
    "    test_errors[num_trees] = 1 - acc\n",
    "    test_losses[num_trees] = loss\n",
    "\n",
    "    if num_trees == T - 1:\n",
    "      print(\"final train error:\", 1 - acc_train)\n",
    "      print(\"final test error:\", 1 - acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
