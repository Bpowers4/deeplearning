{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore vs Exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayesian methoda are beneficial\n",
    "- discuss disadvantages of traditional A/B testing\n",
    "- extreme example: drug is working well, can you stop test and improve quality of life of all participants?\n",
    "- frequentist says no\n",
    "- Bad to stop early. why?\n",
    "- increase chance of FPs(false positive)\n",
    "- p-value can pass below/above threshold over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit\n",
    "- Not all slot machine are equal,1 pays 30% of the time, 1 pays 20% of the time, 1 pays 10% of the time\n",
    "- Same problem as click-through rate or conversion rate\n",
    "- click = get a prize\n",
    "- Traditionality: do A/B test for a specific number N trials, then calculate p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would you do?\n",
    "- One arm gives you a prize 3/3 times, other gives you a prize 0/3 times\n",
    "- you'll intuitively just play the first arm\n",
    "- why? you ADAPT\n",
    "- 3 plays probably not enough to attain significance, you still feel compelled to believe the 1st arm is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "- faces the same problem\n",
    "- teach a machine to play a game\n",
    "- it models rewards it gets based on actions it takes\n",
    "- process could be stochastic\n",
    "- reward estimates will be approximate\n",
    "- early on: few actions taken,unsure about most rewards\n",
    "- can't choose the action that leads to best reward, because currently knowledge about rewards is minimal\n",
    "- only after collecting a lot of data will estimate be accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore/Exploit\n",
    "- This is called the explore-exploit dilemma\n",
    "- If I get 3/3 from bandit 1, 0/3 from bandit2, should I?\n",
    "- Exploit bandit 1 more? or\n",
    "- Explore at random to gather more data?\n",
    "- we'll look at several solutions\n",
    "- all are adaptive, not all are Bayesian\n",
    "- we'll show Bayesian last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[bayesian-ab-testing](https://zlatankr.github.io/posts/2017/04/07/bayesian-ab-testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
