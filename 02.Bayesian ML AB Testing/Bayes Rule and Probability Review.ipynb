{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Bayes Rule Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule & Probability Review\n",
    "- You 've probably already seen below formula\n",
    "\n",
    "$$p(A|B) = \\frac{p(A,B)}{p(B)}$$\n",
    "\n",
    "- $p(A|B)$ - conditional\n",
    "- $p(A,B)$ - joint\n",
    "- $p(B)$ - marginal \n",
    "- This is the most basic form, but too abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Example\n",
    "\n",
    "- A = {Buy, does not buy} ,B = {USA,Candada,Mexico}\n",
    "- Suppose we want to find p(Buy?|Country)\n",
    "\n",
    "[]()|CA|US|MX|\n",
    "----|---|---|---|\n",
    "Buy = 1(did buy)|20|50|10|\n",
    "Buy = 0(did not buy)|300|500|200|\n",
    "\n",
    "- Marginal probability\n",
    "- p(country=Mexico) = 210/(210+550+320) = 0.19\n",
    "- p(country=US) = 550/(210+550+320) = 0.51\n",
    "- p(country=CA) = 320/(210+550+320) = 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Probabilities\n",
    "- Now let's find p(Buy?,Country)\n",
    "- How many possibilities?\n",
    "- Buy?= 2 possibilities\n",
    "- Country = 3 possibilities\n",
    "- Total possibilities = 2 x 3 = 6\n",
    "- Think of it as an area or volume\n",
    "- In general, total = |RV1| x|RV2|x |RV3| x ... x|RVn|\n",
    "- Grows exponentially as we add more variables\n",
    "- Curse of dimensionality\n",
    "- is a bad thing, because as the volume grows\n",
    "- we need to do more computation\n",
    "- need more samples to get accurate estimates\n",
    "- p(Buy = 1,CA) = 20/1080(210+550+320) = 0.019\n",
    "- p(Buy = 0,CA) = 300/1080(210+550+320) = 0.28\n",
    "- p(Buy = 1,US) = 50/1080(210+550+320) = 0.046\n",
    "- p(Buy = 0,US) = 500/1080(210+550+320) = 0.46\n",
    "- p(Buy = 1,MX) = 10/1080(210+550+320) = 0.0093\n",
    "- p(Buy = 0,MX) = 200/1080(210+550+320) = 0.19\n",
    "\n",
    "- These seem a lot smaller than the marginal \n",
    "- Sum of all possible outcomes must = 1\n",
    "- If number of total possibilities grows exponentially,actual probability values will shrink exponentially\n",
    "- Another consequence of curse of dimensionality\n",
    "- Computers have finite precision-32bit float holds 32bits of info\n",
    "- can't store infinite number of values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underflow\n",
    "- As probability -> 0, eventually computer will round down to 0\n",
    "- called the underflow problem\n",
    "- common in probability \n",
    "- log grows slowly as its argument increases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probabilities\n",
    "\n",
    "- p(Buy = 1|CA) = 0.019/0.30 = 0.06\n",
    "- p(Buy = 0|CA) = 0.28/0.30 = 0.93\n",
    "- p(Buy = 1|US) = 0.046/0.51 = 0.09\n",
    "- p(Buy = 0|US) = 0.46/0.51 = 0.91\n",
    "- p(Buy = 1|MX) = 0.009/0.19 = 0.05\n",
    "- p(Buy = 0|MX) = 0.185/0.19 = 0.97\n",
    "\n",
    "note: some roundoff error\n",
    "- No longer sums to 1, sums to 3 why?\n",
    "- we are given country- the space of random variables is only buy/not buy\n",
    "- country is not random here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[]()|CA|US|MX|\n",
    "----|---|---|---|\n",
    "Buy = 1(did buy)|20|50|10|\n",
    "Buy = 0(did not buy)|300|500|200|\n",
    "\n",
    "- p(Buy = 1| Country = US) = p(Buy = 1,Country = US)/p(country = US)\n",
    "- = (50/1080)/[(50+500)/1080]\n",
    "- = 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar but different problem\n",
    "\n",
    "[]()|CA|US|MX|\n",
    "----|---|---|---|\n",
    "Buy = 1(did buy)|20|50|10|\n",
    "Buy = 0(did not buy)|200|500|100|\n",
    "\n",
    "- p(Buy=1|Country=CA) = 0.1\n",
    "- p(Buy=1|Country=US) = 0.1\n",
    "- p(Buy=1|Country=MX) = 0.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence\n",
    "- When 2 variables are independent, the joint becomes the multiple of the marginals, e.g. if A &B are independent:\n",
    "- p(A,B) = p(A)p(B)\n",
    "- so, if Buy & Country are independent\n",
    "- p(Buy|Country) = p(Buy,Country)/p(Country) = p(Buy)p(Country)/p(Country) = p(Buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Bayes Rule\n",
    "- let's make it look more like the form we'll use in \n",
    "$$\n",
    "p(A|B) = \\frac{p(A,B)}{p(B)}\n",
    "$$\n",
    "\n",
    "- The opposite is also true:\n",
    "$$\n",
    "p(B|A) = \\frac{p(B,A)}{p(A)}\n",
    "$$\n",
    "- Since p(A,B) = p(B,A)\n",
    "$$\n",
    "p(A|B) = \\frac{p(B|A)p(A)}{p(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(B) = \\sum_{A}^{}{p(A,B)} = \\sum_{A}^{}\\frac{p(B|A)}{p(A)}\n",
    "$$\n",
    "\n",
    "- If working with continuous distributions, sum turns into integral\n",
    "\n",
    "$$\n",
    "p(A|B) = \\frac{p(B|A)p(A)}{\\int{p(B|A)p(A)dA}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can also think of the bottom term as a normalization constant so that the distribution sums to 1\n",
    "\n",
    "\n",
    "$$\n",
    "p(A|B) \\propto p(B|A)p(A)\n",
    "$$\n",
    "\n",
    "- Many times, we just want the $\\underset{A}{\\operatorname{argmax}}p(A|B)$\n",
    "$$\n",
    "\\underset{A}{\\operatorname{argmax}}p(A|B) = \\underset{A}{\\operatorname{argmax}}p(B|A)p(A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes for classification\n",
    "$$\n",
    "p(y|x) = \\frac{p(x|y)p(y)}{p(x)}\n",
    "$$\n",
    "\n",
    "- p(x|y) is a generative distribution - it tells us what does x look like? given the class is y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Probability Execise\n",
    "\n",
    "- We have a fair coin: p(H) = p(T)= 0.5, H= head, T= Tail\n",
    "- we plan to toss the coin 200 times in total\n",
    "- After 20 tosses, we have 15H, 5T\n",
    "- What is the total # of heads we expect to get by the end of the experiment?\n",
    "(N = 200)\n",
    "\n",
    "## Gambler's Fallacy\n",
    "- You just lost 100 times, you must have a better chance of winning next?\n",
    "- Incorrect\n",
    "- Doesn't matter how many times you have lost already, your chance of losing next are the same as they have always been\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Monty Hall Problem\n",
    "\n",
    "- Famous problem in probability, inspired by a TV game show\n",
    "- TV show was Let's make a deal, host was monty hall, hence the monty hall problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the game works\n",
    "- you pick a door(door 1)\n",
    "- monty hall opens a door you didn't pick, reveals a goat (door 2)\n",
    "- you are given a choice: stay with door 1 or switch to door 3\n",
    "- This question might seem silly, it doesn't matter which door you choose,\n",
    "- the probability of each is 1/3 right?\n",
    "\n",
    "[answer](https://cba.snu.ac.kr/ko/sblcolumn?mode=view&bbsidx=77838)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Maximum likelihood - Mean of Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximum likelihood - what does that mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Gaussian sample\n",
    "- Suppose we have collected one data point from a source of Gaussian distributed data,call it x\n",
    "- What is the probability density of that one data point?\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple sample\n",
    "- In a real experiment, we will collect multiple samples\n",
    "- Typically these samples are IID- Independent and identically distributed\n",
    "- Identically distributed - they are all Gaussian with the same mean/variance\n",
    "- Independent\n",
    "    - If I flip coin and I've gotten 10 heads so far, what's the next toss likely to be?\n",
    "    - p(T|HHHHHHHHHH) = p(T) = 0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint probability density\n",
    "- This independence means I can multiply the probability of each individual sample to get the joint probability of all the samples\n",
    "\n",
    "$$\n",
    "p(x_{1},x_{2},...,x_{N}) =\\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp(-\\frac{1}{2}\\frac{(x_{i}-\\mu)^2}{\\sigma^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data likelihood\n",
    "\n",
    "- we want to phrase it as probability of data given the parameter\n",
    "- p(data|parameter)\n",
    "- Parameters depend on what the model is. e.g. Gaussian, Beta, Gammma, etc\n",
    "- Gaussian has mean/variance , we will focus on mean\n",
    "\n",
    "$$\n",
    "p(x_{1},x_{2},...,x_{N}|\\mu) =\\prod_{i=1}^{N} p(x_{i}|\\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood \n",
    "- How we have likelihood, how do we maxmize it?\n",
    "- what is the best setting of $\\mu$, such that the likelihood is maximized?\n",
    "- When we want to maximize a function wrt a variable, calculus provides the tools we need\n",
    "- Taking the log is useful(log-likelihood)\n",
    "- Why? Gaussian has an exponential,derivative of exponential is the same thing, so it won't be easy to solve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of the exponential\n",
    "- log(exp(A)) = A\n",
    "- This is ok because log() is a monotonically increasing function\n",
    "- if $\\mu^{*}$ yields maximum P,then it also yields maximum log(P)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood\n",
    "\n",
    "$$\n",
    "L = logp(X|\\mu) = log \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp(-\\frac{1}{2}\\frac{(x_{i}-\\mu)^2}{\\sigma^2})\\\\\n",
    "= \\prod_{i=1}^{N} log \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp(-\\frac{1}{2}\\frac{(x_{i}-\\mu)^2}{\\sigma^2}) \\\\\n",
    "= \\prod_{i=1}^{N} -\\frac{(x_{i}-\\mu)^2}{2\\sigma^2} + const\\;wrt\\; \\mu\n",
    "$$\n",
    "\n",
    "## Derivative of log-likelihood\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\sum_{i=1}^{N}\\frac{x_{i}-\\mu}{\\sigma^2} = 0 \\\\\n",
    "\\mu = \\frac{1}{N}\\sum_{i=1}^{N}x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Maximum likelihood - Click-Through Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CTR = click through rate, also conversion rate\n",
    "- Common measure in e-commernce,online advertising, clickbait news sites\n",
    "- Not a Gaussian distribution, more like a coin toss(2 possible outcomes)\n",
    "- Buy/don't buy, click/dont' click\n",
    "- we will use CTR even though all the methods could be applied to conversion rate\n",
    "- They are all just the Bernoulli distribution(first seen in my logistic regression class- cross entropy cost function is log of Bernoulli likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setup\n",
    "- H = click, T = No click, H+T = total number impressions\n",
    "- also IID\n",
    "- Let's call p(H) = p, so p(T) = 1-p\n",
    "- Bernoulli only has 1 parameter(Gaussian has 2)\n",
    "- Suppose we flip 2 H, 3 T - what is the total likelihood?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "- L(2H,3T) = $p^2(1-p)^3$= p(H)p(H)p(T)p(T)p(T)\n",
    "- $L(N_{H},N_{T}) = p^{N_{H}}(1-p)^{N_{T}}$\n",
    "- what's the maximum likelihood estimate of p?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood\n",
    "$$\n",
    "L = log(p^{N_{H}}(1-p)^{N_{T}}) = N_{H}logp + N_{T}log(1-p)\\\\\n",
    "\\frac{\\partial L}{\\partial p} = \\frac{N_{H}}{p} - \\frac{N_{T}}{1-p} = 0\\\\\n",
    "p = \\frac{N_{H}}{N_{H}+N_{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Result\n",
    "- suppose let $x_{i} = 1$ be heads, $x_{i}$=0 be tails\n",
    "$$\n",
    "p=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}\n",
    "$$\n",
    "- Exact same result as the Gaussian (p is the mean of the Bernoulli distribution)\n",
    "\n",
    "- Gaussian mean log likelihood\n",
    "$$\n",
    "\\mu = \\frac{1}{N}\\sum_{i=1}^{N}x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "- Same problem we had before- we don't know how precise this measurement is\n",
    "- Intuitively, we konw 1/10 is not as accurate as 10/100, which is not as accurate as 100/1000\n",
    "- They all give the same p\n",
    "- Next, we'll look at the frequentist way of dealing with this, and later the Bayesian way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-Bayesian/ Frequentist methods of dealing with uncertainty of measurement of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Random Variables\n",
    "- We will use a hat to denote an estimate paramter, no hot to be the true parameter\n",
    "- Is a sum of random variables\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N}x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X is random\n",
    "- Y is random\n",
    "- Z = X + Y\n",
    "- Is Z also random? Yes\n",
    "- It has a probability distribution, a mean, a variance\n",
    "\n",
    "- The trick here is that our estimate of the mean is also a random variable\n",
    "- states that sum of IID random variables tends to a Gaussian distribution\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} \\sim N(\\mu,\\frac{\\sigma^2}{N})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of estimate\n",
    "- As we collect more samples(N), variance decreases\n",
    "- $\\mu$ and $\\sigma$ refer to mean/std dev of X\n",
    "- $\\mu$ estimate should have the same mean of X\n",
    "- more variance in X should lead to more variance in $\\mu$ estimate\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} \\sim N(\\mu,\\frac{\\sigma^2}{N})\\\\\n",
    "E(X) = \\mu, var(X) = \\sigma^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale difference\n",
    "- variance grows proportionally to variance of X\n",
    "- only decreases by square root of N\n",
    "- therefore, need to collect many more samples to account for larger variance\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} \\sim N(\\mu,\\frac{\\sigma^2}{N})\\\\\n",
    "var(\\hat{\\mu}) \\propto \\sigma^2 . var(\\hat{\\mu}) \\propto 1/N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalent formulation\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} - \\mu \\sim N(0,\\frac{\\sigma^2}{N})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "- we want to know the range of values that are likely to contain true $\\mu$\n",
    "- shade in 95% of middle of Gaussian, we can say $\\mu$ is probably here\n",
    "- Note: 95% CI(confidence Interval) doesn't tell us $\\mu$ is in this interval with probability 95%\n",
    "- In reality, all we can say is if we did many experiments to calculate the sample mean, 95% of the time, those confidence intervals would contain the true $\\mu$\n",
    "\n",
    "![](https://cn.bing.com/th?id=OIP.Mmp-41G15YALhVs1_OGisQHaFK&pid=Api&rs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence leval / Significance Level\n",
    "- we call the confidence level 1 - $\\alpha$\n",
    "- we call the significent level $\\alpha$\n",
    "- we'll see significance level again later with statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence interval limits\n",
    "- we want the min/max value for the range where $\\mu$ should lie\n",
    "- let's call them $x_{left},x_{right}$\n",
    "- we want to find the limits such that the area under the Gaussian is 0.95\n",
    "- calculus provides the tools - integral\n",
    "\n",
    "$$\n",
    "0.95 = 1- \\alpha = \\int_{x_{left}}^{x_{right}}N(x;0,\\frac{\\sigma^2}{N})dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardize the normal and rescale\n",
    "- New limits $z_{left}$ and $z_{right}$\n",
    "\n",
    "$$\n",
    "0.95 = 1- \\alpha = \\int_{z_{left}}^{z_{right}}N(z;0,1)dx\\\\\n",
    "z = \\frac{(x - \\mu)}{(\\sigma/\\sqrt{N})}\n",
    "$$\n",
    "\n",
    "- we're pretending $\\mu$  = 0 and we'll shift it back later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Function(CDF)\n",
    "- can we make use of this?\n",
    "\n",
    "$$\n",
    "P(X \\le x) = \\int_{-\\infty}^{x}p(x)dx\n",
    "$$\n",
    "\n",
    "- Guassian is symmetric\n",
    "- so if we want 5% on the tail ends, then we want each tail to be 2.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In other words, $z_{right}$ should give us an area of 1 - 0.05/2 = 0.975\n",
    "\n",
    "$$\n",
    "0.975 = \\int_{-\\infty}^{z_{right}}N(z;0,1)dz = \\Phi(z_{right})\\\\\n",
    "z_{right} = \\Phi^{-1}(0.975)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse CDF\n",
    "- Scipy has a function to do this\n",
    "- scipy.stats.norm.pdf\n",
    "- ppf = percent point function, because statisticians like crazy names\n",
    "- since Gaussian is symmetric\n",
    "\n",
    "$$\n",
    "z_{left} = -z_{right}\\\\\n",
    "z_{left} = \\Phi^{-1}(0.025)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z_{left} = z_{\\alpha/2}\\\\\n",
    "z_{right} = z_{1-\\alpha/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval\n",
    "\n",
    "$$\n",
    "[\\hat{\\mu}+z_{left}\\frac{\\sigma}{\\sqrt{N}},\\hat{\\mu}+z_{right}\\frac{\\sigma}{\\sqrt{N}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we don't know $\\sigma$\n",
    "- But this is a valid approximation\n",
    "\n",
    "$$\n",
    "[\\hat{\\mu}+z_{left}\\frac{\\hat{\\sigma}}{\\sqrt{N}},\\hat{\\mu}+z_{right}\\frac{\\hat{\\sigma}}{\\sqrt{N}}]\\\\\n",
    "\\hat{\\sigma} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_{i}-\\hat{\\mu})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval Approximation\n",
    "\n",
    "95%CI = \n",
    "$$\n",
    " \\approx [\\hat{\\mu}+z_{left}\\frac{\\hat{\\sigma}}{\\sqrt{N}},\\hat{\\mu}+z_{right}\\frac{\\hat{\\sigma}}{\\sqrt{N}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Confidence Interval\n",
    "- replaced the Gaussian symbols with Bernoulli symbols\n",
    "\n",
    "var(X) = p(1-p)  \n",
    "95%CI =   \n",
    "$$\n",
    "\\approx [\\hat{p}+z_{left}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}},\\hat{p}+z_{right}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Bayesian Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CLT(Central Limit Theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[beta-distribution](https://www.statlect.com/probability-distributions/beta-distribution)  \n",
    "[probability theory](http://sanghyukchun.github.io/58/)  \n",
    "[beta distribution calculator](https://keisan.casio.com/exec/system/1180573226)  \n",
    "[beta_distribution_and_baseball](http://varianceexplained.org/statistics/beta_distribution_and_baseball/)  \n",
    "[binomial-distribution](https://www.mathsisfun.com/data/binomial-distribution.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
