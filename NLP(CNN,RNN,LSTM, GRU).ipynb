{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D Convolution\n",
    "- one obvious application: speech recongnition\n",
    "- speech is a 1-D signal\n",
    "- automatic text trasciption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another application - treating a sequence of word vectors as a 1-D signal\n",
    "\n",
    "- length T sentences and size D vector -> TxD 1-dimensional vector signal  \n",
    "![](https://cn.bing.com/th?id=OIP.2Z4scweLbrmB4QZ5AXwJfwHaHY&pid=Api&rs=1&p=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D CNN word embedding Archtecture\n",
    "![](https://cn.bing.com/th?id=OIP.0LXL9feRDVvbjxVOWiMGggHaC_&pid=Api&rs=1&p=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "[kaggle toxic data](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"  \n",
    "\"0000997932d777bf\",\"Explanation\n",
    "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",0,0,0,0,0,0\n",
    "\"000103f0d9cfb60f\",\"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\"  \n",
    ",0,0,0,0,0,0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi-label: a picture of a \"car\" can also be a picture of a \"red object\"\n",
    "- similarly, these comments may have multiple labels, it is both a threat and toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label\n",
    "- treat it like 6 different binary classification problems\n",
    "  - given comment -> is it toxic or not toxic?\n",
    "  - given comment -> is it a threat or not a threat?\n",
    "- It is like having a neural network with 6 seperate binary logistic regression at the end\n",
    "- __Total loss is just the average binary cross-entropy__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "- neural network -> 6 X linear classifier\n",
    "- features = feature_extractor.transform(input_data)\n",
    "- model = LogisticRegression()\n",
    "- model.fit(features from csv,labels)\n",
    "- do this 6 times for each of the 6 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In keras\n",
    "- x = output from previous layers\n",
    "- outputs = Dense(6,activation=\"sigmodi\")(x)\n",
    "- model = Model(inputs,outputs)\n",
    "- model.compile('binary_cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 1000#20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "word the vec [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ] len:100\n",
      "word , vec [-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n",
      "  0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n",
      "  0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n",
      " -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n",
      "  0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n",
      " -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n",
      "  0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n",
      "  0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n",
      " -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n",
      " -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n",
      " -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n",
      "  0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n",
      " -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n",
      " -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n",
      " -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n",
      "  0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n",
      " -0.35111   -0.83155    0.45293    0.082577 ] len:100\n",
      "word . vec [-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n",
      "  0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n",
      "  0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n",
      " -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n",
      "  0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n",
      " -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n",
      "  0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n",
      " -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n",
      " -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n",
      " -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n",
      " -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n",
      "  0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n",
      "  0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n",
      " -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n",
      " -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n",
      " -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n",
      " -0.39242   -0.23394    0.47298   -0.028803 ] len:100\n",
      "word of vec [-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ] len:100\n",
      "word to vec [-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01] len:100\n",
      "word and vec [-0.071953  0.23127   0.023731 -0.50638   0.33923   0.1959   -0.32943\n",
      "  0.18364  -0.18057   0.28963   0.20448  -0.5496    0.27399   0.58327\n",
      "  0.20468  -0.49228   0.19974  -0.070237 -0.88049   0.29485   0.14071\n",
      " -0.1009    0.99449   0.36973   0.44554   0.28998  -0.1376   -0.56365\n",
      " -0.029365 -0.4122   -0.25269   0.63181  -0.44767   0.24363  -0.10813\n",
      "  0.25164   0.46967   0.3755   -0.23613  -0.14129  -0.44537  -0.65737\n",
      " -0.042421 -0.28636  -0.28811   0.063766  0.20281  -0.53542   0.41307\n",
      " -0.59722  -0.38614   0.19389  -0.17809   1.6618   -0.011819 -2.3737\n",
      "  0.058427 -0.2698    1.2823    0.81925  -0.22322   0.72932  -0.053211\n",
      "  0.43507   0.85011  -0.42935   0.92664   0.39051   1.0585   -0.24561\n",
      " -0.18265  -0.5328    0.059518 -0.66019   0.18991   0.28836  -0.2434\n",
      "  0.52784  -0.65762  -0.14081   1.0491    0.5134   -0.23816   0.69895\n",
      " -1.4813   -0.2487   -0.17936  -0.059137 -0.08056  -0.48782   0.014487\n",
      " -0.6259   -0.32367   0.41862  -1.0807    0.46742  -0.49931  -0.71895\n",
      "  0.86894   0.19539 ] len:100\n",
      "word in vec [ 0.085703 -0.22201   0.16569   0.13373   0.38239   0.35401   0.01287\n",
      "  0.22461  -0.43817   0.50164  -0.35874  -0.34983   0.055156  0.69648\n",
      " -0.17958   0.067926  0.39101   0.16039  -0.26635  -0.21138   0.53698\n",
      "  0.49379   0.9366    0.66902   0.21793  -0.46642   0.22383  -0.36204\n",
      " -0.17656   0.1748   -0.20367   0.13931   0.019832 -0.10413  -0.20244\n",
      "  0.55003  -0.1546    0.98655  -0.26863  -0.2909   -0.32866  -0.34188\n",
      " -0.16943  -0.42001  -0.046727 -0.16327   0.70824  -0.74911  -0.091559\n",
      " -0.96178  -0.19747   0.10282   0.55221   1.3816   -0.65636  -3.2502\n",
      " -0.31556  -1.2055    1.7709    0.4026   -0.79827   1.1597   -0.33042\n",
      "  0.31382   0.77386   0.22595   0.52471  -0.034053  0.32048   0.079948\n",
      "  0.17752  -0.49426  -0.70045  -0.44569   0.17244   0.20278   0.023292\n",
      " -0.20677  -1.0158    0.18325   0.56752   0.31821  -0.65011   0.68277\n",
      " -0.86585  -0.059392 -0.29264  -0.55668  -0.34705  -0.32895   0.40215\n",
      " -0.12746  -0.20228   0.87368  -0.545     0.79205  -0.20695  -0.074273\n",
      "  0.75808  -0.34243 ] len:100\n",
      "word a vec [-0.27086    0.044006  -0.02026   -0.17395    0.6444     0.71213\n",
      "  0.3551     0.47138   -0.29637    0.54427   -0.72294   -0.0047612\n",
      "  0.040611   0.043236   0.29729    0.10725    0.40156   -0.53662\n",
      "  0.033382   0.067396   0.64556   -0.085523   0.14103    0.094539\n",
      "  0.74947   -0.194     -0.68739   -0.41741   -0.22807    0.12\n",
      " -0.48999    0.80945    0.045138  -0.11898    0.20161    0.39276\n",
      " -0.20121    0.31354    0.75304    0.25907   -0.11566   -0.029319\n",
      "  0.93499   -0.36067    0.5242     0.23706    0.52715    0.22869\n",
      " -0.51958   -0.79349   -0.20368   -0.50187    0.18748    0.94282\n",
      " -0.44834   -3.6792     0.044183  -0.26751    2.1997     0.241\n",
      " -0.033425   0.69553   -0.64472   -0.0072277  0.89575    0.20015\n",
      "  0.46493    0.61933   -0.1066     0.08691   -0.4623     0.18262\n",
      " -0.15849    0.020791   0.19373    0.063426  -0.31673   -0.48177\n",
      " -1.3848     0.13669    0.96859    0.049965  -0.2738    -0.035686\n",
      " -1.0577    -0.24467    0.90366   -0.12442    0.080776  -0.83401\n",
      "  0.57201    0.088945  -0.42532   -0.018253  -0.079995  -0.28581\n",
      " -0.01089   -0.4923     0.63687    0.23642  ] len:100\n",
      "word \" vec [-0.30457   -0.23645    0.17576   -0.72854   -0.28343   -0.2564\n",
      "  0.26587    0.025309  -0.074775  -0.3766    -0.057774   0.12159\n",
      "  0.34384    0.41928   -0.23236   -0.31547    0.60939    0.25117\n",
      " -0.68667    0.70873    1.2162    -0.1824    -0.48442   -0.33445\n",
      "  0.30343    1.086      0.49992   -0.20198    0.27959    0.68352\n",
      " -0.33566   -0.12405    0.059656   0.33617    0.37501    0.56552\n",
      "  0.44867    0.11284   -0.16196   -0.94346   -0.67961    0.18581\n",
      "  0.060653   0.43776    0.13834   -0.48207   -0.56141   -0.25422\n",
      " -0.52445    0.097003  -0.48925    0.19077    0.21481    1.4969\n",
      " -0.86665   -3.2846     0.56854    0.41971    1.2294     0.78522\n",
      " -0.29369    0.63803   -1.5926    -0.20437    1.5306     0.13548\n",
      "  0.50722    0.18742    0.48552   -0.28995    0.19573    0.0046515\n",
      "  0.092879  -0.42444    0.64987    0.52839    0.077908   0.8263\n",
      " -1.2208    -0.34955    0.49855   -0.64155   -0.72308    0.26566\n",
      " -1.3643    -0.46364   -0.52048   -1.0525     0.22895   -0.3456\n",
      " -0.658     -0.16735    0.35158    0.74337    0.26074    0.061104\n",
      " -0.39079   -0.84557   -0.035432   0.17036  ] len:100\n",
      "Found 1000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('./large_files/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "    count = 0  \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        \n",
    "        \n",
    "        count += 1\n",
    "        if count < 10:\n",
    "            print(\"word {} vec {} len:{}\".format(word,vec,len(vec)))\n",
    "        \n",
    "        if count >= 1000:\n",
    "            break\n",
    "print('Found %s word vectors.' % len(word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['is', 'run', 'ii', 'again', 'february', 'been', 'am', 'every', 'my', '-', 'between', 'nothing', 'international', 'previous', 'reports', 'close', 'into', 'vice', 'ministry', 'monday', 'least', 'evidence', 'provide', 'same', 'since', 'having', 'ca', 'tax', \"''\", 'attacks', 'hit', 'continue', 'deal', 'fire', 'committee', 'talks', 'future', 'church', 'today', 'california', 'common', 'something', 'used', ';', 'free', 'i', 'canada', 'allowed', 'comes', 'growing', 'japanese', 'republican', 'months', 'remains', 'won', 'authorities', 'received', 'light', 'commission', 'construction', 'first', 'popular', 'scored', 'life', '2009', 'closed', 'clear', 'reporters', 'reached', 'them', 'days', 'election', 'against', 'cut', 'or', 'family', 'percent', 'five', '2005', 'they', 'vote', 'could', 'party', 'stocks', 'billion', 'face', 'much', 'just', 'important', 'french', \"'ve\", 'development', 'such', 'further', 'released', 'art', '12', 'military', 'southern', 'u.s.', '24', 'father', 'young', 'operation', 'army', 'away', 'believe', 'however', 'shot', 'road', 'anti', 'forces', 'biggest', '--', 'schools', 'britain', 'less', 'groups', 'he', 'german', 'can', 'secretary', 'open', 'song', 'said', 'interest', 'saturday', 'human', 'village', 'january', 'despite', 'level', 'always', 'already', 'growth', 'district', 'although', 'largest', 'games', 'november', 'de', 'showed', 'are', 'indian', 'use', 'night', 'minutes', 'trade', 'try', 'various', 'held', 'series', 'countries', '2010', 'board', 'contract', 'services', 'off', 'live', 'course', 'we', 'across', 'nine', 'nations', 'class', '$', 'world', 'few', 'hope', 'university', 'issue', 'recent', 'spokesman', 'come', 'palestinian', 'mother', 'russia', 'study', 'trading', 'means', '17', 'fourth', 'win', 'official', 'created', 'work', 'began', 'space', 'thing', 'especially', '2000', 'best', 'from', 'association', 'march', 'thousands', 'kind', 'women', 'town', 'stage', 'on', 'play', 'able', '4', 'border', 'government', 'analysts', 'point', 'throughout', 'chairman', 'career', 'feel', 'see', 'bad', 'india', 'strong', 'had', 'here', 'post', 'legal', '40', 'record', 'july', 'markets', 'getting', 'within', 'august', 'bank', 'end', 'south', 'an', 'council', 'think', 'investors', 'your', 'at', 'whether', 'events', 'nuclear', 'head', 'afghanistan', 'times', '8', '1997', '1', 'john', 'sold', 'black', 'failed', 'president', 'instead', \"'ll\", 'pay', 'music', 'decided', 'english', 'last', 'its', '11', 'internet', 'toward', '10', 'named', 'officer', 'executive', 'campaign', 'band', 'key', '`', 'presidential', 'education', 'currently', 'ago', 'hold', 'own', 'probably', 'total', 'index', 'than', 'allow', 'son', 'often', 'woman', 'which', 'time', 'established', 'school', 'green', 'a', 'long', 'relations', 'former', 'those', 'body', 'one', 'that', 'data', 'out', 'little', 'saw', 'video', 'technology', 'effort', 'lower', 'division', 'bill', ',', 'lot', 'published', 'asia', 'general', 'take', 'together', 'book', 'rather', 'college', 'led', 'you', 'her', 'include', 'dollars', 'children', 'as', 'makes', 'security', 'energy', 'then', 'war', 'european', 'things', 'star', 'season', 'region', 'man', 'and', 'forward', 'foreign', 'any', 'nato', 'has', 'summer', 'korea', 'worked', 'island', 'los', 'center', '&amp;', '’s', 'research', 'would', 'until', 'australian', 'king', 'two', 'where', 'never', 'information', 'george', 'part', 'state', 'national', 'behind', 'group', 'mission', 'member', 'area', 'economic', 'lives', 'top', 'match', 'water', 'obama', 'france', 'get', 'spent', 'james', 'issues', 'clinton', 'person', 'taken', 'nation', 'fell', 'started', 'added', 'this', '2008', 'plans', 'based', 'day', 'drug', 'june', 'un', 'league', 'seen', 'north', 'possible', 'age', 'people', 'were', 'eastern', 'ever', 'sent', 'by', 'case', \"'\", 'china', 'population', 'also', 'prime', '13', 'new', 'inc.', \"'s\", 'robert', 'saying', 'present', 'staff', 'next', 'how', 'troops', '14', 'control', 'victory', 'number', 'interview', 'living', 'wrote', 'prices', 'like', '2', 'community', 'about', 'minister', 'loss', 'york', 'manager', 'will', 'senior', 'question', 'operations', 'states', 'care', '21', 'british', 'was', 'media', 'average', 'available', 'union', 'came', 'home', 'early', 'england', 'morning', 'football', 'tv', 'change', 'doing', 'running', 'move', 'players', 'after', 'west', 'return', 'major', '5', 'middle', 'during', 'most', 'help', 'conference', 'friday', 'being', 'majority', 'if', 'himself', \"'re\", 'others', 'country', '``', 'team', 'april', 'our', 'big', 'making', 'workers', 'stock', 'beat', 'director', 'charges', 'battle', 'old', 'may', 'financial', '\"', 'airport', ':', 'she', 'each', 'great', 'story', 'health', '%', 'but', 'located', 'it', 'years', 'either', 'continued', 'sunday', 'economy', 'leader', 'market', 'now', 'power', 'before', 'died', 'final', \"'m\", 'competition', 'all', 'texas', 'red', 'regional', 'performance', 'have', 'went', 'no.', 'involved', 'championship', 'share', 'down', 'there', 'large', 'yet', 'u.n.', 'too', 'house', 'reported', 'opposition', 'why', 'fighting', 'should', 'way', 'taking', 'almost', 'call', 'job', 'support', 'game', 'taiwan', '18', 'in', 'outside', 'daily', 'russian', 'opening', 'though', 'meeting', 'officials', 'when', 'several', 'personal', 'fund', '1994', 'wanted', 'must', 'problem', 'cooperation', 'put', 'without', 'current', 'go', 'their', 'did', 'told', 'still', 'the', 'side', 'opened', 'defense', 'crisis', 'management', 'term', 'asked', 'along', 'arrested', 'social', 'place', 'quarter', 'left', 'child', 'shows', 'described', 'mark', 'islamic', 'say', 'month', 'thought', 'court', 'industry', 'working', 'leave', 'white', 'michael', 'act', 'form', 'around', 'later', 'park', 'no', 'united', 'products', 'station', 'show', '2004', 'associated', 'tuesday', 'central', 'very', 'political', 'hard', 'october', 'given', 'sports', 'works', 'bush', 'seven', 'hong', 'served', '2001', 'look', 'keep', 'civil', 'weapons', 'wednesday', 'leading', 'african', 'signed', 'coach', 'accused', 'members', 'florida', 'only', 'field', 'law', 'returned', 'to', 'give', 'hours', 'parties', 'local', 'low', 'many', '2011', 'agreed', 'included', 'democratic', 'four', '_', 'six', 'judge', 'program', 'room', 'thursday', 'situation', 'really', 'whose', 'decision', 'production', 'poor', 'per', 'israeli', 'statement', 'recently', '6', 'among', 'western', 'far', 'known', 'teams', 'ended', 'film', 'australia', '9', 'meet', 'tried', 'likely', 'rates', 'gas', 'met', 'television', 'done', 'problems', 'american', 'air', 'other', 'including', 'result', 'both', 'who', 'turned', 'beijing', 'global', 'angeles', 'eu', 'men', 'followed', 'administration', 'independent', '3', 'cost', 'training', 'press', 'original', 'building', 'needed', '15', 'another', 'ahead', 'peace', 'found', 'up', 'attention', 'lead', 'player', 'rights', 'expected', 'because', 'fall', 'city', 'report', 'event', 'near', 'due', 'high', 'called', 'students', 'agreement', 'capital', '2012', 'list', 'companies', 'example', '30', '2002', 'asian', 'helped', '100', 'results', 'medical', 'half', 'over', 'force', 'province', 'department', 'me', 'be', 'test', ')', 'winning', 'announced', 'more', 'similar', \"n't\", 'iran', 'miles', 'business', 'banks', 'federal', 'chief', 'budget', '19', 'built', 'americans', 'annual', 'society', 'made', 'second', 'looking', '50', 'love', 'network', '?', 'africa', 'policy', 'role', 'land', 'iraqi', 'buy', 'album', 'river', 'dead', 'company', 'late', 'want', 'italy', 'leaders', 'police', 'rose', 'private', 'radio', 'killed', 'not', 'three', 'says', 'east', 'year', 'kong', 'investigation', 'london', 'round', '–', 'following', 'calls', 'century', 'project', 'action', '20', 'israel', 'history', 'sea', 'single', 'trying', 'third', 'written', 'process', 'turn', 'senate', 'good', 'justice', 'america', '/', 'back', '!', 'so', 'played', 'oil', 'of', 'europe', 'some', 'became', 'even', '2006', 'elections', 'born', 'what', 'playing', '(', 'become', 'agency', 'latest', 'public', 'different', 'money', 'iraq', 'aid', 'street', 'let', 'club', 'million', 'title', 'death', 'with', 'soon', 'base', 'coming', 'increase', 'brought', 'price', 'points', '16', 'goal', 'county', 'set', 'appeared', '22', 'offer', 'germany', 'fight', 'using', 'does', 'office', 'newspaper', 'position', 'real', 'fact', 'plan', 'right', 'dollar', 'lost', 'full', 'once', 'san', 'him', 'his', '.', 'main', 'northern', 'start', 'these', 'movement', 'considered', 'news', 'race', 'hand', 'remain', 'higher', 'september', 'soldiers', 'week', 'sales', '2007', 'might', 'nearly', 'past', 'through', 'al', 'know', 'us', 'demand', 'prison', 'special', 'weeks', 'find', 'moved', 'got', 'well', 'david', 'gave', 'computer', 'car', 'rate', 'areas', 'release', 'washington', 'stop', 'cup', 'ground', 'under', 'organization', 'hospital', 'trial', 'name', 'violence', 'order', 'bring', '...', 'yen', 'pakistan', 'families', 'republic', 'japan', 'eight', 'front', 'took', 'according', 'system', 'wife', 'december', 'site', 'chinese', 'going', 'enough', 'earlier', 'food', 'cases', 'parliament', '7', 'efforts', 'congress', 'coast', 'attack', '1998', 'shares', 'need', 'for', 'pressure', 'short', 'make', 'st.', 'exchange', '1996', 'period', '2003', 'service', 'tour', 'runs', 'mexico', '25', 'line', 'do', 'while', 'investment', 'paul', '1999', 'small', 'gold', 'visit', 'better'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in comments...\n"
     ]
    }
   ],
   "source": [
    "# prepare text samples and their labels\n",
    "print('Loading in comments...')\n",
    "\n",
    "train = pd.read_csv(\"./large_files/jigsaw-toxic-comment-classification-challenge/train.csv\",nrows = 200)\n",
    "sentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\n",
    "possible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "targets = train[possible_labels].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sentences (strings) into integers\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "# print(\"sequences:\", sequences); exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, [347, 84, 1, 102, 89, 148, 31, 348, 65, 288], 200, (200,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[0]),sequences[0][:10],len(sequences),sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length: 467\n",
      "min sequence length: 1\n",
      "median sequence length: 31\n"
     ]
    }
   ],
   "source": [
    "print(\"max sequence length:\", max(len(s) for s in sequences))\n",
    "print(\"min sequence length:\", min(len(s) for s in sequences))\n",
    "s = sorted(len(s) for s in sequences)\n",
    "print(\"median sequence length:\", s[len(s) // 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3768 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['is', 'label', 'thank', 'february', 'dolls', 'mussolini', 'am', 'sentence', 'sampled', 'stop', 'tonight', 'archaeology', 'ox', 'speed', 'indication', 'mismatch', 'attempts', 'accurate', 'nothing', 'shatter', 'asshole', 'reports', 'writing', 'into', 'asymmetrical', 'hastol', 'same', 'since', 'sityush', 'renominate', 'guidelines', 'eighty', 'box', 'thats', 'attacks', 'college', 'geometry', 'm122', 'drum', 'dark', 'designer', '38', 'deficient', 'today', 'used', 'unless', 'free', 'appearence', 'warranted', 'entirely', 'bacteria', 'familiar', 'migration', 'next', 'life', 'metal', 'articles', 'headed', 'expertise', 'year', 'ref', 'templates', 'date', 'outcomes', 'speedy', 'equals', 'kasuga', 'possible', 'statistics', 'deserves', 'wiaga', 'or', '26', 'similarly', 'attend', \"you're\", '2005', 'they', 'grab', 'effluent', 'party', 'fact', 'content', 'much', 'chemical', 'appear', 'fail', 'speak', 'happen', 'spending', 'bitch', 'debate', 'loves', 'seriously', 'such', 'literally', 'further', '78', 'put', 'did', 'justices', \"''''''chatspy\", 'warren', \"you'll\", 'sucker', 'arabic', 'newbies', 'core', 'ass', 'away', 'mainspace', 'changing', 'y', 'collusion', 'looks', 'science', 'exact', 'yale', 'exceed', 'practitioners', 'library', 'bi', 'nanotubes’', 'guess', 'thinking', 'luzerne', 'can', 'delete', 'absolute', 'category', 'dirt', 'said', 'illustrates', 'foremost', 'evaluation', 'falls', 'interest', 'mom', 'push', 'summarize', 'primary', 'prove', 'threatening', 'combining', 'january', 'wondered', 'level', 'sit', 'happened', 'pairs', 'unattractive', 'baseless', '23', 'downloads', 'natural', 'arguing', 'stub', 'distance', \"article's\", 'plays', 'episode', 'throwing', 'minutes', 'gibberish', 'artie', 'creatures', 'december', 'make', 'irish', 'again', 'longer', 'course', 'ml', 'ignore', 'spot', 'approach', 'hope', 'banned', 'screwjob', 'necessarily', 'solid', 'issue', 'recent', 'continue', 'application', '24', 'processes', 'of', 'russia', 'bodies', '28', 'skyhooks', 'rhetoric', 'feb', 'refers', 'work', 'things', 'space', 'files', 'from', 'throw', 'functioned', \"shouldn't\", 'above', 'apologized', 'assessment', 'pdf', 'www', 'town', 'couple', 'on', 'play', 'jpg', \"magazine's\", 'following', 'border', 'nominations', 'point', 'citiations', 'comprehensive', 'arm', 'depends', 'bad', 'necessary', 'had', 'post', 'follow', 'nazi', 'concept', 'mathematically', 'birth', 'meaning', 'existing', '‘buckminster', 'rfa', 'within', 'august', 'plasma', 'attainable', 'ever', 'south', 'region', 'imbalance', 'noble', 'rotating', 'actual', 'physics', 'symmetry', 'overweight', 'potential', 'head', 'kalttari', 'sry', 'evaluated', 'wacko', 'resolved', '1', 'eugenics', 'fix', 'rest', 'voice', 'older', 'before', 'notice', 'rurika', 'signed', 'music', 'goofed', 'indicate', 'minimum', 'last', 'theorize', 'its', 'pages', 'lines', 'sample', 'manner', 'picture', 'besides', 'mabuska', 'wikipedians', 'differences', 'archipelago', 'band', 'helpwikipedia', 'unfortunately', 'semitian', 'contrary', 'qoute', 'someone', 'ago', \"sister's\", 'event', 'followup', 'intentions', 'skyhook', 'judged', '2016', 'than', 'citation', 'editing', 'vehicle', 'which', 'administrative', 'presently', 'brains', 'a', 'harlan', 'variety', 'haplogroup', 'redirect', 'bow', 'cant', 'skyhook”', 'extinct', 'abusing', 'fruit', 'one', 'references', 'contribution', 'that', 'website', 'tether', 'boy', 'speculation', 'block', 'tfd', 'lower', 'additional', '3', 'sixty', 'asia', 'serves', 'raulbot', 'unknown', 'language', 'cheating', 'arabs', 'stated', 'predominantly', 'griffith', 'diplomatic', 'shit', 'you', 'moore', 'her', \"wikipedia's\", 'children', 'as', 'bees', 'offensive', 'european', 'finishing', 'agriculture', 'army', 'consisted', 'describes', 'desk', 'and', 'attribute', 'subsection', 'wow', \"''''''the\", '–', 'heinous', 'any', 'invented', 'motives', 'has', 'inability', 'benjamin', 'worked', 'cost', 'wing', \"company's\", 'speeds', 'seek', '79', 'until', 'allow', 'another', 'clear', 'ryo', 'putting', 'two', '82', 'demographics', 'rationale', 'key', 'santanas', 'state', 'farmers', '39', 'behind', 'carrier', 'yours', \"it's\", 'images', 'pop', 'top', 'yvesnimmo', 'match', 'genes', 'complainant', 'dropdown', 'get', 'licensing', 'spent', 'supporting', 'especially', 'corporate', 'ejaculating', \"china's\", 'birthday', 'divided', '—', 'uploading', 'fucking', 'anything', 'based', 'meet', 'either', 'discussions', 'ordnance', 'lobby', 'audio', 'learned', 'people', 'were', 'eastern', 'april', 'years', 'prevarications', 'case', 'way', 'investigator', 'transport', 'currently', 'several', 'also', 'new', 'perversion', 'dh', \"weren't\", 'uncivil', 'hi', 'sound', 'present', '77', 'negative', 'how', 'includes', 'instead', 'huh', 'judging', 'page', 'scenario', 'number', 'm95', 'living', 'ww2', 'arab', 'shwain', 'perfection', 'pedophilia', 'alternate', \"can't\", 'cock', 'abc', 'thus', \"'talk\", 'failure', 'symmetric', 'constantly', 'intent', 'age', 'jun', 'engineeringly', 'respect', 'e', 'will', 'logged', 'decided', 'looking', 'states', 'lie', 'care', 'apology', 'rational', 'satisfy', 'son', 'available', 'moscow', 'unsigned', 'type', 'came', 'pointed', 'temperatures', 'domain', 'raised', 'chernodrinski', 'technologies', 'change', 'hamish', 'appropriate', 'these', 'after', 'vfd', 'return', 'mother', \"neiln's\", 'called', 'name', 'most', 'help', 'reading', 'if', 'cheers', 'motive', 'mbunreachtnaheireann', 'country', 'media', 'culture', 'view', 'close', 'text', 'c', 'suffice', 'spurious', 'virus', 'etc', 'slaying', 'arms', 'ex', '188', '01', \"giant's\", 'moved', 'nobody', 'gear', 'aramaic', '93', 'skinhead', 'org', '34', 'located', 'humorous', 'bible', 'it', 'dry', 'contributions', 'changed', \"petras'\", 'clean', 'now', 'signpost', 'orbit', 'beware', 'iq', 'all', 'meow', 'applicable', 'accidents', '76', 'origins', 'dreams', 'polygraph', 'alone', 'oppose', 'scan', 'recantations', 'forgive', 'body', 'settled', '51', '“hypersonic', 'fighting', 'praise', 'trivia', 'ahead', 'job', 'glens', 'grudge', 'here', 'method', '18', 'persuaded', 'meetings', 'outside', 't', 'surprised', 'though', 'ok', 'absence', 'deemed', 'for', 'personal', '1994', \"everybody's\", 'meantime', 'basin', 'teh', 'snowcrystals', 'comments', 'problem', 'quote', 'sites', 'properly', 'current', 'go', 'dept', 'b', 'causeway', 'still', 'why', 'side', 'obviously', 'notion', 'tank', 'commercial', 'anyone', 'elevator', 'along', 'tools', '169', 'impact', 'noticed', 'character', 'sitting', 'juelz', 'triva', 'filmplot', 'antisemmitian', 'unsubscribe', 'enforcing', 'say', 'thought', 'problems', 'radical', 'bunch', 'considering', 'sort', 'bothered', 'james', '89', 'mentions', 'undeletion', 'forehead', 'researchers', 'form', 'agrees', 'freely', 'later', '–talk', 'cunts', 'behavior', '122', 'erased', 'everything', 'express', 'should', 'god', 'hard', 'trace', 'style', 'neutral', '1600', 'eighteen', 'look', 'including', 'phd', 'bald', 'marshall', 'itself', '18th', 'template', 'deepu', 'web', 'notable', 'suffer', 'hole', 'florida', 'masthead', 'only', 'field', 'cothurnocystis', 'to', 'gives', 'chicago', 'hours', 'introductory', 'generarizations', 'included', \"don't\", 'six', 'technically', 'accusations', 'child', 'ray', 'situation', 'really', 'whose', 'substantial', 'innocent', 'per', 'lange', '21wikipedia', 'systems', 'comparing', 'disagreement', 'acctually', 'dsm', 'known', 'decent', 'dude', \"they're\", 'episodes', 'aspects', 'uploaded', 'aware', 'socialist', 'sullivan', 'earth', 'upon', 'speaks', 'done', 'fashioned', 'american', 'ulterior', 'air', 'below', '500', 'cantor', 'improving', 'certainly', 'regarding', 'global', 'scare', 'exposed', 'men', 'entire', 'htm', 'built', 'doubt', 'malicious', 'experimenting', 'saudiarabia', 'bit', 'needed', 'hesitate', 'talk•contribs', 'checking', 'pretty', 'peace', 'backlog', 'goat', 'bullets', '32', 'attention', 'proposed', 'made', 'btw', '”', 'because', '1995', 'santana', 'report', 'strategy', 'kathleen', 'changes', 'explicit', 'holder', 'alternatives', 'belong', 'old', 'explanation', '27', 'andy', 'mason', 'suggestion', 'pretend', 'mitsurugi', \"kanin's\", 'empirical', 'turns', 'reasons', 'complete', 'educated', 'catagory', 'file', 'over', 'exclusive', 'universal', 'takes', 'mainland', 'me', 'be', 'anecdote', 'fascinating', 'herbert', 'entry', 'responses', 'anyway', 'lets', 'diplomats', 'dissenters', 'faith', 'tagged', 'com', '19', 'predominant', 'rubbish', '191', 'descend', 'second', 'names', 'everyone', 'wikipedia', 'christian', 'strengths', '50', 'awesome', 'reusable', 'yourself', 'configurations', 'frank', 'assyrian', 'fort', 'ix', 'selected', 'want', 'studying', 'explain', 'establishment', 'killed', 'easily', '2012', 'irrefutable', 'faculty', 'east', 'closure', 'finding', 'introduction', 'fine', 'types', 'juvenile', 'southeast', 'ii', 'hopes', 'assign', 'history', 'welcome', 'inappropriate', 'owner', 'third', 'process', 'surrounding', 'requesting', 'word', 'blog', 'clarence', 'provide', 'antonin', 'back', 'wanting', 'tag', 'summary', 'gang', '2c', 'proponents', 'even', '2006', 'born', 'wonder', 'what', 'jayjg', 'verify', 'claim', 'liberally', 'concern', 'apologise', 'stuck', 'masses', 'public', 'credentials', 'probabilities', 'jump', 'administrators', 'italy', 'improbable', 'street', 'insulting', 'recongise', 'unhelpful', 'bisexual', 'fuckcock', 'gays', 'harassed', 'actually', 'wiki', 'difficult', 'passed', 'death', 'your', \"'nonsense'\", 'undoing', 'false', 'unfounded', 'zubrin', 'coming', \"that's\", 'hurt', 'ask', 'shitstorm', \"subject's\", \"i'd\", 'bottom', 'points', 'sucking', 'briefly', 'hospital', 'general', 'voted', 'vandalising', 'im', 'http', 'using', 'stylophora', 'eced', 'phone', 'deleted', 'position', 'real', 'drink', 'initial', \"armenia's\", '109', 'destroying', 'convenience', 'posted', 'telling', 'saying', 'tiger', 'caltech', 'archive', 'sure', 'assessed', 'heterosexual', 'wry', 'status', 'dependence', 'critical', 'followups', 'interested', 'conservatives', 'scientific', 'payload', 'bias', 'feelings', 'september', '2007', 'therefore', 'might', 'obvious', 'through', 'lineages', 'decreases', \"hammer's\", 'fir', 'dont', 'ancestry', 'place', 'failed', 'stalking', 'accepted', 'copyrighted', 'computer', 'when', 'abn', '154', 'homosexuality', 'greetings', 'pool', 'socialistic', 'middle', 'straight', 'project', 'pentagram', 'got', 'figure', \"'45\", '20', \"isn't\", 'republic', 'generating', 'slow', 'empty', 'yes', 'bullet', 'seeing', 'source', 'dedicating', 'straw', 'suggests', 'remember', 'snowflake', 'appreciated', 'removed', 'begin', 'socialists', 'unlikely', 'do', 'non', 'aggravate', 'single', 'attack', 'cover', 'observe', 'opposite', 'uncovered', 'particular', 'connections', 'renzo', '2003', 'improvement', 're', 'flagella', 'forced', 'filthy', 'fan', 'dissident', 'hyperskyhook', 'while', 'remove', 'paul', 'speedily', 'clutter', 'a2', 'except', 'intelligence', 'fuller', 'run', 'lazy', 'been', 'സംവാദം', 'every', 'seem', 'kicked', 'my', 'yayoi', 'neff', 'between', 'claimed', 'concepts', 'tensile', 'pictures', 'previous', 'our', 'roger', 'mention', 'talking', 'shocks', 'he', 'evidence', 'prevent', 'paark', 'having', 'fucked', 'address', 'untagged', 'sin', 'data', 'stands', 'future', 'inclusion', 'vandalism', 'common', 'something', 'whether', 'disregard', 'i', 'store', 'japanese', 'months', 'justify', 'brute', 'books', 'hindi', 'archangel', 'aswell', 'context', 'forty', 'tatoo', 'first', 'naturally', 'atomic', 'playground', 'myself', 'absurd', 'reached', 'them', 'hood', 'better', '1equalvoice1', 'suggested', 'forcing', 'against', 'civility', 'mine', '139', \"'s\", 'eventually', 'activity', '159', 'speculated', 'learning', 'like', '161', 'orbiting', 'fuckeeed', 'details', 'section', 'sidaway', 'could', 'stopped', 'message', 'basis', 'honeycomb', 'just', 'numerous', 'drown', 'specifies', 'important', 'movements', 'duck', 'subjected', 'ireland', 'destructive', 'base', '1983', 'banks', 'piece', 'hand', 'https', 'violating', 'southern', 'achieve', 'southeastern', 'o', 'young', 'encyclopedia', 'helps', \"'image'\", 'orbital', 'judgment', 'believe', 'deserve', 'merely', 'definitive', 'anti', 'format', '\\xa0·', 'reverted', 'english', '48', 'someones', 'less', 'well', 'bbq', 'list', 'great', 'constitutionofireland', 'study', 'lakes', '201', 'review', 'airplane', 'id', 'almighty', 'eg', 'reasonings', 'jams', 'sir', 'irregular', 'always', 'already', 'lloyd', 'those', 'dulithgow', 'games', 'understand', 'de', 'tidying', 'hasty', '17', 'neutrality', 'cause', 'regarded', 'use', 'heading', 'hurdle', 'scalia', 'jacko', 'roc', 'forming', '—the', 'official', 'off', 'combined', 'singles', 'fat', 'stuff', 'we', 'across', 'class', 'fistfuckee', '200', 'pals', 'honors', 'bye', 'similar', 'few', 'thomas', '229', 'horny', \"'i'll\", 'comment', 'todsy', 'fundamental', 'come', 'designed', 'probably', 'obtained', 'removing', 'proper', 'kultury', 'falsities', 'libbrecht', 'el', 'answering', 'learn', 'sorry', 'developed', 'thing', 'felice', 'dick', 'listed', 'kenneth', 'best', 'kind', 'suggest', 'but', \"emma's\", 'consistently', 'able', '4', 'issued', 'websites', 'serious', '04', 'government', 'uh', 'surely', 'bands', 'thanks', 'perfectly', 'feel', 'note', 'weeds', 'retired', 'sexually', 'quickly', 'wikiproject', 'legal', 'inane', 'record', 'hero', 'talked', 'gw', 'utc', 'yeah', 'ammended', 'myths', 'getting', 'hammer', 'candidate', 'criteria', 'end', 'origin', 'yangtze', 'think', 'argument', 'shut', 'preceding', 'muslims', \"aren't\", 'thatso', 'add', 'times', 'disagree', 'description', 'recantation', 'jewish', 'reasonable', 'montreal', 'tosser', 'john', 'xd', 'architecture', 'willing', 'nationality', 'sex', 'apologize', 'asks', 'ethnicity', 'heart', 'sternhell', 'are', 'have', 'formed', 'whats', 'officially', 'went', '11', 'profession', 'conversation', 'unfair', 'jackson', 'toward', '10', 'damage', 'magic', 'feasible', 'named', 'stars', 'reviewed', 'reviewer', \"page'\", 'insult', 'addition', 'haywire', 'answer', 'term', 'talibans', 'down', 'nor', 'hold', 'reader', 'mcdonald', 'kindly', 'classical', 'author', 'questions', \"you've\", 'often', 'information', 'hell', 'time', 'school', 'long', 'jumps', 'nice', 'former', 'seen', 'probability', 'defined', 'belated', 'see', 'out', 'little', 'appointed', 'conclusion', 'edited', 'anymore', 'effort', 'oh', 'harvard', 'impressive', 'harshly', 'scholars', 'lot', 'legit', 'surface', 'cheapest', 'apparently', 'take', 'together', 'publisher', 'int', 'consistent', 'dna', 'gay', 'aircracft', 'barnstar', 'alignment', '12', 'breathing', 'include', 'confirm', 'eat', 'matt', 'makes', 'own', 'then', 'war', 'preferences', 'taking', 'unclear', 'attracting', \"there's\", 'intending', 'featured', 'numbers', 'ie', 'machine', 'mentioned', 'gently', '465', 'let', 'korea', 'try', 'attracted', 'restatement', 'textile', 'research', 'resolve', 'would', 'an', \"file's\", 'near', 'revert', 'rachel', 'never', 'constitutes', 'part', 'create', 'disputing', 'group', 'deaths', 'vandalisms', 'chance', 'consider', 'snowflakes', 'maybe', 'reliable', 'theories', 'objected', 'discussion', 'haplogroups', 'materials', 'definition', 'avoided', \"'juvenile'\", 'in', 'issues', 'facts', 'taken', 'possibillity', 'formatting', 'sincerely', 'started', 'added', 'this', 'sandbox', 'without', 'heavy', 'day', \"i've\", 'talk', 'others', 'encyclopedic', 'edu', 'studies', 'sent', 'by', 'mythology', 'sorrows', 'fossilized', 'china', 'syriac', 'justified', 'lisak', 'hoped', '58', 'else', 'bother', 'matter', 'defame', 'tony', 'poker', 'realistically', 'relevant', 'songs', 'conflict', '205', 'assert', 'right', 'discuss', 'perhaps', 'digg', 'give', 'fenian', 'orientation', 'solution', 'community', 'gypsys', 'stronger', 'about', 'ga', 'seconds', 'york', 'greetingshhh', 'altitu', 'kalchchari', 'rate', 'question', 'abt', 'breeding', 'dead', '46', 'was', 'up', 'refuting', 'males', 'scholary', 'becomes', 'drive', 'clicking', 'rid', 'grasp', 'contribs', 'finally', 'exactly', 'doing', 'username', 'considered', 'npv', 'atmospheric', 'deletion', 'amaze', 'unspecified', 'absolutely', 'major', '5', 'wp', 'japan', 'being', 'access', 'himself', 'redundant', 'specific', '“we', 'related', 'repost', 'incriminate', 'fascism', 'everytime', 'misread', 'making', 'rather', 'georgiev', 'extend', \"you'\", 'according', 'virtual', 'transliterating', \"website's\", 'may', 'diggs', 'violate', 'deliberate', 'blue', 'she', 'each', 'expert', 'offer', 'story', 'qualifies', 'recognize', 'locking', 'fingers', 'value', 'jack', 'snake', '2000', 'purist', 'higher', 'where', \"i'm\", 'admin', 'become', 'shirvington', 'owning', 'upper', 'freshman', 'boeing', 'texas', 'symmetrical', 'appears', 'ways', 'premature', 'president', 'concluding', 'there', 'usually', 'too', 'seems', 'allegedly', 'adding', 'ad', 'wonju', 'bear', 'mail', 'edit', 'elsewhere', 'almost', 'call', 'placed', 'keeps', 'support', 'competitive', 'reinforce', 'person', 'caculator', 'russian', 'paragraph', 'guild', 'disorder', 'simply', 'power', '107', 'read', \"doesn't\", 'wrong', 'lack', 'citizenship', 'wanted', 'must', 'metro', 'contain', 'consensus', 'guys', 'odd', 'appease', 'length', 'volume', 'their', 'gfdl', \"i'll\", 'tests', 'populations', 'the', 'invite', 'amro', 'member', 'assumed', 'jay', 'account', 'greatly', \"d'aww\", 'matches', 'claims', 'social', 'find', 'acknowledged', 'boilerplate', 'left', 'image', 'forgetting', 'nonsensical', 'babe', 'testing', 'significance', 'mcdickerson', 'wrestling', 'echinodermata', 'copyright', 'voydan', 'disruptively', 'uncommon', 'hardcore', 'hey', 'leave', 'repeat', 'white', 'population', 'michael', \"noseptember's\", 'around', 'park', 'no', 'agree', 'specify', 'transgressions', 'dispersals', 'please', 'show', 'observed', 'strengthen', 'tagging', '224', 'edits', 'bullshit', 'very', 'thread', 'given', 'specified', 'self', 'gan', 'works', 'with', 'questioning', 'contributing', 'bilateral', 'seven', '215', 'somebody', 'shave', \"sakazaki's\", 'closer', 'terrorism', 'limit', 'മെട്രോ', 'matched', 'satanistic', 'wrote', 'described', 'waste', 'deadly', \"wasn't\", 'goes', 'law', \"'animal\", 'transliteration', 'oppression', 'forensic', 'local', 'neolithic', 'many', 'realise', 'dominance', 'four', 'transliterate', 'warning', 'check', \"didn't\", 'native', 'student', 'decision', 'however', 'statement', 'communist', '6', 'bang', 'kanin', 'die', 'far', 'film', 'kid', 'jmabel', \"page's\", 'intelligent', 'selecting', 'douche', 'completely', 'piffling', 'b2', 'nose', 'at', '06', 'hominem', 'other', 'hypothesizes', 'both', 'who', 'true', 'lies', 'mediocre', 'although', \"womb'\", 'knowledge', 'girl', 'colour', 'intend', 'fella', 'brutally', 'funny', \"'fight\", '15', 'quite', 'enhanced', '187', 'profile', 'found', 'alien', 'bring', 'lead', 'lacking', 'sense', 'achievable', 'fall', 'hypersonic', 'rigged', 'version', 'display', '21', 'judgement', 'reference', 'subpages', 'arabization', 'lake', 'terms', 'stupid', 'write', '\\xa0「」¤\\xa0•\\xa0¢\\xa0', 'companies', 'example', 'spinning', 'holocaust', '2002', 'asian', 'integrated', '100', 'results', '70', 'seemingly', 'camera', 'force', 'generally', 'correctly', 'stanford', 'test', 'background', 'direction', \"'syrthiss's\", 'front', 'absoutely', 'balance', 'eatwell', 'aroused', 'facility', 'unnecessary', 'mean', 'christ', 'gas', 'conforms', 'closely', 'homosexual', 'biographies', 'planes', 'suggestions', '1st', 'v', 'homostelea', 'leno', 'design', 'license', 'policy', 'role', 'updating', 'river', 'company', 'zeev', 'late', 'diliff', 'links', 'meets', 'improved', 'bunyan', \"cam's\", 'forbid', 'radial', 'not', 'three', 'says', 'ambiguous', 'truth', 'arguments', 'some', 'awful', 'warnings', 'response', 'licensed', 'atleast', 'managed', 'concluded', 'clearly', 'sufficient', 'fundamentally', 'gone', 'notability', 'മോസ്കോ', 'combating', 'selective', 'payloads', 'trying', 'ceases', 'written', 'turn', 'bart133', 'good', 'choose', 'happy', 'leftist', 'so', '2', 'b1', 'thinks', 'recanted', 'argue', 'shooting', 'malayalam', 'cocksucker', 'assertion', 'ignored', 'griffin', \"god's\", 'masturbating', 'beyond', 'tags', 'different', 'pronounce', 'money', 'inc', 'guy', \"o'donohue\", 'needs', 'warring', 'responded', 'respected', 'piss', 'tethers', 'club', \"santana's\", 'lineage', 'apologies', 'topic', 'link', 'events', 'screwed', 'introducing', 'contest', 'brought', 'imo', 's', 'pair', 'academics', '16', 'county', 'peacefully', 'concerned', 'sources', '22', 'nope', 'created', 'researching', 'phase', 'does', 'metallica', 'crystals', 'despite', 'article', 'sexual', 'regions', 'fuck', 'full', 'chromosomes', 'tell', \"breeding'\", 'him', 'his', 'migrants', 'man', 'congratulations', 'start', 'considerate', 'academic', 'minimization', 'news', 'messy', 'deleting', '57', 'detailed', 'increasing', 'delting', 'shall', 'remain', 'saw', 'threats', 'week', 'reducing', 'delay', 'homie', 'drastically', 'video', 'moving', 'slavs', 'know', 'us', 'miss', 'a1', 'prehistoric', 'studied', 'arsenal', '2004', 'expires', 'propose', 'release', 'original', 'parzival418', 'reason', 'fac', \"freedom'\", 'reiterates', 'face', 'suggesting', 'explaining', 'under', 'looked', 'jews', 'reconciling', 'unsourced', '•', 'info', 'criticism', 'translate', 'db', 'promoter', 'fair', 'more', 'qualify', 'took', 'carol', 'valid', 'system', 'carbon', 'expect', 'christians', 'site', 'going', 'earlier', 'food', 'cases', 'appreciate', '7', 'benefit', 'words', 'effectively', 'cardozo', 'keep', 'fictional', 'need', 'accused', 'degree', 'regards', 'short', \"won't\", 'once', 'members', 'rule', 'operational', 'visit', 'bully', 'jurists', 'comming', 'mass', 'wish', 'subject', 'constitution', 'roots', 'reconsidered'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['explanation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (200, 100)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 347,  84,   1, 102,  89, 148,  31, 348,  65, 288,  53,\n",
       "        47,  16,  69,  90,   8, 810,  26, 124,   6,  54,  40, 211,   1,\n",
       "       243,  27,   1,  28,  29, 212,  48,  95, 811], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2idx dict has word key and corresponding integer value  \n",
    "- embedding_vector matrix has integer key index and corresponding word vector from glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_)\n",
    "x = Conv1D(128, 3, activation='relu')(x) # 128 output filters, 3 kernel size\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(len(possible_labels), activation='sigmoid')(x)\n",
    "\n",
    "model = Model(input_, output)\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='rmsprop',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data,\n",
    "  targets,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "\n",
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the mean AUC over each label\n",
    "p = model.predict(data)\n",
    "aucs = []\n",
    "for j in range(6):\n",
    "    auc = roc_auc_score(targets[:,j], p[:,j])\n",
    "    aucs.append(auc)\n",
    "print(np.mean(aucs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://blogs.mcgill.ca/cambam/files/2012/07/fig1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)  \n",
    "[ROC explanation](https://jeongchul.tistory.com/545)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
