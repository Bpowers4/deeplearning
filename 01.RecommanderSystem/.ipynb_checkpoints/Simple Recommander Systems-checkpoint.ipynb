{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More problems with average rating\n",
    "- What if N is very small or 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\bar{X}= \\frac{1}{N}\\sum_{i=1}^{N} X_i \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing (or dampening)\n",
    "- we've seen this in the context of NLP for word probabilities, also in PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "r= \\frac{\\sum_{i=1}^{N} X_i + \\lambda\\mu_0}{N+\\lambda}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- could make $\\mu_0$ the global average, or just some middle value like 3(3 Star value of 5 Star score)\n",
    "- 1000 4 star ratings $\\mu_0$ = 3, $\\lambda$ = 1 -> 3.999\n",
    "- 5 4 star ratings -> 3.83\n",
    "- one 4 star ratings -> 3.5\n",
    "- We'll see how bayesian approach yields same formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore- Exploit Dilemma \n",
    "- we've discussed before in context of A/B testing and Reinforcement Learning, in case you want to learn more\n",
    "- You are playing slot machines at casino\n",
    "- All machines look ths same, so you have to play them to determine which is best  \n",
    "- suppose only 2 outcomes: win(1), lose(0)\n",
    "- best machine has highest win rate \n",
    "\\begin{equation*}\n",
    "\\hat{p}= p(win) = \\frac{wins}{total}\n",
    "\\end{equation*}\n",
    "\n",
    "- How many times should I play each machine?\n",
    "- Too few times-> estimate is no good\n",
    "    - so collecting more data is good\n",
    "- win once out of 3 -> p(win) = 1/3\n",
    "    - fat confidence interval\n",
    "- traditional statistical tests can tell us whether or not there's a significant difference between win rates and between machines\n",
    "- Play 10 different machines 100 times each = 1000 turns\n",
    "- 9 machines are suboptimal-> 900 turns yielded a suboptimal reward\n",
    "- Hence we have a dilemma, play more? or play less?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore-Exploit in Recommenders\n",
    "- Ex. You watch a bunch of YouTube videos on how to make eggs\n",
    "- Now your recommendations are filled with videos about making eggs\n",
    "- Probably suboptimal- once I've figured out how to make eggs, I don't want to watch more eggs videos\n",
    "- Should there be a stronger exploration component?\n",
    "- Maybe I'd like to see movie trailers or machine learning videos\n",
    "- But you could just as easily explore with knitting videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore- Exploit\n",
    "- How do we strike a balance between these 2 opposing forces?\n",
    "- Smoothed average gives us one part of the solution\n",
    "    - But it's fine if you just want to do something simple\n",
    "- Next: The Bayesian solution to this problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach to the Explore-Exploit Dilemma\n",
    "- Main theme: Everything is a random variable\n",
    "- Random variables have a probability distribution, parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Example\n",
    "- Binary outcome\n",
    "- PMF(probability mess function) of x is given below\n",
    "- In the Bayesian paradigm, $\\pi$ is also a RV(random variable), what is distribution?\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x)= \\pi^{x}(1-\\pi)^{1-x}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distribution of $\\pi$\n",
    "\n",
    "- $\\pi$ has a Beta distribution\n",
    "- x is a discrete Random variable, but $\\pi$ is a continuous random variable\n",
    "- $\\pi$ is in the range [0,1]\n",
    "- x only has one parameter: $\\pi$\n",
    "- $\\pi$ has 2 parameters: $\\alpha,\\beta$\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi) = \\frac{1}{B(\\alpha,\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} , B(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\n",
    "\\end{equation*}\n",
    "\n",
    "[beta distribution wikipedia](https://en.wikipedia.org/wiki/Beta_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian approach\n",
    "- Non-Bayesian (frequentist)\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\prod_{i=1}^{N}\\pi^{x_{i}}(1-\\pi)^{1-x_{i}} ,(likelihood) \\\\\n",
    "\\hat{\\pi} = argmax_{\\pi}(L) = \\frac{1}{N}\\sum_{i=1}^{N}x_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "- Bayesian\n",
    "    - There is no $\\hat{\\pi}$ \n",
    "    - Instead, we want p($\\pi | x_{1},...,x_{N}) = p(\\pi |X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just use Bayers rule\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = \\frac{p(X|\\pi)p(\\pi)}{\\int_{0}^{1}{p(X|\\pi)p(\\pi)d\\pi}}\n",
    "\\end{equation*}\n",
    "\n",
    "- In general, posterior = likelihood * prior / normalizing constant\n",
    "- we can choose p($\\pi$) = Beta(1,1) which is the uniform dist\n",
    "- Why make this so complicated? integral are hard\n",
    "- In general, they are usually infeasible to calculate(but not in our case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change it to a proportionality\n",
    "- Drop the normalizing constant\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = \\propto{p(X|\\pi)p(\\pi)}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug in the things we konw\n",
    "\n",
    "- we know the expression for likelihood and prior, let's just plug them in\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = \\propto[\\prod_{i=1}^{N}\\pi^{x_{i}}(1-\\pi)^{1-x_{i}}]\\frac{1}{B(\\alpha,\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n",
    "p(\\pi|X) = \\propto{\\pi^{\\alpha+(\\sum_{i=1}^{N}X_{i})-1}}(1-\\pi)^{\\beta+N-{\\sum_{i=1}^{N}X_{i}}-1}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we found?\n",
    "- The posterior p($\\pi$|X) = is simply another Beta distribution\n",
    "- Posterior\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\pi|X) = const \\times [{\\pi^{\\alpha+(\\sum_{i=1}^{N}X_{i})-1}}(1-\\pi)^{\\beta+N-{\\sum_{i=1}^{N}X_{i}}-1}] \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "- Prior\n",
    "\\begin{equation*}\n",
    "p(\\pi) = const \\times \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\n",
    "\\end{equation*}\n",
    "\n",
    "- We have discovered\n",
    "\\begin{equation*}\n",
    "\\pi|X \\sim Beta(\\alpha',\\beta'), \\alpha' = \\alpha+\\sum_{i=1}^{N}X_{i},\\beta' = \\beta+N -\\sum_{i=1}^{N}X_{i}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Remark\n",
    "\n",
    "- From the formula, it appears we have collected N samples of X, and we update the posterior after having collected all of them\n",
    "- But we can update the posterior after collecting each sample\n",
    "- The posterior of one step becomes the prior of the next step\n",
    "- Online Learning\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha' = \\alpha+\\sum_{i=1}^{N}X_{i},\\beta' = \\beta+N -\\sum_{i=1}^{N}X_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "1. $\\pi \\sim Beta(1,1)$\n",
    "2. Sample $x_{1}$\n",
    "3. $\\pi \\sim Beta(1+x_{1},1+1-x_{1})$\n",
    "4. Sample $x_{2}$\n",
    "5. $\\pi \\sim Beta(1+x_{1}+x_{2},1+2-x_{1}-x_{2})$\n",
    "6. etc... in general: $\\alpha' = \\alpha + x_{i},\\beta' = \\beta +1 - x+{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Conjugate Priors\n",
    "\n",
    "- In general, this integral is hard to calculate, except in special circumstances(like the one we just looked at) \n",
    "- Special pairs of likelihood and prior, such that the posterior has same type of distribution as prior \n",
    "- It's the exception rather than the rule \n",
    "- Check out the wikipedia page on conjugate priors for a list \n",
    "\n",
    " \\begin{equation*}\n",
    "p(\\pi|X) = \\frac{p(X|\\pi)p(\\pi)}{\\int_{0}^{1}{p(X|\\pi)p(\\pi)d\\pi}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
