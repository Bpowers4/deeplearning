{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) User User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[user user collaborative filtering blog](https://medium.com/@tomar.ankur287/user-user-collaborative-filtering-recommender-system-51f568489727)\n",
    "\n",
    "[blog 2](https://medium.com/sfu-big-data/recommendation-systems-user-based-collaborative-filtering-using-n-nearest-neighbors-bf7361dc24e0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " []() |Batman  | X-Men| Star wars| The notebook| Bridget Jones's Diary|\n",
    "--|--|--|--|--|--|\n",
    "Alice|5|4.5|5|2|1|\n",
    "Bob|4.5|4| []()|2|2|\n",
    "Carol|2|3|1|5|5|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alice and Bob seem to be in agreement, disagree with Carol\n",
    "- Is Star Wars a good recommendation for Bob?\n",
    "- Intuitively, we see that Bob's ratings are similar to Alice's, thus he is likely to also like Star Wars\n",
    "- Math-speak: Alice's and Bob's ratings are highly correlated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Rating\n",
    "- Why is it limited?\n",
    "- It treats everyone's rating of the movie equally\n",
    "- Bob's s(i,j) equally depends on Alice's rating and Carol's rating, even though he doesn't agree with Carol\n",
    "\n",
    "\\begin{equation*}\n",
    "s(i,j) = \\frac{\\sum_{i' \\in \\Omega}^{}r_{i'j}}{|\\Omega_{j}|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting Ratings\n",
    "- We'll see how to calculate these weighs later\n",
    "- Intuitively, I want it to be small for users I don't agree with, large for users I do agree with\n",
    "\n",
    "\\begin{equation*}\n",
    "s(i,j) = \\frac{\\sum_{i' \\in \\Omega_{j}}^{}w_{ii'} r_{i'j}}{\\sum_{i' \\in \\Omega_{j}}^{}w_{ii'}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another issue with average rating\n",
    "- your interpretation of a rating is different from mine\n",
    "- Users can be biased to be optimistic or pessimistic\n",
    "- E.g, optimistic: most movies are a 5, bad movie is a 3\n",
    "- E.g. pessimistic: most movies are 1 or 2, good movie is a 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deviation\n",
    "- We don't care about your absolute rating, we care how much it deviates from your own average\n",
    "    - if your average is 2.5, but you rate something a 5, it must be really great\n",
    "    - if you rate everything a 5,it's difficult to know how those items compare\n",
    "    \n",
    "$\\overline{r}_{i}$ is sample mean of r      \n",
    "$dev(i,j) = r(i,j) - \\overline{r}_{i}$, for a known rating\n",
    "\n",
    "- My predicted deviation is the average deviation for a movie\n",
    "- Then my predicted rating is my own average + predicted deviation\n",
    "\n",
    "$dev(i,j) = r(i,j) - \\overline{r}_{i}$ for a known rating  \n",
    "$\\hat{dev(i,j)} = \\frac{1}{|\\Omega_{j}|}\\sum_{i'\\in \\Omega_{j}}^{}r(i',j)-\\overline{r}_{i'}$  \n",
    "for a prediction from known ratings  \n",
    "$s(i,j) = \\overline{r}_{i} + \\frac{\\sum_{i'\\in \\Omega_{j}}^{}r(i',j)-\\overline{r}_{i'}}{|\\Omega_{j}|} = \\overline{r}_{i}+\\hat{dev(i,j)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine\n",
    "- Combine the idea of deviations with the idea of weightings to get our final formula\n",
    "- Note: absolute value since weights can be negative\n",
    "\n",
    "\\begin{equation*}\n",
    "s(i,j) = \\overline{r}_{i} + \\frac{\\sum_{i'\\in \\Omega_{j}}^{}w_{ii'}(r_{i'j}-\\overline{r}_{i'})}{\\sum_{i'\\in \\Omega_{j}}^{}|w_{ii'}|}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate weights\n",
    "\n",
    "- How do we calculate correlations between 2 sequences of data?\n",
    "- Pearson correlation coefficient\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varrho_{xy} = \\frac{\\sum_{i=1}^{N}(x_{i}-\\overline{x})-(y_{i}-\\overline{y})}{\\sqrt{\\sum_{i=1}^{N}(x_{i}-\\overline{x})^2} \\sqrt{\\sum_{i=1}^{N}(y_{i}-\\overline{y})^2}}\n",
    "\\end{equation*}  \n",
    "\n",
    "- Problem: our matrix is sparse\n",
    "\n",
    "- Idea: just use the data we have\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{ii'} = \\frac{\\sum_{j\\in \\Psi_{ii'}}^{}(r_{ij}-\\overline{r}_{i})-(r_{i'j}-\\overline{r}_{i'})}{\\sqrt{\\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-\\overline{r}_{i})^2} \\sqrt{\\sum_{j \\in \\Psi_{i'}}^{}(r_{i'j}-\\overline{r}_{i'})^2}}\n",
    "\\end{equation*}  \n",
    "\n",
    "$\\Psi_{i}$ = set of movies that user i has rated  \n",
    "$\\Psi_{ii'}$ = set of movies both user i and user i' have rated  \n",
    "$\\Psi_{ii'} = \\Psi_{i} \\cap \\Psi_{i'}$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about cosine similarity?\n",
    "\n",
    "- Pearson correlation is old school statistics\n",
    "- But wait\n",
    "\n",
    "\\begin{equation*}\n",
    "cos\\theta = \\frac{x^{T}y}{|x||y|} = \\frac{\\sum_{i=1}^{N}x_{i}y_{i}}{\\sqrt{\\sum_{i=1}^{N}x_{i}^2}\\sqrt{\\sum_{i=1}^{N}y_{i}^2}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity vs Pearson correlation\n",
    "- They are the same, Except Pearson is centered\n",
    "- We want to center them anyway because we're working with deviations, not absolute ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "- What if 2 users have zero movies in common, or just few?\n",
    "- If zero, don't consider, i' in user i's calculation- it can't be calculated\n",
    "- If few(e.g. <5) then don't use the weight, since not enough data to be accurate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood\n",
    "\n",
    "- In practice, don't sum over all users who rated movie j(takes too long)\n",
    "- It can help to precompute weights beforehand\n",
    "- Non-trivial: instead of summing over all users, take the ones with the highest weight\n",
    "- E.g. user K nearest neighbors, K = 25 up to 50\n",
    "\n",
    "\\begin{equation*}\n",
    "s(i,j) = \\overline{r}_{i} + \\frac{\\sum_{i'\\in \\Omega_{j}}^{}w_{ii'}(r_{i'j}-\\overline{r}_{i'})}{\\sum_{i'\\in \\Omega_{j}}^{}|w_{ii'}|}\n",
    "\\end{equation*}\n",
    "\n",
    "- Is it userful to keep negative weights? Yes A strong correlation is very negative - no correlation is zero\n",
    "- Thus, you might want to keep neighbors sorted on absolute correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "[kaggle movielens 20 million](https://www.kaggle.com/grouplens/movielens-20m-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "- If you're a company building a recommendation engine,you need to make recommendations for all N users\n",
    "- we need w(i,i') for i = 1,...,N -> $O(n^2)$\n",
    "- Including the time to calculate each correlation -> $O(N^2M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "- Suppose we have 100k = $10^5$ users\n",
    "- Then we need$(10^5)^2 = 10^10$ weights\n",
    "- 10 billion\n",
    "- we are reaching the limits of capacity\n",
    "- Not just size, but time required to calculate $O(N^2)$ is slow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestion: Work on a subset of data\n",
    "- What I did\n",
    "- Take a top n users and top m movies (n < N, m <M)\n",
    "- Top user = user who has rated the most movies\n",
    "- Top movie = movie that has been rated the most times\n",
    "- Yields a more dense matrix\n",
    "- Experiment to determine a workable value for n and m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation for a single user\n",
    "- Naive calculation\n",
    "- Calculate weights between this user and all users: O(MN)\n",
    "- Calculate scores s(i,j) for all items: also O(MN)\n",
    "    - M items\n",
    "    - Sum over N terms for each item\n",
    "- Sort the scores: O(MlogM)\n",
    "- Total: O(MN)+ O(MlogM)\n",
    "- This is theoretical- practically, you can make things faster\n",
    "    - Precomputing user-user weights\n",
    "    - For score, only sum using users similar to me- if top K, then O(MK)\n",
    "    - Sorting, if keep only L items, then O(MLogL) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Precomputing\n",
    " - Precompute recommendations using big data technologies as offline jobs\n",
    " - You don't want to do an O(MlogM) sort in real time - even O(M) would be bad\n",
    " - Use a cronjob to run your task everyday at some time, e.g. 3pm\n",
    " - Your API can simply retreive items already stored in a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User IDs\n",
    "- rating.csv  \n",
    "- User IDs from 1... 100k\n",
    "- IDs index a Numpy array, so we want them to start from 0\n",
    "- also want to make sure we utilize all space in the array\n",
    "- so we don't want the max User IDs to be 100k, but only have 500 users\n",
    "- The only step is to subtract 1 from each ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie IDs\n",
    "- go from 1 ... 100k\n",
    "- But they are not sequential\n",
    "- only ~20k movies\n",
    "- for these, we should make a new mapping that goes from 0 ~20k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinking the data\n",
    "\n",
    "- Full dataset is too large to perform $O(N^2)$ algorithm\n",
    "- User-User CF won't scale on a single machine\n",
    "- If you're an expert at big data(Spark), you can write a distributed job\n",
    "- preprocessing_shrink.py\n",
    "    - select subset of users and movies\n",
    "    - Users who rated the most movies\n",
    "    - Movies who've been rated by the most users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df = pd.read_csv('../large_files/movielens-20m-dataset/edited_rating.csv')\n",
    "print(\"original dataframe size:\", len(df))\n",
    "\n",
    "N = df.userId.max() + 1 # number of users\n",
    "M = df.movie_idx.max() + 1 # number of movies\n",
    "\n",
    "user_ids_count = Counter(df.userId)\n",
    "movie_ids_count = Counter(df.movie_idx)\n",
    "\n",
    "# number of users and movies we would like to keep\n",
    "n = 10000\n",
    "m = 2000\n",
    "\n",
    "user_ids = [u for u, c in user_ids_count.most_common(n)]\n",
    "movie_ids = [m for m, c in movie_ids_count.most_common(m)]\n",
    "\n",
    "# make a copy, otherwise ids won't be overwritten\n",
    "df_small = df[df.userId.isin(user_ids) & df.movie_idx.isin(movie_ids)].copy()\n",
    "# need to remake user ids and movie ids since they are no longer sequential\n",
    "new_user_id_map = {}\n",
    "i = 0\n",
    "for old in user_ids:\n",
    "  new_user_id_map[old] = i\n",
    "  i += 1\n",
    "print(\"i:\", i)\n",
    "\n",
    "new_movie_id_map = {}\n",
    "j = 0\n",
    "for old in movie_ids:\n",
    "  new_movie_id_map[old] = j\n",
    "  j += 1\n",
    "print(\"j:\", j)\n",
    "\n",
    "print(\"Setting new ids\")\n",
    "df_small.loc[:, 'userId'] = df_small.apply(lambda row: new_user_id_map[row.userId], axis=1)\n",
    "df_small.loc[:, 'movie_idx'] = df_small.apply(lambda row: new_movie_id_map[row.movie_idx], axis=1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table to Dictionary\n",
    "- In code, I want to ask questions like\n",
    "    - given user i, which movies j did they rate?\n",
    "    - given movie j, which users i have rated it?\n",
    "    - given user i and movie j, what is the rating?\n",
    "- Pandas is like SQL table, so we should be able to write \"queries\" to grab this info?\n",
    "- Python dictionaries are already a key ,value lookup\n",
    "    - user2movie: user ID -> movie ID\n",
    "    - movie2user : movie ID -> user ID\n",
    "    - usermovie2rating: (user ID, movie ID) -> rating\n",
    "    \n",
    "- looping through the array would be O(NM)\n",
    "- looping through dictionary O(|$\\Omega$|)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) User User Collaborative Filtering in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open('user2movie.json', 'rb') as f:\n",
    "  user2movie = pickle.load(f)\n",
    "\n",
    "with open('movie2user.json', 'rb') as f:\n",
    "  movie2user = pickle.load(f)\n",
    "\n",
    "with open('usermovie2rating.json', 'rb') as f:\n",
    "  usermovie2rating = pickle.load(f)\n",
    "\n",
    "with open('usermovie2rating_test.json', 'rb') as f:\n",
    "  usermovie2rating_test = pickle.load(f)\n",
    "\n",
    "N = np.max(list(user2movie.keys())) + 1\n",
    "# the test set may contain movies the train set doesn't have data on\n",
    "m1 = np.max(list(movie2user.keys()))\n",
    "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
    "M = max(m1, m2) + 1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "s(i,j) = \\overline{r}_{i} + \\frac{\\sum_{i'\\in \\Omega_{j}}^{}w_{ii'}(r_{i'j}-\\overline{r}_{i'})}{\\sum_{i'\\in \\Omega_{j}}^{}|w_{ii'}|}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "```python\n",
    "K = 25 # number of neighbors we'd like to consider\n",
    "limit = 5 # number of common movies users must have in common in order to consider\n",
    "neighbors = [] # store neighbors in this list\n",
    "averages = [] # each user's average rating for later use\n",
    "deviations = [] # each user's deviation for later use\n",
    "for i in range(N):\n",
    "  # find the 25 closest users to user i\n",
    "  movies_i = user2movie[i]\n",
    "  movies_i_set = set(movies_i)\n",
    "  \n",
    "  # calculate avg and deviation\n",
    "  ratings_i = { movie:usermovie2rating[(i, movie)] for movie in movies_i }\n",
    "  avg_i = np.mean(list(ratings_i.values()))\n",
    "  dev_i = { movie:(rating - avg_i) for movie, rating in ratings_i.items() }\n",
    "  dev_i_values = np.array(list(dev_i.values()))\n",
    "  sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
    "    \n",
    "  ...\n",
    "  # save these for later use\n",
    "  averages.append(avg_i)\n",
    "  deviations.append(dev_i)\n",
    "\n",
    "  sl = SortedList()\n",
    "  for j in range(N):\n",
    "    # don't include yourself\n",
    "    if j != i:\n",
    "      movies_j = user2movie[j]\n",
    "      movies_j_set = set(movies_j)\n",
    "      common_movies = (movies_i_set & movies_j_set) # intersection\n",
    "      if len(common_movies) > limit:\n",
    "        # calculate avg and deviation\n",
    "        ratings_j = { movie:usermovie2rating[(j, movie)] for movie in movies_j }\n",
    "        avg_j = np.mean(list(ratings_j.values()))\n",
    "        dev_j = { movie:(rating - avg_j) for movie, rating in ratings_j.items() }\n",
    "        dev_j_values = np.array(list(dev_j.values()))\n",
    "        sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
    "\n",
    "        # calculate correlation coefficient\n",
    "        numerator = sum(dev_i[m]*dev_j[m] for m in common_movies)\n",
    "        w_ij = numerator / (sigma_i * sigma_j)\n",
    "\n",
    "        # insert into sorted list and truncate\n",
    "        # negate weight, because list is sorted ascending\n",
    "        # maximum value (1) is \"closest\"\n",
    "        sl.add((-w_ij, j))\n",
    "        if len(sl) > K:\n",
    "          del sl[-1]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Item - Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap about User User Collaborative Filtering\n",
    "- For user user CF, I want to find users like me\n",
    "- The movie that those users have seen, that I haven't seen, become my recommendations\n",
    "- It's intuitive that if they are like me, I would like movies they've rated highly\n",
    "- 2 users are similar of their row vectors have small distance between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Item \n",
    "- What if we looked column-wise instead?\n",
    "- Let's find 2 products that are similar\n",
    "- They are similar if their column vectors' distance is small\n",
    "\n",
    "![](https://takuti.github.io/Recommendation.jl/latest/assets/images/cf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "- The correlation between the column vectors is high\n",
    "- If you like Power Rangers, you'll also like Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " []() |Power Rangers  | Transformers| Ninja Turtles|\n",
    "--|--|--|--|\n",
    "Alice|4.5|5|4|\n",
    "Bob|5|5|4.5 |\n",
    "Carol|2|2|0.5|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Correlation\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{jj'} = \\frac{\\sum_{i\\in \\Psi_{jj'}}^{}(r_{ij}-\\overline{r}_{j})-(r_{ij'}-\\overline{r}_{j'})}{\\sqrt{\\sum_{i\\in \\Psi_{jj'}}^{}(r_{ij}-\\overline{r}_{j})^2} \\sqrt{\\sum_{i \\in \\Psi_{jj'}}^{}(r_{ij'}-\\overline{r}_{j'})^2}}  \n",
    "\\end{equation*}  \n",
    "\n",
    "$\\Omega_{j} = $ users who rated item j   \n",
    "$\\Omega_{jj'} = $ users who rated item j and item j'   \n",
    "$\\overline{r}_{j} = $ average rating for item j  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Score\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "s(i,j) = \\overline{r}_{j} + \\frac{\\sum_{j'\\in \\Psi_{i}}^{}w_{jj'}(r_{ij'}-\\overline{r}_{j'})}{\\sum_{j'\\in \\Psi_{i}}^{}|w_{jj'}|}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\Psi_{i}$ = items user i has rated\n",
    "\n",
    "- Deviation : how much user i likes item j': compared to how much everyone else likes j'(IMO, not as intuitive as user user CF)\n",
    "- if user i really likes j' and j is similar to j'($w_{jj'}$ is high), then user i probably likes j too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "- User User CF: choose items for a user, because items have been liked by similar users\n",
    "- Item Item CF: choose items for a user, because this user has liked similar items in the past\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Prespective\n",
    "- pretend items are people, so they have feelings\n",
    "- Flip the user-item matrix sideways\n",
    "- Each entry tells me how much item j likes user i\n",
    "- To choose a user to recommend to item j, I can look at other items j' who liked the same users as item j\n",
    "- If item j and item j' are similar, then they like the same users\n",
    "- User-based and Item-based CF are mathmatically identical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical differences\n",
    "\n",
    "- When comparing 2 items, you have a lot more data than when comparing 2 users\n",
    "    - Each user: up to ~20k items to look at\n",
    "    - Each item: up to ~100k users to look at\n",
    "    - Thus for item item cf , weights are calculated based on more data\n",
    "- Item Item CF is faster\n",
    "    - given a user, calculate scores for each item: O($M^2N$)\n",
    "        - there are $M^2$ item item weights, and each vector is length N\n",
    "    - for user based CF, we saw $O(N^2M)$\n",
    "    - N >> M , so $N^2$ compared to $M^2$ is even worse\n",
    "    \n",
    "- Item based CF is more accurate    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Item - Item Collaborative Filtering in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open('user2movie.json', 'rb') as f:\n",
    "  user2movie = pickle.load(f)\n",
    "\n",
    "with open('movie2user.json', 'rb') as f:\n",
    "  movie2user = pickle.load(f)\n",
    "\n",
    "with open('usermovie2rating.json', 'rb') as f:\n",
    "  usermovie2rating = pickle.load(f)\n",
    "\n",
    "with open('usermovie2rating_test.json', 'rb') as f:\n",
    "  usermovie2rating_test = pickle.load(f)\n",
    "\n",
    "K = 20 # number of neighbors we'd like to consider\n",
    "limit = 5 # number of common movies users must have in common in order to consider\n",
    "neighbors = [] # store neighbors in this list\n",
    "averages = [] # each item's average rating for later use\n",
    "deviations = [] # each item's deviation for later use\n",
    "\n",
    "for i in range(M):# Item M\n",
    "  # find the K closest items to item i\n",
    "  users_i = movie2user[i]\n",
    "  users_i_set = set(users_i)\n",
    "\n",
    "  # calculate avg and deviation\n",
    "  ratings_i = { user:usermovie2rating[(user, i)] for user in users_i }\n",
    "  avg_i = np.mean(list(ratings_i.values()))\n",
    "  dev_i = { user:(rating - avg_i) for user, rating in ratings_i.items() }\n",
    "  dev_i_values = np.array(list(dev_i.values()))\n",
    "  sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
    "\n",
    "  # save these for later use\n",
    "  averages.append(avg_i)\n",
    "  deviations.append(dev_i)\n",
    "\n",
    "  sl = SortedList()\n",
    "  for j in range(M):\n",
    "    # don't include yourself\n",
    "    if j != i:\n",
    "      users_j = movie2user[j]\n",
    "      users_j_set = set(users_j)\n",
    "      common_users = (users_i_set & users_j_set) # intersection\n",
    "      if len(common_users) > limit:\n",
    "        # calculate avg and deviation\n",
    "        ratings_j = { user:usermovie2rating[(user, j)] for user in users_j }\n",
    "        avg_j = np.mean(list(ratings_j.values()))\n",
    "        dev_j = { user:(rating - avg_j) for user, rating in ratings_j.items() }\n",
    "        dev_j_values = np.array(list(dev_j.values()))\n",
    "        sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
    "\n",
    "        # calculate correlation coefficient\n",
    "        numerator = sum(dev_i[m]*dev_j[m] for m in common_users)\n",
    "        w_ij = numerator / (sigma_i * sigma_j)\n",
    "\n",
    "        # insert into sorted list and truncate\n",
    "        # negate weight, because list is sorted ascending\n",
    "        # maximum value (1) is \"closest\"\n",
    "        sl.add((-w_ij, j))\n",
    "        if len(sl) > K:\n",
    "          del sl[-1]\n",
    "\n",
    "  # store the neighbors\n",
    "  neighbors.append(sl)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Collaborative Filtering Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section summary\n",
    "- previously, we considered s(j) only - a single score for each item regardless of which user is looking\n",
    "- This section: s(i,j) score depends on user i and item j\n",
    "- Problem with average rating\n",
    "    - not all rating should be treated equally\n",
    "    - User who I agree with should be weighted higher\n",
    "    - Users I disagree with should be weighted lower\n",
    "- Pearson correlation as weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deviations\n",
    "- scores as deviation (how much better a user likes an item compared to how they normally feel) \n",
    "\n",
    "- Looking at this equation, we can see that it's just linear regression\n",
    "- Gives us yet another alternative to Pearson correlation\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{d}(i,j) = \\sum_{i'\\in \\Omega_{j}}^{} w_{ii'}d(i',j)\n",
    "\\end{equation*}\n",
    "\n",
    "$\\hat{d}(i,j)$ is Output prediction  \n",
    "$d(i',j)$ is input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User User and Item Item\n",
    "\n",
    "- Item based CF is more accurate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
