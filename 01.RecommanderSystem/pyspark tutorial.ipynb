{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/databricks/spark-training/master/website/img/matrix_factorization.png))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"spark-env\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark import SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('recommend').getOrCreate()\n",
    "\n",
    "data = spark.read.csv('./data/movielens-20m-dataset/rating.csv', inferSchema=True, header=True)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     0|      0|     0|        0|\n",
      "+------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count null value\n",
    "from pyspark.sql.functions import col,sum\n",
    "data.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+\n",
      "|summary|           userId|           movieId|            rating|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "|  count|         20000263|          20000263|          20000263|\n",
      "|   mean|69045.87258292554| 9041.567330339605|3.5255285642993797|\n",
      "| stddev|40038.62665316145|19789.477445413166|1.0519889192942444|\n",
      "|    min|                1|                 1|               0.5|\n",
      "|    max|           138493|            131262|               5.0|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count Null value\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "rows = data.count()\n",
    "summary = data.describe()\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      2|   3.5|2005-04-02 23:53:47|\n",
      "|     1|     29|   3.5|2005-04-02 23:31:16|\n",
      "|     1|     32|   3.5|2005-04-02 23:33:39|\n",
      "|     1|     47|   3.5|2005-04-02 23:32:07|\n",
      "|     1|     50|   3.5|2005-04-02 23:29:40|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset to train and test\n",
    "# train_data, test_data = data.randomSplit([0.8, 0.2])\n",
    "train_data, test_data = data.randomSplit([0.1, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares (ALS)\n",
    "\n",
    "- Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by using Alternating Least Squares. The implementation in MLlib has these parameters:\n",
    "\n",
    "    - numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n",
    "    - rank is the number of latent factors in the model.\n",
    "    - iterations is the number of iterations to run.\n",
    "    - lambda specifies the regularization parameter in ALS.\n",
    "    - implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.\n",
    "    - alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=10, regParam=0.1, rank=8, nonnegative=True, coldStartStrategy=\"drop\",\\\n",
    "          userCol='userId', itemCol='movieId', ratingCol='rating')\n",
    "model = als.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorized user matrix with rank = 8\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|[0.5212919, 1.131...|\n",
      "| 20|[0.0, 0.001707591...|\n",
      "| 30|[0.1865501, 0.0, ...|\n",
      "| 40|[0.0, 0.16109952,...|\n",
      "| 50|[1.0250514, 0.901...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------------------------------\n",
      "Factorized item matrix with rank = 8\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|[0.72092277, 0.33...|\n",
      "| 20|[0.020011581, 0.3...|\n",
      "| 30|[0.059172858, 0.1...|\n",
      "| 40|[0.61517817, 0.21...|\n",
      "| 50|[0.37421164, 0.88...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Factorized user matrix with rank = %d' % model.rank)\n",
    "model.userFactors.show(5)\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print('Factorized item matrix with rank = %d' % model.rank)\n",
    "model.itemFactors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended top users (e.g. 1 top user) for all items with the corresponding predicted ratings:\n",
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|   1580| [[96311, 6.142189]]|\n",
      "|   4900| [[25990, 5.686511]]|\n",
      "|   5300| [[50025, 5.850458]]|\n",
      "|   6620|[[129644, 5.50455...|\n",
      "|   7240|[[26606, 5.0060287]]|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------------------------------\n",
      "Recommended top items (e.g. 1 top item) for all users with the corresponding predicted ratings:\n",
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   148|[[101425, 5.76914...|\n",
      "|   463|[[101425, 5.89643...|\n",
      "|   471|[[101425, 5.59479...|\n",
      "|   496|[[101425, 7.220426]]|\n",
      "|   833|[[74754, 4.4274783]]|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Recommended top users (e.g. 1 top user) for all items with the corresponding predicted ratings:')\n",
    "model.recommendForAllItems(1).show(5)\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print('Recommended top items (e.g. 1 top item) for all users with the corresponding predicted ratings:')\n",
    "model.recommendForAllUsers(1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+----------+\n",
      "|userId|movieId|rating|          timestamp|prediction|\n",
      "+------+-------+------+-------------------+----------+\n",
      "| 74757|    148|   3.5|2003-09-29 16:35:35|  2.599261|\n",
      "| 96393|    148|   3.0|2000-09-28 19:41:30| 3.1067977|\n",
      "| 53338|    148|   1.0|1996-06-09 11:30:25| 2.5434332|\n",
      "| 22684|    148|   4.0|1996-05-14 07:10:00| 2.7635562|\n",
      "| 97435|    148|   4.0|2003-01-13 18:48:42| 3.0464308|\n",
      "|137949|    148|   4.0|2000-02-18 21:37:43| 3.0111864|\n",
      "| 19067|    148|   2.0|1996-05-30 19:07:44| 1.2519557|\n",
      "| 87301|    148|   2.0|2000-11-23 02:05:35| 3.3392105|\n",
      "| 88527|    148|   2.0|2000-08-07 14:48:44|  2.563173|\n",
      "|108726|    148|   3.0|2000-01-25 20:23:13| 2.0031128|\n",
      "| 92852|    148|   3.0|1996-08-12 01:23:51| 2.6814404|\n",
      "|123246|    148|   3.0|1996-05-25 09:37:36|  2.161244|\n",
      "| 20132|    148|   3.0|2002-05-19 02:36:33| 2.3097687|\n",
      "| 22884|    148|   3.0|1999-12-11 21:31:08|  2.385243|\n",
      "| 96427|    148|   3.0|1997-04-03 23:47:22|  3.200615|\n",
      "| 10303|    148|   3.0|1999-10-25 13:16:01| 2.7765472|\n",
      "| 36821|    148|   4.0|2001-01-13 06:33:18|  3.210737|\n",
      "| 83090|    148|   2.0|2002-08-26 22:20:25|  1.645154|\n",
      "| 44979|    148|   3.0|1996-04-29 11:43:40| 2.6323419|\n",
      "| 91782|    148|   3.0|1996-10-27 08:58:12|  2.587711|\n",
      "+------+-------+------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let see how the model perform\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the predictions\n",
    "\n",
    "- Evaluate the model by computing the RMSE on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error of the test_data: 0.9222\n"
     ]
    }
   ],
   "source": [
    "# check the root mean squared error\n",
    "evaluator = RegressionEvaluator(metricName='rmse', predictionCol='prediction', labelCol='rating')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('Root mean squared error of the test_data: %.4f' % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|    11|    253|   4.5|2009-01-02 01:16:36|\n",
      "|    11|    441|   1.5|2009-01-01 23:52:42|\n",
      "|    11|    500|   4.5|2009-01-02 01:15:49|\n",
      "|    11|    616|   4.0|2009-01-01 04:02:22|\n",
      "|    11|    631|   2.0|2009-01-01 04:32:18|\n",
      "|    11|    741|   4.5|2009-01-01 05:44:19|\n",
      "|    11|    858|   2.5|2009-01-02 01:15:24|\n",
      "|    11|   1210|   4.5|2009-01-02 01:13:45|\n",
      "|    11|   1291|   4.5|2009-01-02 01:16:41|\n",
      "|    11|   1527|   4.5|2009-01-02 01:17:56|\n",
      "|    11|   1688|   3.0|2009-01-01 04:44:35|\n",
      "|    11|   2011|   5.0|2009-01-01 04:58:45|\n",
      "|    11|   2134|   5.0|2009-01-01 05:56:38|\n",
      "|    11|   2378|   3.0|2009-01-02 00:24:51|\n",
      "|    11|   2688|   3.5|2009-01-01 04:04:16|\n",
      "|    11|   2717|   4.5|2009-01-01 05:35:10|\n",
      "|    11|   2793|   5.0|2009-01-01 04:42:38|\n",
      "|    11|   2916|   5.0|2009-01-01 05:47:50|\n",
      "|    11|   3702|   4.0|2009-01-01 05:45:26|\n",
      "|    11|   3897|   4.0|2009-01-01 04:33:32|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see historical rating of the user\n",
    "user_history = train_data.filter(train_data['userId']==11)\n",
    "user_history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|userId|\n",
      "+-------+------+\n",
      "|      1|    11|\n",
      "|     10|    11|\n",
      "|     19|    11|\n",
      "|     32|    11|\n",
      "|     39|    11|\n",
      "|     65|    11|\n",
      "|    110|    11|\n",
      "|    145|    11|\n",
      "|    150|    11|\n",
      "|    153|    11|\n",
      "|    158|    11|\n",
      "|    160|    11|\n",
      "|    165|    11|\n",
      "|    170|    11|\n",
      "|    172|    11|\n",
      "|    173|    11|\n",
      "|    185|    11|\n",
      "|    208|    11|\n",
      "|    231|    11|\n",
      "|    256|    11|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a list of movies we are thinking to offer\n",
    "user_suggest = test_data.filter(train_data['userId']==11).select(['movieId', 'userId'])\n",
    "user_suggest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|movieId|userId|prediction|\n",
      "+-------+------+----------+\n",
      "|   2571|    11| 4.8414764|\n",
      "|   2959|    11|  4.773497|\n",
      "|  59315|    11| 4.6840982|\n",
      "|  58559|    11|  4.678283|\n",
      "|   1196|    11|   4.65102|\n",
      "|  64034|    11| 4.6153884|\n",
      "|    318|    11| 4.6108084|\n",
      "|  33794|    11| 4.5910625|\n",
      "|   1198|    11| 4.5810275|\n",
      "|  60069|    11| 4.5555177|\n",
      "|   6539|    11|   4.54848|\n",
      "|    356|    11| 4.5424957|\n",
      "|   1036|    11|  4.539179|\n",
      "|  51662|    11|  4.516956|\n",
      "|   5903|    11| 4.5067563|\n",
      "|   7254|    11|  4.504723|\n",
      "|   2762|    11| 4.5040073|\n",
      "|    589|    11| 4.4953265|\n",
      "|    260|    11|  4.486622|\n",
      "|    593|    11|  4.484823|\n",
      "+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# offer movies with a high predicted rating\n",
    "user_offer = model.transform(user_suggest)\n",
    "user_offer.orderBy('prediction', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[spark-dataframe-and-operations](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)\n",
    "\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2016/10/DataFrame-in-Spark.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
