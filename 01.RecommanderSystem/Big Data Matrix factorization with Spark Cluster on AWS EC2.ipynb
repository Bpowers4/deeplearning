{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data and Spark Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our data is large\n",
    "- MovieLens 100k - that's tiny\n",
    "- we use 20M-> 200x larger\n",
    "- Does it compare to what large corporation deal with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Netflix\n",
    "    - 100 million users\n",
    "    - we have ~130k users\n",
    "    - 15k titles(hard to find data)\n",
    "    - we have 26k movies\n",
    "    - 500k movie exist(not including TV shows)\n",
    "- YouTube\n",
    "    - 1.8 billion users\n",
    "    - 7 billion videos\n",
    "    - unlike standard dataset, M>N\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "- Unforfunately a buzzword\n",
    "- I think of it as any technology that requires distributed computing\n",
    "- e.g. Database sharding\n",
    "    - User IDs 1-1000 on one machine\n",
    "    - User IDs 1001-2000 on another machine\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharding Files\n",
    "\n",
    "## Distributed Compute\n",
    "- Imagine having to do\n",
    "\n",
    "```python\n",
    "for i in range(N)\n",
    "```\n",
    "\n",
    "- but N is very large\n",
    "- can split the work across multiple nodes\n",
    "- what if each machine does a separate chunk of the loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you don't have to manually write code to pass data to worker machines\n",
    "- In the old days you'd have to, but these days we have more redundant,fault-tolerant libraries such as Hadoop and Spark\n",
    "- Main tip: Keep in mind how we modify data in a Pandas DataFrame\n",
    "\n",
    "```python\n",
    "def some_func(row):\n",
    "    out = do_something_to(row)\n",
    "    return out\n",
    "df['new_column'] = df.apply(some_func,axis = 1)\n",
    "result = map(some_func,some_sequence) # pure python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History Lesson\n",
    "- MapReduce was developed at Google\n",
    "- Hadoop(open-source version of MapReduce) was built at Yahoo, now part of Apache\n",
    "- Spark builds on top of Hadoop, makes MapReduce code easier to write \n",
    "- That's why Spark install comes with Hadoop\n",
    "- Hadoop is written in Java, thus you also have to install Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Online\n",
    "\n",
    "- Mostly learning about how Matrix Factorization in Spark API works\n",
    "- Most of the work is in setup\n",
    "- 2 methods\n",
    "    - 1) Local\n",
    "    - 2) Amazone EC2 cluster(more realistic)\n",
    "- If you are already working for a company doing big data, you probably already have a dedicated ops team for provisioning machines\n",
    "- Don't make your manager angry by accidentally spending thousands of dollars on EC2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Spark in your local Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Windows \n",
    "    - Ubuntu inside a VirtualBox\n",
    "- Mac\n",
    "    - get homebrew\n",
    "    - xcode-select --install\n",
    "    - install java\n",
    "    - brew cask install java\n",
    "    - brew install scala\n",
    "    - brew install apache-spark\n",
    "    - pip install pyspark\n",
    "    - spark-shell\n",
    "        - 1 + 2\n",
    "        - 3\n",
    "    - python\n",
    "- Ubuntu\n",
    "    - sudo apt update\n",
    "    - sudo apt install default-jdk\n",
    "    - sudo apt install scala\n",
    "    - pip install pyspark\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# pyspark.mllib is like scipy in pyspark version\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "import os\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "import os\n",
    "\n",
    "# load in the data\n",
    "data = sc.textFile(\"./01.RecommanderSystem/data/movielens-20m-dataset/rating.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# convert into a sequence of Rating objects\n",
    "ratings = data.map(\n",
    "  lambda l: l.split(',')\n",
    ").map(\n",
    "   # UserId,MovieId,Rating(1,1.5,3.5 5 and etc)\n",
    "  lambda l: Rating(int(l[0]), int(l[1]), float(l[2]))\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# split into train and test\n",
    "train, test = ratings.randomSplit([0.8, 0.2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# train the model\n",
    "# K is latent dimensionality\n",
    "K = 10\n",
    "epochs = 10\n",
    "model = ALS.train(train, K, epochs)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# train\n",
    "x = train.map(lambda p: (p[0], p[1]))\n",
    "p = model.predictAll(x).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = train.map(lambda r: ((r[0], r[1]), r[2])).join(p)\n",
    "# joins on first item: (user_id, movie_id)\n",
    "# each row of result is: ((user_id, movie_id), (rating, prediction))\n",
    "mse = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"train mse: %s\" % mse)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"spark-ntlk-env\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "import os\n",
    "\n",
    "# load in the data\n",
    "data = sc.textFile(\"./data/movielens-20m-dataset/rating.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into a sequence of Rating objects\n",
    "ratings = data.map(\n",
    "  lambda l: l.split(',')\n",
    ").map(\n",
    "  lambda l: Rating(int(l[0]), int(l[1]), float(l[2]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "train, test = ratings.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "K = 10\n",
    "epochs = 10\n",
    "model = ALS.train(train, K, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spark-submit --master spakr://localhost:7077 spark2.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://2.bp.blogspot.com/-72rQVVqBQ_c/VfCe_hwW23I/AAAAAAAAMYQ/BflO3g-Kd6c/s640/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "sc = SparkContext(\"local\", \"Your App Name Here\")\n",
    "\n",
    "\n",
    "# load in the data\n",
    "# data = sc.textFile(\"../large_files/movielens-20m-dataset/small_rating.csv\")\n",
    "data = sc.textFile(\"../large_files/movielens-20m-dataset/rating.csv.gz\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Spark on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0) Install Spark locally\n",
    "- 1) Create AWS key pair and credentials\n",
    "- 2) Clone the spark-ec2 tool from github\n",
    "- 3) Create a cluster\n",
    "- 4) Run our scrip,spark2.py\n",
    "- 5) Destroy cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# setup key pair\n",
    "\n",
    "![](https://docs.slashdb.com/user-guide/assets/keypairs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://buildkiteassets.com/assets/docs/tutorials/elastic_ci_stack_aws/ec2-create-key-pair-8f7684bec4b3d001b9e20633617548fa552b48734f59508f29e8b608978a7011.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the PEM in a safe place\n",
    "\n",
    "## Create Credential\n",
    "![](https://docs.aws.amazon.com/IAM/latest/UserGuide/images/security-credentials-user.shared.console.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cloud-gc.readthedocs.io/en/stable/_images/access_key_console.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables\n",
    "\n",
    "- export AWS_ACCESS_KEY_ID = ...\n",
    "- export AWS_SECRET_ACCESS_KEY = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clone spark-ec2 repo\n",
    "\n",
    "[spark-ec2 repo ](https://github.com/amplab/spark-ec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## launch your cluster\n",
    "- run from the folder you just cloned\n",
    "\n",
    "```python\n",
    "./spark-ec2 -k AWS2018-3 -i AWS2018-3.pem -s 5 --spot-price=0.04 launch rec-cluster2\n",
    "\n",
    "-k (--key-pair): name of your key pair\n",
    "-i (--identity-file): PEM file you downloaded\n",
    "-s (--slave): number of slave machines\n",
    "--spot-price:bid amount(in dollars)\n",
    "on-demand: expensive, guaranteed\n",
    "spot-pricing: cheap, bid enough to win, can be interrupted\n",
    "Default instance type is m3.large(2vCPUs, 7.5GB RAM)\n",
    "$0.03/h\n",
    "Might change in future since m3 is older generation, m4 is current\n",
    "can use --instance-type to change\n",
    "--region : specifying region\n",
    "rec-cluster2 is the cluster name\n",
    "\n",
    "chmod 400 AWSKey2018-2.pem\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy the URL of your master node\n",
    "\n",
    "- in AWS terminal, http://ec2_xxxxx-computexx.amazonaws.com:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code\n",
    "\n",
    "```python\n",
    "spark-submit --master spakr://ec2-xxxx-amazonaws.com:7077 spark2.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jave Heap memory error\n",
    "\n",
    "```python\n",
    "spark-submit --master spakr://ec2-xxxx-amazonaws.com:7077 spark2.py --executor-memory 10g --driver-memory 10g\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data files\n",
    "- data would be stored on HDFS or S3\n",
    "- latest spark has trouble with S3 protocol\n",
    "- if your company has HDFS, try it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destroy cluster\n",
    "\n",
    "```python\n",
    "./spark-ec2 destroy rec-cluster2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[pyspark in virtualenv](https://henning.kropponline.de/2016/09/17/running-pyspark-with-virtualenv/)  \n",
    "[movielens dataset](https://grouplens.org/datasets/movielens/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/databricks/spark-training/master/website/img/matrix_factorization.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
