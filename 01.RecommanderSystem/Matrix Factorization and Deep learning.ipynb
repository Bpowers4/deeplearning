{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://katbailey.github.io/images/matrix_factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rating matrix(NxM)\n",
    "- N = num users\n",
    "- M = num items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization \n",
    "- In supervised ML, we want accuracy(predictions close to target)\n",
    "- In recommenders, what we want is a score to sort recommendations by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Outline\n",
    "- Basic form of Matrix Factorization model\n",
    "- Define a loss, minimize it\n",
    "- 2 impl\n",
    "    - numpy - direct from theory\n",
    "    - Keras\n",
    "- Extend keras model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Matrix Factorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors\n",
    "- 10 = 5 x2 \n",
    "- 15 = 3 x 5\n",
    "- 30 = 3 x 10 = 15 x 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matirx Factorization \n",
    "\n",
    "- Split the matrix into the product of 2 other matrices\n",
    "- R hat is approximates R - it is our model of R\n",
    "![](https://www.kukuxiaai.com/images/blog/recommended_system/udemy/mf_1.png)\n",
    "\n",
    "- W( N x K) - user matrix, U (M x K) - movie matrix\n",
    "- K somewhere from 10 -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## think about R\n",
    "- W and U should be much smaller than R\n",
    "- R is N x M\n",
    "- represent it using a special data structure\n",
    "    - Dict{(u,m) -> r}\n",
    "- If N = 130k, M = 26k\n",
    "    - N x M = 3.38 billion\n",
    "    - ratings = 20 million\n",
    "    - space used: 20 million/3.38billion = 0.006\n",
    "- This is called a sparse representation\n",
    "\n",
    "![](https://4.bp.blogspot.com/-95QD5t9Lha4/Wd7uWnBZBeI/AAAAAAAADg4/xB4VnnxM0UgUp15lNmB3aHCXYGejpm4OACLcBGAs/s1600/matrix_factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some calculations\n",
    "- If k = 10, N = 130k, M = 26k, then size of W and U \n",
    "- NK + MK = 1.56 million\n",
    "- how much savings?\n",
    "- 1.56 million / 3.38 billion = 0.0005\n",
    "- this is good, we like # parameters < # of data pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens if you try to calculate W$U^T$ in code?\n",
    "- Don't do it, the result is NxM, which I just told you is exactly what we don't want\n",
    "    - Unless you've selected a small subset of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One rating\n",
    "- this is easy, just a dot product between 2 vectors of size K\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j}, \\hat{r}_{ij} = \\hat{R}[i,j] , w_{i} = W[i], u_{j} = U[j]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why dose it make sense?\n",
    "- From a mathmetical standpoint, we know from SVD(singular value decomposition) that a matrix X can be decomposed into 3 seperate matrices multiplied together\n",
    "\n",
    "- X(N x M), U(N x K), S(K x K) , V(M x K)\n",
    "- If I multiply U by S, I just get another N x K matrix\n",
    "    - Then X is a product of 2 matrices, just like matrix factorization\n",
    "    - or equivalently, I could combine S with $V^{T}$\n",
    "- R(rating matrix) is sparse\n",
    "    - If U,S and V can properly approximate a full X matrix, then surely it can approximate a mostly empty R matrix \n",
    "    \n",
    "\\begin{equation*}\n",
    "X = USV^{T}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/229f77d2cb173c1cef4d6cfbab2e905e/svd_matrices.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- Each of the K elements in $W_{i}$ and $u_{j}$ is a feature\n",
    "- Let's suppose K=5, and they are\n",
    "    - Action/adventure\n",
    "    - Comedy\n",
    "    - Romance\n",
    "    - Horror\n",
    "    - Animation\n",
    "- $w_{i}(1)$ is how much user i likes action\n",
    "- $w_{i}(2)$ is how much user i likes comedy \n",
    "- $u_{j}(1)$ is how much movie j contains action\n",
    "- $u_{j}(2)$ is how much movie j contains comedy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens when we dot $w_{i}^{T}u_{j}$ ?\n",
    "- How well do user i's preferences correlate with movie j's attributes?\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i}^{T}u_{j} = ||w_{i}||||u_{j}|| cos\\theta \\propto sim(i,j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "- Action/adventure\n",
    "- Comedy\n",
    "- Romance\n",
    "- Horror\n",
    "- Animation\n",
    "\n",
    "- $w_{i}$ = (1, 0.8, -1, 0.1, 1)\n",
    "- $u_{j}$ = (1,1.5,-1.3,0, 1.2)\n",
    "- result = 1 * 1 + 0.8 * 1.5 + 1 * 1.3 + 0.1 * 0 + 1 * 1.2 = 4.7 (too high)\n",
    "- Why?\n",
    "    - +ve x +ve -> +ve\n",
    "    - -ve x -ve -> +ve\n",
    "\n",
    "\n",
    "- $w_{i}$ = (1, 0.8, -1, 0.1, 1)\n",
    "- $u_{j}$ = (-1,-1,1,0, -1)\n",
    "- result = 1 * -1 + 0.8 * -1 + -1 * 1 + 0.1 * 0 + 1 * -1 = -3.8 (too low)\n",
    "- Why?\n",
    "    - +ve x -ve -> -ve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "- You can't choose feature 1 to be action, feature 2 to be comedy\n",
    "- Each feature is latent, and K is the latent dimensionality\n",
    "- Hidden causes\n",
    "- Why user i like Power rangers?\n",
    "    - hidden cause is that user i likes action, and Power rangers has action\n",
    "- We don't know the meaning of any feature without inspecting it\n",
    "- Ex. check top 10 movies that have the largest value for feature 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised machine learning\n",
    "\n",
    "- recall our previous discussion, we could predict how much a user likes an item, by extracting features from both, and feeding it into a model like random forest or neural network\n",
    "- the difference is that Matrix Fatorization extracts the features automatically using only ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "![](https://www.kukuxiaai.com/images/blog/recommended_system/udemy/mf_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we ensure our approximation is good?\n",
    "\n",
    "$R \\approx \\hat{R} = WU^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared error loss\n",
    "\n",
    "\\begin{equation*}\n",
    "J = \\sum_{i,j\\in\\Omega}^{}(r_{ij}-\\hat{r}_{ij})^2 = \\sum_{i,j\\in\\Omega}^{}(r_{ij}-w_{i}^{T}u_{j})^2\n",
    "\\end{equation*}\n",
    "\n",
    "$\\Omega$ = set of pairs(i,j) where user i rated movie j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize the loss\n",
    "- How? Find the gradient, set it to 0, solve for the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for W\n",
    "\n",
    "- careful about which sets are being summed over\n",
    "- For J, we want to sum over all ratings\n",
    "- For a particular user vector $w_{i}$, we only care about movies that user rated \n",
    "- Try to isoloate $w_{i}$\n",
    "- it's stuck inside a dot product \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w_{i}} = 2 \\sum_{j\\in \\Psi_{i}}^{}(r_{ij} - w_{i}^{T}u_{j})(-u_{j}) = 0  \\\\\n",
    "\\sum_{j\\in\\Psi_{i}}^{}(w_{i}^{T}u_{j})u_{j} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j} \\\\\n",
    "\\sum_{j\\in\\Psi_{i}}^{}(u_{j}^{T}w_{i})u_{j} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "- scalar x vector = vector x scalar\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{j\\in\\Psi_{i}}^{}u_{j}(u_{j}^{T}w_{i}) = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "- drop the brackets\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T}w_{i} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "(\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})w_{i} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "- Now it's just Ax = b, which we know how to solve\n",
    "- x = np.linalg.solve(A,b)\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i} = (\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1} \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for U\n",
    "\n",
    "- symmetric in W and U, so the steps should be the same\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial u_{j}} = 2 \\sum_{i\\in \\Omega_{j}}^{}(r_{ij} - w_{i}^{T}u_{j})(-w_{i}) = 0  \\\\\n",
    "\\sum_{i\\in\\Omega_{j}}^{}(w_{i}^{T}u_{j})w_{i} = \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\\\\\n",
    "\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T}u_{j} = \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\\\\\n",
    "(\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})u_{j} = \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\\\\\n",
    "u_{j} = (\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})^{-1} \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-way dependency\n",
    "- solution for W depends on U\n",
    "- solution for U depends on W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Algorithm\n",
    " \n",
    " - W = randn(N,K) U = randn(M,K)\n",
    " - for t in range(T):\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i} = (\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1} \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j} \\\\\n",
    "u_{j} = (\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})^{-1} \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "- Does it matter which order you update in? it doesn't matter\n",
    "- Should you use the old values of W when updating U?\n",
    "    - Tends to go faster if you use the new values\n",
    "    - computationally, if you wanted to use the old values, you'd have to make a copy(very slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Matrix Factorization , Expanding our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Terms\n",
    "- It thus makes sense to add bias terms to the MF model\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j} + b_{i}+ c_{j}+ \\mu \\\\\n",
    "\\end{equation*}\n",
    "$b_{i}$ = user bias  \n",
    "$c_{j}$ = movie bias  \n",
    "$\\mu$ = global average  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\\begin{equation*}\n",
    "J = \\sum_{i,j\\in\\Omega}^{}(r_{ij}-\\hat{r}_{ij})^2 \\\\\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j}+ b_{i}+c_{j}+ \\mu\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for W\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w_{i}} = 2 \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}-c_{j}-\\mu)(-u_{j}) = 0 \\\\\n",
    "\\sum_{j\\in\\Psi_{i}}^{}(w_{i}^{T}u_{j})u_{j} = \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)u_{j} \\\\\n",
    "w_{i} = (\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1}  \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "## Solving for U\n",
    "\n",
    "\\begin{equation*}\n",
    "u_{j} = (\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})^{-1}  \\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)w_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "## Solving for b\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b_{i}} = 2 \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}-c_{j}-\\mu)(-1) = 0 \\\\\n",
    "b_{i} = \\frac{1}{|\\Psi_{i}|}\\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-c_{j}-\\mu)\n",
    "\\end{equation*}\n",
    "\n",
    "## Solving for c\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial c_{j}} = 2 \\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}-c_{j}-\\mu)(-1) = 0 \\\\\n",
    "c_{j} = \\frac{1}{|\\Omega_{j}|}\\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-w_{i}^{T}u_{j}-c_{j}-\\mu)\n",
    "\\end{equation*}\n",
    "\n",
    "- Don't need to update global average(just calculate it directlry from train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Matrix Factorization ,Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "- A technique to prevent overfitting and help generalization\n",
    "- In linear regression\n",
    "- Model $\\hat{y} = w^{T}x$\n",
    "- Objective $J = \\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i})^2 +\\lambda||w||_{2}^{2}$\n",
    "- Solution $ w = (\\lambda I + X^{T}X)^{-1}X^{T}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Matrix Factorization\n",
    "- Same approach, add squared magnitude of each parameter multiplied by regularization constant\n",
    "- $||*||_{F}$ is called the Frobenius norm\n",
    "\n",
    "$J = \\sum_{i,j\\in \\Omega}^{}(r_{ij}-\\hat{r}_{ij})^2 +\\lambda(||W||_{F}^{2}+||U||_{F}^{2}+||b||_{2}^{2}+||c||_{2}^{2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for W\n",
    "- Derivatives are additive, we just need to differentiate the 2nd term and add it to the existing derivative\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w_{i}} = 2\\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-W_{i}^{T}u_{j}- b_{i}-c_{j}-\\mu)(-u_{j})+2\\lambda w_{i} = 0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you can't see how I differentiated $w_{i}$ wrt Frobenius Norm, expand it\n",
    "- Now it's just a dot product which we know how to differentiate\n",
    "\n",
    "\\begin{equation*}\n",
    "||W||_{F}^{2} = \\sum_{i=1}^{N}\\sum_{k=1}^{K}|w_{ik}|^{2} = \\sum_{i=1}^{N}||w_{i}||_{2}^{2}= \\sum_{i=1}^{N}w_{i}^{T}w_{i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sum_{j\\in \\Psi_{i}}^{}u_{j}u_{j}^{T}w_{i}+ \\lambda w_{i} =\\sum_{j\\in \\Psi_{i}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)(u_{j}) \\\\\n",
    "(\\sum_{j\\in \\Psi_{i}}^{}u_{j}u_{j}^{T}+ \\lambda I) w_{i} =\\sum_{j\\in \\Psi_{i}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)u_{j} \\\\\n",
    "w_{i} = (\\sum_{j\\in \\Psi_{i}}^{}u_{j}u_{j}^{T}+ \\lambda I)^{-1}\\sum_{j\\in \\Psi_{i}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)u_{j}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for U \n",
    "\\begin{equation*}\n",
    "u_{j} = (\\sum_{i\\in \\Omega_{j}}^{}w_{i}w_{i}^{T}+ \\lambda I)^{-1}\\sum_{i\\in \\Omega_{j}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)w_{i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solev for b\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b_{i}} = 2\\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}- c_{j}-\\mu)(-1)+2\\lambda b_{i} = 0 \\\\\n",
    "\\sum_{j\\in \\Psi_{i}}^{}b_{i}+\\lambda b_{i} = \\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j} -c_{j}-\\mu) \\\\\n",
    "b_{i}((\\sum_{j\\in \\Psi_{i}}^{}1) + \\lambda) = \\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j} -c_{j}-\\mu) \\\\\n",
    "b_{i} = \\frac{1}{|\\Psi_{i}|+\\lambda}\\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j} -c_{j}-\\mu)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solev for c\n",
    "\n",
    "\\begin{equation*}\n",
    "c_{j} = \\frac{1}{|\\Omega_{j}|+\\lambda}\\sum_{i\\in \\Omega_{j}}^{}(r_{ij}-w_{i}^{T}u_{j} -b_{i}-\\mu)\n",
    "\\end{equation*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
