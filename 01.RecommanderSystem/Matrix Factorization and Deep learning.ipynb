{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://katbailey.github.io/images/matrix_factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rating matrix(NxM)\n",
    "- N = num users\n",
    "- M = num items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization \n",
    "- In supervised ML, we want accuracy(predictions close to target)\n",
    "- In recommenders, what we want is a score to sort recommendations by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Outline\n",
    "- Basic form of Matrix Factorization model\n",
    "- Define a loss, minimize it\n",
    "- 2 impl\n",
    "    - numpy - direct from theory\n",
    "    - Keras\n",
    "- Extend keras model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Matrix Factorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors\n",
    "- 10 = 5 x2 \n",
    "- 15 = 3 x 5\n",
    "- 30 = 3 x 10 = 15 x 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matirx Factorization \n",
    "\n",
    "- Split the matrix into the product of 2 other matrices\n",
    "- R hat is approximates R - it is our model of R\n",
    "![](https://www.kukuxiaai.com/images/blog/recommended_system/udemy/mf_1.png)\n",
    "\n",
    "- W( N x K) - user matrix, U (M x K) - movie matrix\n",
    "- K somewhere from 10 -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## think about R\n",
    "- W and U should be much smaller than R\n",
    "- R is N x M\n",
    "- represent it using a special data structure\n",
    "    - Dict{(u,m) -> r}\n",
    "- If N = 130k, M = 26k\n",
    "    - N x M = 3.38 billion\n",
    "    - ratings = 20 million\n",
    "    - space used: 20 million/3.38billion = 0.006\n",
    "- This is called a sparse representation\n",
    "\n",
    "![](https://4.bp.blogspot.com/-95QD5t9Lha4/Wd7uWnBZBeI/AAAAAAAADg4/xB4VnnxM0UgUp15lNmB3aHCXYGejpm4OACLcBGAs/s1600/matrix_factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some calculations\n",
    "- If k = 10, N = 130k, M = 26k, then size of W and U \n",
    "- NK + MK = 1.56 million\n",
    "- how much savings?\n",
    "- 1.56 million / 3.38 billion = 0.0005\n",
    "- this is good, we like # parameters < # of data pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens if you try to calculate W$U^T$ in code?\n",
    "- Don't do it, the result is NxM, which I just told you is exactly what we don't want\n",
    "    - Unless you've selected a small subset of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One rating\n",
    "- this is easy, just a dot product between 2 vectors of size K\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j}, \\hat{r}_{ij} = \\hat{R}[i,j] , w_{i} = W[i], u_{j} = U[j]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why dose it make sense?\n",
    "- From a mathmetical standpoint, we know from SVD(singular value decomposition) that a matrix X can be decomposed into 3 seperate matrices multiplied together\n",
    "\n",
    "- X(N x M), U(N x K), S(K x K) , V(M x K)\n",
    "- If I multiply U by S, I just get another N x K matrix\n",
    "    - Then X is a product of 2 matrices, just like matrix factorization\n",
    "    - or equivalently, I could combine S with $V^{T}$\n",
    "- R(rating matrix) is sparse\n",
    "    - If U,S and V can properly approximate a full X matrix, then surely it can approximate a mostly empty R matrix \n",
    "    \n",
    "\\begin{equation*}\n",
    "X = USV^{T}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/229f77d2cb173c1cef4d6cfbab2e905e/svd_matrices.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- Each of the K elements in $W_{i}$ and $u_{j}$ is a feature\n",
    "- Let's suppose K=5, and they are\n",
    "    - Action/adventure\n",
    "    - Comedy\n",
    "    - Romance\n",
    "    - Horror\n",
    "    - Animation\n",
    "- $w_{i}(1)$ is how much user i likes action\n",
    "- $w_{i}(2)$ is how much user i likes comedy \n",
    "- $u_{j}(1)$ is how much movie j contains action\n",
    "- $u_{j}(2)$ is how much movie j contains comedy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens when we dot $w_{i}^{T}u_{j}$ ?\n",
    "- How well do user i's preferences correlate with movie j's attributes?\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i}^{T}u_{j} = ||w_{i}||||u_{j}|| cos\\theta \\propto sim(i,j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "- Action/adventure\n",
    "- Comedy\n",
    "- Romance\n",
    "- Horror\n",
    "- Animation\n",
    "\n",
    "- $w_{i}$ = (1, 0.8, -1, 0.1, 1)\n",
    "- $u_{j}$ = (1,1.5,-1.3,0, 1.2)\n",
    "- result = 1 * 1 + 0.8 * 1.5 + 1 * 1.3 + 0.1 * 0 + 1 * 1.2 = 4.7 (too high)\n",
    "- Why?\n",
    "    - +ve x +ve -> +ve\n",
    "    - -ve x -ve -> +ve\n",
    "\n",
    "\n",
    "- $w_{i}$ = (1, 0.8, -1, 0.1, 1)\n",
    "- $u_{j}$ = (-1,-1,1,0, -1)\n",
    "- result = 1 * -1 + 0.8 * -1 + -1 * 1 + 0.1 * 0 + 1 * -1 = -3.8 (too low)\n",
    "- Why?\n",
    "    - +ve x -ve -> -ve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "- You can't choose feature 1 to be action, feature 2 to be comedy\n",
    "- Each feature is latent, and K is the latent dimensionality\n",
    "- Hidden causes\n",
    "- Why user i like Power rangers?\n",
    "    - hidden cause is that user i likes action, and Power rangers has action\n",
    "- We don't know the meaning of any feature without inspecting it\n",
    "- Ex. check top 10 movies that have the largest value for feature 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised machine learning\n",
    "\n",
    "- recall our previous discussion, we could predict how much a user likes an item, by extracting features from both, and feeding it into a model like random forest or neural network\n",
    "- the difference is that Matrix Fatorization extracts the features automatically using only ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "![](https://www.kukuxiaai.com/images/blog/recommended_system/udemy/mf_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we ensure our approximation is good?\n",
    "\n",
    "$R \\approx \\hat{R} = WU^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared error loss\n",
    "\n",
    "\\begin{equation*}\n",
    "J = \\sum_{i,j\\in\\Omega}^{}(r_{ij}-\\hat{r}_{ij})^2 = \\sum_{i,j\\in\\Omega}^{}(r_{ij}-w_{i}^{T}u_{j})^2\n",
    "\\end{equation*}\n",
    "\n",
    "$\\Omega$ = set of pairs(i,j) where user i rated movie j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize the loss\n",
    "- How? Find the gradient, set it to 0, solve for the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for W\n",
    "\n",
    "- careful about which sets are being summed over\n",
    "- For J, we want to sum over all ratings\n",
    "- For a particular user vector $w_{i}$, we only care about movies that user rated \n",
    "- Try to isoloate $w_{i}$\n",
    "- it's stuck inside a dot product \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w_{i}} = 2 \\sum_{j\\in \\Psi_{i}}^{}(r_{ij} - w_{i}^{T}u_{j})(-u_{j}) = 0  \\\\\n",
    "\\sum_{j\\in\\Psi_{i}}^{}(w_{i}^{T}u_{j})u_{j} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j} \\\\\n",
    "\\sum_{j\\in\\Psi_{i}}^{}(u_{j}^{T}w_{i})u_{j} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "- scalar x vector = vector x scalar\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{j\\in\\Psi_{i}}^{}u_{j}(u_{j}^{T}w_{i}) = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "- drop the brackets\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T}w_{i} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "(\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})w_{i} = \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "- Now it's just Ax = b, which we know how to solve\n",
    "- x = np.linalg.solve(A,b)\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i} = (\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1} \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for U\n",
    "\n",
    "- symmetric in W and U, so the steps should be the same\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial u_{j}} = 2 \\sum_{i\\in \\Omega_{j}}^{}(r_{ij} - w_{i}^{T}u_{j})(-w_{i}) = 0  \\\\\n",
    "\\sum_{i\\in\\Omega_{j}}^{}(w_{i}^{T}u_{j})w_{i} = \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\\\\\n",
    "\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T}u_{j} = \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\\\\\n",
    "(\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})u_{j} = \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\\\\\n",
    "u_{j} = (\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})^{-1} \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-way dependency\n",
    "- solution for W depends on U\n",
    "- solution for U depends on W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Algorithm\n",
    " \n",
    " - W = randn(N,K) U = randn(M,K)\n",
    " - for t in range(T):\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i} = (\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1} \\sum_{j\\in\\Psi_{i}}^{}r_{ij}u_{j} \\\\\n",
    "u_{j} = (\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})^{-1} \\sum_{i\\in\\Omega_{j}}^{}r_{ij}w_{i}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "- Does it matter which order you update in? it doesn't matter\n",
    "- Should you use the old values of W when updating U?\n",
    "    - Tends to go faster if you use the new values\n",
    "    - computationally, if you wanted to use the old values, you'd have to make a copy(very slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Matrix Factorization , Expanding our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Terms\n",
    "- It thus makes sense to add bias terms to the MF model\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j} + b_{i}+ c_{j}+ \\mu \\\\\n",
    "\\end{equation*}\n",
    "$b_{i}$ = user bias  \n",
    "$c_{j}$ = movie bias  \n",
    "$\\mu$ = global average  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\\begin{equation*}\n",
    "J = \\sum_{i,j\\in\\Omega}^{}(r_{ij}-\\hat{r}_{ij})^2 \\\\\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j}+ b_{i}+c_{j}+ \\mu\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for W\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w_{i}} = 2 \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}-c_{j}-\\mu)(-u_{j}) = 0 \\\\\n",
    "\\sum_{j\\in\\Psi_{i}}^{}(w_{i}^{T}u_{j})u_{j} = \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)u_{j} \\\\\n",
    "w_{i} = (\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1}  \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)u_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "## Solving for U\n",
    "\n",
    "\\begin{equation*}\n",
    "u_{j} = (\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})^{-1}  \\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)w_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "## Solving for b\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b_{i}} = 2 \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}-c_{j}-\\mu)(-1) = 0 \\\\\n",
    "b_{i} = \\frac{1}{|\\Psi_{i}|}\\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-c_{j}-\\mu)\n",
    "\\end{equation*}\n",
    "\n",
    "## Solving for c\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial c_{j}} = 2 \\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}-c_{j}-\\mu)(-1) = 0 \\\\\n",
    "c_{j} = \\frac{1}{|\\Omega_{j}|}\\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-w_{i}^{T}u_{j}-c_{j}-\\mu)\n",
    "\\end{equation*}\n",
    "\n",
    "- Don't need to update global average(just calculate it directlry from train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Matrix Factorization ,Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "- A technique to prevent overfitting and help generalization\n",
    "- In linear regression\n",
    "- Model $\\hat{y} = w^{T}x$\n",
    "- Objective $J = \\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i})^2 +\\lambda||w||_{2}^{2}$\n",
    "- Solution $ w = (\\lambda I + X^{T}X)^{-1}X^{T}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Matrix Factorization\n",
    "- Same approach, add squared magnitude of each parameter multiplied by regularization constant\n",
    "- $||*||_{F}$ is called the Frobenius norm\n",
    "\n",
    "$J = \\sum_{i,j\\in \\Omega}^{}(r_{ij}-\\hat{r}_{ij})^2 +\\lambda(||W||_{F}^{2}+||U||_{F}^{2}+||b||_{2}^{2}+||c||_{2}^{2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for W\n",
    "- Derivatives are additive, we just need to differentiate the 2nd term and add it to the existing derivative\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w_{i}} = 2\\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-W_{i}^{T}u_{j}- b_{i}-c_{j}-\\mu)(-u_{j})+2\\lambda w_{i} = 0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you can't see how I differentiated $w_{i}$ wrt Frobenius Norm, expand it\n",
    "- Now it's just a dot product which we know how to differentiate\n",
    "\n",
    "\\begin{equation*}\n",
    "||W||_{F}^{2} = \\sum_{i=1}^{N}\\sum_{k=1}^{K}|w_{ik}|^{2} = \\sum_{i=1}^{N}||w_{i}||_{2}^{2}= \\sum_{i=1}^{N}w_{i}^{T}w_{i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sum_{j\\in \\Psi_{i}}^{}u_{j}u_{j}^{T}w_{i}+ \\lambda w_{i} =\\sum_{j\\in \\Psi_{i}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)(u_{j}) \\\\\n",
    "(\\sum_{j\\in \\Psi_{i}}^{}u_{j}u_{j}^{T}+ \\lambda I) w_{i} =\\sum_{j\\in \\Psi_{i}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)u_{j} \\\\\n",
    "w_{i} = (\\sum_{j\\in \\Psi_{i}}^{}u_{j}u_{j}^{T}+ \\lambda I)^{-1}\\sum_{j\\in \\Psi_{i}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)u_{j}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for U \n",
    "\\begin{equation*}\n",
    "u_{j} = (\\sum_{i\\in \\Omega_{j}}^{}w_{i}w_{i}^{T}+ \\lambda I)^{-1}\\sum_{i\\in \\Omega_{j}}^{} (r_{ij}- b_{i}-c_{j}-\\mu)w_{i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solev for b\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b_{i}} = 2\\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-b_{i}- c_{j}-\\mu)(-1)+2\\lambda b_{i} = 0 \\\\\n",
    "\\sum_{j\\in \\Psi_{i}}^{}b_{i}+\\lambda b_{i} = \\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j} -c_{j}-\\mu) \\\\\n",
    "b_{i}((\\sum_{j\\in \\Psi_{i}}^{}1) + \\lambda) = \\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j} -c_{j}-\\mu) \\\\\n",
    "b_{i} = \\frac{1}{|\\Psi_{i}|+\\lambda}\\sum_{j\\in \\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j} -c_{j}-\\mu)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solev for c\n",
    "\n",
    "\\begin{equation*}\n",
    "c_{j} = \\frac{1}{|\\Omega_{j}|+\\lambda}\\sum_{i\\in \\Omega_{j}}^{}(r_{ij}-w_{i}^{T}u_{j} -b_{i}-\\mu)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Matrix Factorization in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# N is user number, M is number of movies\n",
    "N = np.max(list(user2movie.keys())) + 1\n",
    "# the test set may contain movies the train set doesn't have data on\n",
    "m1 = np.max(list(movie2user.keys()))\n",
    "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
    "M = max(m1, m2) + 1\n",
    "print(\"N:\", N, \"M:\", M)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{i}$ = user i  \n",
    "$u_{j}$ = movie j  \n",
    "$b_{i}$ = user bias  \n",
    "$c_{j}$ = movie bias  \n",
    "$\\mu$ = global average  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j} + b_{i}+ c_{j}+ \\mu \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "```python\n",
    "# initialize variables\n",
    "K = 10 # latent dimensionality\n",
    "W = np.random.randn(N, K)\n",
    "b = np.zeros(N)\n",
    "U = np.random.randn(M, K)\n",
    "c = np.zeros(M)\n",
    "mu = np.mean(list(usermovie2rating.values()))\n",
    "# prediction[i,j] = W[i].dot(U[j]) + b[i] + c.T[j] + mu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "J = \\sum_{i,j\\in\\Omega}^{}(r_{ij}-\\hat{r}_{ij})^2 \\\\\n",
    "\\hat{r}_{ij} = w_{i}^{T}u_{j}+ b_{i}+c_{j}+ \\mu\n",
    "\\end{equation*}\n",
    "\n",
    "```python\n",
    "def get_loss(d):\n",
    "  # d: (user_id, movie_id) -> rating\n",
    "  N = float(len(d))\n",
    "  sse = 0\n",
    "  for k, r in d.items():\n",
    "    i, j = k\n",
    "    p = W[i].dot(U[j]) + b[i] + c[j] + mu\n",
    "    sse += (p - r)*(p - r)\n",
    "  return sse / N\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "w_{i} = (\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1}  \\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)u_{j}\\\\\n",
    "b_{i} = \\frac{1}{|\\Psi_{i}|}\\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-w_{i}^{T}u_{j}-c_{j}-\\mu)\n",
    "\\end{equation*}\n",
    "\n",
    "$(\\sum_{j\\in\\Psi_{i}}^{}u_{j}u_{j}^{T})^{-1}$ is matrix  \n",
    "$\\sum_{j\\in\\Psi_{i}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)u_{j}$ is vector  \n",
    "Ax = b  \n",
    "\n",
    "```python\n",
    "for i in range(N):\n",
    "    # for W\n",
    "    matrix = np.eye(K) * reg\n",
    "    vector = np.zeros(K)\n",
    "\n",
    "    # for b\n",
    "    bi = 0\n",
    "    for j in user2movie[i]:\n",
    "      r = usermovie2rating[(i,j)]\n",
    "      matrix += np.outer(U[j], U[j])\n",
    "      vector += (r - b[i] - c[j] - mu)*U[j]\n",
    "      bi += (r - W[i].dot(U[j]) - c[j] - mu)\n",
    "\n",
    "    # set the updates\n",
    "    W[i] = np.linalg.solve(matrix, vector)\n",
    "    b[i] = bi / (len(user2movie[i]) + reg)\n",
    "```\n",
    "\n",
    "```python\n",
    "# another implementation\n",
    "for i in range(N):\n",
    "    m_ids, r = user2movierating[i]\n",
    "    matrix = U[m_ids].T.dot(U[m_ids]) + np.eye(K) * reg\n",
    "    vector = (r - b[i] - c[m_ids] - mu).dot(U[m_ids])\n",
    "    bi = (r - U[m_ids].dot(W[i]) - c[m_ids] - mu).sum()\n",
    "\n",
    "    # set the updates\n",
    "    W[i] = np.linalg.solve(matrix, vector)\n",
    "    b[i] = bi / (len(user2movie[i]) + reg)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "u_{j} = (\\sum_{i\\in\\Omega_{j}}^{}w_{i}w_{i}^{T})^{-1}  \\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-b_{i}-c_{j}-\\mu)w_{i}\\\\\n",
    "c_{j} = \\frac{1}{|\\Omega_{j}|}\\sum_{i\\in\\Omega_{j}}^{}(r_{ij}-w_{i}^{T}u_{j}-c_{j}-\\mu)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "```python\n",
    "# update U and c\n",
    "  t0 = datetime.now()\n",
    "  for j in range(M):\n",
    "    # for U\n",
    "    matrix = np.eye(K) * reg\n",
    "    vector = np.zeros(K)\n",
    "\n",
    "    # for c\n",
    "    cj = 0\n",
    "    try:\n",
    "      for i in movie2user[j]:\n",
    "        r = usermovie2rating[(i,j)]\n",
    "        matrix += np.outer(W[i], W[i])\n",
    "        vector += (r - b[i] - c[j] - mu)*W[i]\n",
    "        cj += (r - W[i].dot(U[j]) - b[i] - mu)\n",
    "\n",
    "      # set the updates\n",
    "      U[j] = np.linalg.solve(matrix, vector)\n",
    "      c[j] = cj / (len(movie2user[j]) + reg)\n",
    "\n",
    "      if j % (M//10) == 0:\n",
    "        print(\"j:\", j, \"M:\", M)\n",
    "    except KeyError:\n",
    "      # possible not to have any ratings for a movie\n",
    "      pass\n",
    "```\n",
    "\n",
    "```python\n",
    "# another implementation\n",
    "for j in range(M):\n",
    "    try:\n",
    "      u_ids, r = movie2userrating[j]\n",
    "      matrix = W[u_ids].T.dot(W[u_ids]) + np.eye(K) * reg\n",
    "      vector = (r - b[u_ids] - c[j] - mu).dot(W[u_ids])\n",
    "      cj = (r - W[u_ids].dot(U[j]) - b[u_ids] - mu).sum()\n",
    "\n",
    "      # set the updates\n",
    "      U[j] = np.linalg.solve(matrix, vector)\n",
    "      c[j] = cj / (len(movie2user[j]) + reg)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.w3resource.com/w3r_images/linear-algebra-image-exercise-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) SVD(Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "- X can be decomposed into a product of U,S,V\n",
    "- X(N x M),U(N x M), S(M x M) , V(M x M)\n",
    "- No immediate savings.. size(X) == size(U)\n",
    "\n",
    "$X=USV^{T}$\n",
    "\n",
    "![](http://slideplayer.com/slide/3900405/13/images/25/Singular+Value+Decomposition.jpg)\n",
    "![](https://image.slidesharecdn.com/4772391/95/building-a-recommendation-engine-an-example-of-a-product-recommendation-engine-21-728.jpg?cb=1279279430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD\n",
    "- U,S and V are ordered by importance\n",
    "- Thus, it's possible to cut off parts of U,S, and V to get the best possible low rank approximation of X\n",
    "- U(N x K), S(K x K), V(M x K) -> result is still N x M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD \n",
    "- What's the justification for calling matrix factorization \"SVD\"?\n",
    "- S can be absorbed into U or V, we get an (N x K)x(K x M) -> (N x M)\n",
    "- The model is thus exactly like MF  \n",
    "\\begin{equation*}\n",
    "\\hat{x}_{ij} = \\sum_{k}^{}u_{ik}s_{kk}v_{kj} = \\sum_{k}^{}(u_{ik}s_{kk})v_{kj} = \\sum_{k}^{}u'_{ik}v_{kj} = u_{i}'^{T}v_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "![](http://shakthydoss.com/wp-content/uploads/2014/06/Singular-value-decomposition.jpg)\n",
    "![](http://image.slideserve.com/244082/singular-value-decomposition-l.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But how does SVD actually work? How do we find U,S and V?\n",
    "- Hint: not by setting the gradient to 0\n",
    "- Rough outline\n",
    "    - Assume N > M , rank of X is M \n",
    "    - Find 2 matrices: $X_{T}X$(M x M) and $XX^{T}$(N x N)\n",
    "    - $X^{T}X$ is proportional to the covariance of X, if X has already been centered\n",
    "    - another motivation for bias term\n",
    "    \n",
    "- Find the eigenvalues and eigenvectors of $X^{T}X$ and $XX^{T}$    \n",
    "- There will be M of them, and they are the same for both matrices\n",
    "- Suppose we found all the eigenvalues( np.linalg.eigh)\n",
    "    - $\\lambda_{1},\\lambda_{2},...,\\lambda_{M}$\n",
    "- S is just a diagonal matrix of the square root of them\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "    \\sqrt{\\lambda_{1}} & 0 & ... & 0 \\\\\n",
    "    0 & \\sqrt{\\lambda_{2}} & ... & 0 \\\\\n",
    "    ... & ... & ... & ... \\\\\n",
    "    0 & 0 & ... & \\sqrt{\\lambda_{M}} \\\\\n",
    "    \\end{pmatrix}  \n",
    "$$\n",
    "\n",
    "- for each eigenvalue, there's a corresponding eigenvector\n",
    "$$\n",
    "(X^{T}X)v_{i} = \\lambda_{i}v_{i}\\\\\n",
    "(XX^{T})u_{i} = \\lambda_{i}u_{i}\\\\\n",
    "$$\n",
    "\n",
    "- Important result, v's are orthonormal, u's are orthonormal\n",
    "\n",
    "$$\n",
    "u_{i}^{T}u_{j} = 0, i \\neq j \\\\\n",
    "u_{i}^{T}u_{j} = 1 \\\\\n",
    "v_{i}^{T}v_{j} = 0, i \\neq j \\\\\n",
    "v_{i}^{T}v_{j} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "- SVD doesn't even work when X has missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Probabilistic Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with a probablistic model\n",
    "- The mean of R is $WU^{T}$\n",
    "\n",
    "$$\n",
    "R \\sim N(WU^{T},\\sigma^{2})\\\\\n",
    "r_{ij} \\sim N(w_{i}^{T}u_{j},\\sigma^{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing the likelihood\n",
    "\n",
    "- When we have a probability distribution, and we want to find the parameters of it, how can we do that?\n",
    "- Maximum likelihood estimation\n",
    "- collect data from the distribution, maximize the likelihood function wrt parameters\n",
    "\n",
    "$$\n",
    "L = \\prod_{i,j\\in \\Omega}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp(\\frac{-1}{2\\sigma^{2}} (r_{ij}-w_{i}^{T}u_{j})^{2})\\\\\n",
    "W,U = \\underset{W,U}{\\operatorname{argmax}}L\n",
    "$$\n",
    "\n",
    "- Usual next step: take the log of the likelihood\n",
    "- Maximizing this is usually easier\n",
    "- This is just the negative of our loss\n",
    "\n",
    "$$\n",
    "log(L) =C - \\prod_{i,j\\in \\Omega}\\frac{1}{2\\sigma^{2}}(r_{ij}-w_{i}^{T}u_{j})^{2}\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Estimation\n",
    "- Maximum a posterior estimation\n",
    "\n",
    "$$\n",
    "MLE: p(R|W,R) \\\\\n",
    "MAP :p(W,U|R)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Bayes rule\n",
    "\n",
    "$$\n",
    "p(W,U|R) = \\frac{p(R|W,U)p(W)p(U)}{p(R)}\n",
    "$$\n",
    "\n",
    "- Thus we must define the priors p(W), and p(U)\n",
    "- p(R) is a constant wrt W,U, can be dropped\n",
    "\n",
    "$$\n",
    "p(W) = N(0,\\lambda^{-1})\\\\\n",
    "p(U) = N(0,\\lambda^{-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Objective\n",
    "- we want to take the log of this\n",
    "\n",
    "$$\n",
    "L = \\prod_{i,j\\in \\Omega}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp(\\frac{-1}{2\\sigma^{2}} (r_{ij}-w_{i}^{T}u_{j})^{2}) \\times  \\frac{\\lambda}{\\sqrt{2\\pi}}e^{\\frac{-\\lambda}{2}||W||_{F}^{2}}\\frac{\\lambda}{\\sqrt{2\\pi}}e^{\\frac{-\\lambda}{2}||U||_{F}^{2}}\n",
    "$$\n",
    "\n",
    "- This is just regularized matrix factorization(constant are irrelevant)\n",
    "$$\n",
    "L =C_{0}-C_{1} \\sum_{i,j\\in\\Omega}^{}(r_{ij}-w_{i}^{T}u_{j})^{2}- \\frac{-\\lambda}{2}||W||_{F}^{2} - \\frac{-\\lambda}{2}||U||_{F}^{2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Bayesian Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summarize the Bayesian approach as everything is a random variable\n",
    "\n",
    "$$\n",
    "R \\sim N(WU^{T},\\sigma^{2})\\\\\n",
    "r_{ij} \\sim N(w_{i}^{T}u_{j},\\sigma^{2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(W) = N(0,\\lambda^{-1})\\\\\n",
    "p(U) = N(0,\\lambda^{-1})\n",
    "$$\n",
    "\n",
    "- Posterior\n",
    "- This time, we can't ignore p(R), because we don't want the argmax\n",
    "- Because this is Bayesian, we want the actual distribution p(W,U|R)\n",
    "\n",
    "$$\n",
    "p(W,U|R) = \\frac{p(R|W,U)p(W)p(U)}{p(R)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is our goal?\n",
    "- we still want to predict ratings e.g. $r_{ij}$\n",
    "- since this is Bayesian it'll be $p(r_{ij}|R)$\n",
    "- R is collected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected value\n",
    "- How about $E(r_{ij}|R)$\n",
    "- In general\n",
    "$$\n",
    "E(x) = \\int_{-\\infty}^{\\infty}xp(x)dx\n",
    "$$\n",
    "\n",
    "- For us\n",
    "$$\n",
    "E(r_{ij}|R) = \\int r_{ij}p(r_{ij}|R)dr_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(r_{ij}|R) = \\int r_{ij}p(r_{ij}|W,U) p(W,U|R)dWdUdr_{ij}\n",
    "$$\n",
    "\n",
    "- $p(r_{ij}|W,U)$ : our original gaussian\n",
    "- $p(W,U|R)$ : posterior\n",
    "\n",
    "$$\n",
    "\\int r_{ij}p(r_{ij}|W,U) dr_{ij} = E(r_{ij}|W,U) = w_{i}^{T}u_{j}\\\\\n",
    "r_{ij} \\sim N(w_{i}^{T}u_{j},\\sigma^{2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(r_{ij}|R)= \\int w_{i}^{T}u_{j} p(W,U|R)dWdU \\\\\n",
    "E(r_{ij}|R)= E(w_{i}^{T}u_{j}|R)\n",
    "$$\n",
    "\n",
    "- we can approximate this expected value\n",
    "- A mean can be approximated by the sample mean, if we sample from the correct distribution\n",
    "- MCMC(Markov Chain MonteCarlo) - Gibbs Sampling\n",
    "\n",
    "$$\n",
    "E(r_{ij}|R) \\approx \\frac{1}{T}\\sum_{t=1}^{T}w_{i}^{(t)T}u_{j}^{(t)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Matrix Factorization In Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm for training neural nets: gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Matirx Factorization\n",
    "- Both gradient descent and alternating least squares are valid training algorithms for matrix factorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "- N = (vocabulary size) K =(feature vector size)\n",
    "- N x K matrix\n",
    "\n",
    "```python\n",
    "emb_u = Embedding(N,K)\n",
    "emb_m = Embedding(M,K)\n",
    "\n",
    "#inputs\n",
    "u = Input() #user\n",
    "m = Input() #movie\n",
    "\n",
    "eu = emb_u(u) # returns a K-size vector\n",
    "em = emb_m(m) \n",
    "\n",
    "# dot them to get prediction\n",
    "r = dot(eu,em)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias terms\n",
    "- not straightforward\n",
    "- keras is not a low level library\n",
    "- Embedding(N,1) gives me a scalar for a user bias\n",
    "\n",
    "```python\n",
    "bias_u = Embedding(N,1)\n",
    "bias_m = Embedding(N,1)\n",
    "\n",
    "bu = bias_u(u)\n",
    "bm = bias_u(m)\n",
    "\n",
    "r = add(r,bu,bm) # r = dot(eu,em)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global average\n",
    "- we want the model $r_{ij} = dot(w_{i},u_{j}) + b_{i}+c_{j}+mu$\n",
    "- simple solution\n",
    "- make the target = actual ratings - mu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Matrix Factorization In Keras In Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# load in the data\n",
    "df = pd.read_csv('../large_files/movielens-20m-dataset/edited_rating.csv')\n",
    "\n",
    "N = df.userId.max() + 1 # number of users\n",
    "M = df.movie_idx.max() + 1 # number of movies\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# initialize variables\n",
    "K = 10 # latent dimensionality\n",
    "mu = df_train.rating.mean()\n",
    "epochs = 15\n",
    "reg = 0. # regularization penalty\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# keras model\n",
    "u = Input(shape=(1,))\n",
    "m = Input(shape=(1,))\n",
    "u_embedding = Embedding(N, K, embeddings_regularizer=l2(reg))(u) # (N, 1, K)\n",
    "m_embedding = Embedding(M, K, embeddings_regularizer=l2(reg))(m) # (N, 1, K)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "'''\n",
    "u_embedding,m_embedding\n",
    "(<tf.Tensor 'embedding_1/GatherV2:0' shape=(?, 1, 5) dtype=float32>,\n",
    " <tf.Tensor 'embedding_2/GatherV2:0' shape=(?, 1, 5) dtype=float32>)\n",
    "''' \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "u_bias = Embedding(N, 1, embeddings_regularizer=l2(reg))(u) # (N, 1, 1)\n",
    "m_bias = Embedding(M, 1, embeddings_regularizer=l2(reg))(m) # (N, 1, 1)\n",
    "x = Dot(axes=2)([u_embedding, m_embedding]) # (N, 1, 1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x = Add()([x, u_bias, m_bias])\n",
    "x = Flatten()(x) # (N, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = Model(inputs=[u, m], outputs=x)\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  # optimizer='adam',\n",
    "  # optimizer=Adam(lr=0.01),\n",
    "  optimizer=SGD(lr=0.08, momentum=0.9),\n",
    "  metrics=['mse'],\n",
    ")\n",
    "\n",
    "r = model.fit(\n",
    "  x=[df_train.userId.values, df_train.movie_idx.values],\n",
    "  y=df_train.rating.values - mu,\n",
    "  epochs=epochs,\n",
    "  batch_size=128,\n",
    "  validation_data=(\n",
    "    [df_test.userId.values, df_test.movie_idx.values],\n",
    "    df_test.rating.values - mu\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input is a feature vector\n",
    "\n",
    "- we already have a feature vector that represents the user, and a feature vector that represents the item\n",
    "- User vector: W[i]\n",
    "- Movie vector: U[j]  \n",
    "\n",
    "\\begin{array}\n",
    " UUser & \\xrightarrow{} & Embedding & \\xrightarrow{} & Concat & \\xrightarrow{} & Neural Network\\\\\n",
    " Movie & \\xrightarrow{} & Embedding & \\nearrow & \\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- since it's regression, final layer is Dense(1) with no activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter \n",
    "- number layers\n",
    "- number hidden units\n",
    "- hidden activation\n",
    "- optimizer\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# keras model\n",
    "u = Input(shape=(1,))\n",
    "m = Input(shape=(1,))\n",
    "u_embedding = Embedding(N, K)(u) # (N, 1, K)\n",
    "m_embedding = Embedding(M, K)(m) # (N, 1, K)\n",
    "u_embedding = Flatten()(u_embedding) # (N, K)\n",
    "m_embedding = Flatten()(m_embedding) # (N, K)\n",
    "x = Concatenate()([u_embedding, m_embedding]) # (N, 2K)\n",
    "\n",
    "# the neural network\n",
    "x = Dense(400)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(100)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[u, m], outputs=x)\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  # optimizer='adam',\n",
    "  # optimizer=Adam(lr=0.01),\n",
    "  optimizer=SGD(lr=0.08, momentum=0.9),\n",
    "  metrics=['mse'],\n",
    ")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
