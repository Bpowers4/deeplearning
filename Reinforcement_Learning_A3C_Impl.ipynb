{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- main (global policy and value networks)\n",
    "    - Create and coordinate workers \n",
    "    - check CPUs available, create threads and workers\n",
    "    - Initialize global thread safe counter,every workers know when to quit\n",
    "    \n",
    "- worker (contains local policy and value networks)\n",
    "    - copy weights from global network\n",
    "    - play episodes\n",
    "    - send gradients back to master\n",
    "\n",
    "\\begin{equation*}\n",
    "g_{local} = \\frac {\\partial L(\\theta_{local})}{\\partial \\theta_{local}} \\\\\n",
    "\\theta_{global} = \\theta_{global} - \\eta g_{local}\\\\\n",
    "\\end{equation*}\n",
    "    \n",
    "- nets\n",
    "    - definition of poilicy and value network\n",
    "    - variable scopes\n",
    "    - \"reuse\" arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Policy,Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor(input_):\n",
    "    # We only want to create the weights once\n",
    "    # In all future calls we should set reuse = True\n",
    "\n",
    "    # scale the inputs from 0..255 to 0..1\n",
    "    input_ = tf.to_float(input_) / 255.0\n",
    "\n",
    "    # conv layers\n",
    "    conv1 = tf.contrib.layers.conv2d(\n",
    "      input_,\n",
    "      16, # num output feature maps\n",
    "      8,  # kernel size\n",
    "      4,  # stride\n",
    "      activation_fn=tf.nn.relu,\n",
    "      scope=\"conv1\")\n",
    "    conv2 = tf.contrib.layers.conv2d(\n",
    "      conv1,\n",
    "      32, # num output feature maps\n",
    "      4,  # kernel size\n",
    "      2,  # stride\n",
    "      activation_fn=tf.nn.relu,\n",
    "      scope=\"conv2\")\n",
    "\n",
    "    # image -> feature vector\n",
    "    flat = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "    # dense layer\n",
    "    fc1 = tf.contrib.layers.fully_connected(\n",
    "      inputs=flat,\n",
    "      num_outputs=256,\n",
    "      scope=\"fc1\")\n",
    "\n",
    "    return fc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork:\n",
    "    def __init__(self, num_outputs, reg=0.01):\n",
    "        \n",
    "        self.num_outputs = num_outputs\n",
    "        # states = N x 28224(84*84*4)\n",
    "        self.states = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        \n",
    "        # Advantage = G - V(s)\n",
    "        self.advantage = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        \n",
    "        # Selected actions\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "        \n",
    "        # Since we set reuse=False here, that means we MUST\n",
    "        # create the PolicyNetwork before creating the ValueNetwork\n",
    "        # ValueNetwork will use reuse=True\n",
    "        with tf.variable_scope(\"shared\", reuse=False):\n",
    "            fc1 = build_feature_extractor(self.states)\n",
    "            self.debug_fc1 = fc1\n",
    "        # Use a separate scope for output and loss\n",
    "        with tf.variable_scope(\"policy_network\"):\n",
    "            self.logits = tf.contrib.layers.fully_connected(fc1, num_outputs, activation_fn=None)\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            \n",
    "            # Sample an action\n",
    "            cdist = tf.distributions.Categorical(logits=self.logits)\n",
    "            self.sample_action = cdist.sample()\n",
    "            \n",
    "            # Add regularization to increase exploration\n",
    "            self.entropy = -tf.reduce_sum(self.probs * tf.log(self.probs), axis=1)\n",
    "            \n",
    "            # Get the predictions for the chosen actions only\n",
    "            batch_size = tf.shape(self.states)[0]\n",
    "            gather_indices = tf.range(batch_size) * tf.shape(self.probs)[1] + self.actions\n",
    "            self.selected_action_probs = tf.gather(tf.reshape(self.probs, [-1]), gather_indices)\n",
    "            \n",
    "            self.loss = tf.log(self.selected_action_probs) * self.advantage + reg * self.entropy\n",
    "            self.loss = -tf.reduce_sum(self.loss, name=\"loss\")\n",
    "            \n",
    "            # training\n",
    "            # tf.train.RMSPropOptimizer(learning_rate,decay,momentum,epsilon,...)\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "            # tf.train.AdamOptimizer(learning_rate,beta1,beta2,epsilon,...)\n",
    "            # self.optimizer = tf.train.AdamOptimizer(0.00025)\n",
    "            \n",
    "            # we'll need these later for running gradient descent steps\n",
    "            self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            self.grads_and_vars = [[grad, var] for grad, var in self.grads_and_vars if grad is not None]\n",
    "\n",
    "\n",
    "            \n",
    "class ValueNetwork:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Placeholders for our input\n",
    "        # After resizing we have 4 consecutive frames of size 84 x 84\n",
    "        self.states = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value, reward value\n",
    "        self.targets = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "\n",
    "        # Since we set reuse=True here, that means we MUST\n",
    "        # create the PolicyNetwork before creating the ValueNetwork\n",
    "        # PolictyNetwork will use reuse=False\n",
    "        with tf.variable_scope(\"shared\", reuse=True):\n",
    "            fc1 = build_feature_extractor(self.states)\n",
    "            \n",
    "        # Use a separate scope for output and loss\n",
    "        with tf.variable_scope(\"value_network\"):\n",
    "            self.vhat = tf.contrib.layers.fully_connected(\n",
    "                inputs=fc1,\n",
    "                num_outputs=1,\n",
    "                activation_fn=None)\n",
    "            self.debug_vhat = self.vhat\n",
    "            self.vhat = tf.squeeze(self.vhat, squeeze_dims=[1], name=\"vhat\")\n",
    "            \n",
    "            # tf.losses.mean_squared_error( rewards - valuenetwork's output)\n",
    "            self.loss = tf.squared_difference(self.vhat, self.targets)\n",
    "            self.loss = tf.reduce_sum(self.loss, name=\"loss\")\n",
    "            \n",
    "            # training\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "            # self.optimizer = tf.train.AdamOptimizer(0.00025)\n",
    "\n",
    "            # we'll need these later for running gradient descent steps\n",
    "            self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            self.grads_and_vars = [[grad, var] for grad, var in self.grads_and_vars if grad is not None]\n",
    "            \n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should use this to create networks\n",
    "# to ensure they're created in the correct order\n",
    "def create_networks(num_outputs):\n",
    "    policy_network = PolicyNetwork(num_outputs=num_outputs)\n",
    "    value_network = ValueNetwork()\n",
    "    return policy_network, value_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step:\n",
    "    def __init__(self, state, action, reward, next_state, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform raw images for input into neural network\n",
    "# 1) Convert to grayscale\n",
    "# 2) Resize\n",
    "# 3) Crop\n",
    "class ImageTransformer:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope(\"image_transformer\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output,\n",
    "                [84, 84],\n",
    "                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def transform(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.output, { self.input_state: state })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial state by repeating the same frame 4 times\n",
    "# generating (84,84,4)\n",
    "def repeat_frame(frame):\n",
    "    return np.stack([frame] * 4, axis=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create next state by shifting each frame by 1\n",
    "# Throw out the oldest frame\n",
    "# And concatenate the newest frame\n",
    "# 84 * 84 * ( 3(old) + 1(next_frame))\n",
    "def shift_frames(state, next_frame):\n",
    "    return np.append(state[:,:,1:], np.expand_dims(next_frame, 2), axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Tensorflow op to copy weights from one scope to another\n",
    "def get_copy_params_op(src_vars, dst_vars):\n",
    "    src_vars = list(sorted(src_vars, key=lambda v: v.name))\n",
    "    dst_vars = list(sorted(dst_vars, key=lambda v: v.name))\n",
    "\n",
    "    ops = []\n",
    "    for s, d in zip(src_vars, dst_vars):\n",
    "        op = d.assign(s)\n",
    "        ops.append(op)\n",
    "\n",
    "    return ops\n",
    "\n",
    "def make_train_op(local_net, global_net):\n",
    "    \"\"\"\n",
    "    Use gradients from local network to update the global network\n",
    "    \"\"\"\n",
    "\n",
    "    # Idea:\n",
    "    # We want a list of gradients and corresponding variables\n",
    "    # e.g. [[g1, g2, g3], [v1, v2, v3]]\n",
    "    # Since that's what the optimizer expects.\n",
    "    # But we would like the gradients to come from the local network\n",
    "    # And the variables to come from the global network\n",
    "    # So we want to make a list like this:\n",
    "    # [[local_g1, local_g2, local_g3], [global_v1, global_v2, global_v3]]\n",
    "\n",
    "    # First get only the gradients\n",
    "    local_grads, _ = zip(*local_net.grads_and_vars)\n",
    "    \n",
    "    # Clip gradients to avoid large values\n",
    "    local_grads, _ = tf.clip_by_global_norm(local_grads, 5.0)\n",
    "    \n",
    "    # Get global vars\n",
    "    _, global_vars = zip(*global_net.grads_and_vars)\n",
    "    \n",
    "    # Combine local grads and global vars\n",
    "    local_grads_global_vars = list(zip(local_grads, global_vars))\n",
    "    \n",
    "    # Run a gradient descent step, e.g.\n",
    "    # var = var - learning_rate * grad\n",
    "    return global_net.optimizer.apply_gradients(\n",
    "            local_grads_global_vars,\n",
    "            global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker object to be run in a thread\n",
    "# name (String) should be unique for each thread\n",
    "# env (OpenAI Gym Environment) should be unique for each thread\n",
    "# policy_net (PolicyNetwork) should be a global passed to every worker\n",
    "# value_net (ValueNetwork) should be a global passed to every worker\n",
    "# returns_list (List) should be a global passed to every worker\n",
    "class Worker:\n",
    "    def __init__(self,name,env,policy_net,value_net,global_counter,returns_list\n",
    "                 ,discount_factor=0.99,max_global_steps=None):\n",
    "\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.global_policy_net = policy_net\n",
    "        self.global_value_net = value_net\n",
    "        self.global_counter = global_counter\n",
    "        self.discount_factor = discount_factor\n",
    "        self.max_global_steps = max_global_steps\n",
    "        self.global_step = tf.train.get_global_step()\n",
    "        self.img_transformer = ImageTransformer()\n",
    "        \n",
    "        # Create local policy and value networks that belong only to this worker\n",
    "        with tf.variable_scope(name):\n",
    "            # self.policy_net = PolicyNetwork(num_outputs=policy_net.num_outputs)\n",
    "            # self.value_net = ValueNetwork()\n",
    "            self.policy_net, self.value_net = create_networks(policy_net.num_outputs)\n",
    "            \n",
    "        # We will use this op to copy the global network weights\n",
    "        # back to the local policy and value networks\n",
    "        self.copy_params_op = get_copy_params_op(\n",
    "                  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"global\"),\n",
    "                  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/'))\n",
    "        \n",
    "        # These will take the gradients from the local networks\n",
    "        # and use those gradients to update the global network\n",
    "        self.vnet_train_op = make_train_op(self.value_net, self.global_value_net)\n",
    "        self.pnet_train_op = make_train_op(self.policy_net, self.global_policy_net)\n",
    "\n",
    "        self.state = None # Keep track of the current state\n",
    "        self.total_reward = 0. # After each episode print the total (sum of) reward\n",
    "        self.returns_list = returns_list # Global returns list to plot later\n",
    "        \n",
    "    def run(self, sess, coord, t_max):\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            # Assign the initial state\n",
    "            self.state = repeat_frame(self.img_transformer.transform(self.env.reset()))\n",
    "\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    # Copy weights from  global networks to local networks\n",
    "                    sess.run(self.copy_params_op)\n",
    "\n",
    "                    # Collect some experience\n",
    "                    steps, global_step = self.run_n_steps(t_max, sess)\n",
    "\n",
    "                    # Stop once the max number of global steps has been reached\n",
    "                    if self.max_global_steps is not None and global_step >= self.max_global_steps:\n",
    "                        coord.request_stop()\n",
    "                        return\n",
    "\n",
    "                    # Update the global networks using local gradients\n",
    "                    self.update(steps, sess)\n",
    "\n",
    "            except tf.errors.CancelledError:\n",
    "                return\n",
    "    \n",
    "    def sample_action(self, state, sess):\n",
    "        # Make input N x D (N = 1)\n",
    "        feed_dict = { self.policy_net.states: [state] }\n",
    "        actions = sess.run(self.policy_net.sample_action, feed_dict)\n",
    "        # Prediction is a 1-D array of length N, just want the first value\n",
    "        return actions[0]\n",
    "    \n",
    "    def get_value_prediction(self, state, sess):\n",
    "        # Make input N x D (N = 1)\n",
    "        feed_dict = { self.value_net.states: [state] }\n",
    "        vhat = sess.run(self.value_net.vhat, feed_dict)\n",
    "        # Prediction is a 1-D array of length N, just want the first value\n",
    "        return vhat[0]\n",
    "    \n",
    "    def run_n_steps(self, n, sess):\n",
    "        steps = []\n",
    "        for _ in range(n):\n",
    "            # Take a step\n",
    "            action = self.sample_action(self.state, sess)\n",
    "            next_frame, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            # Shift the state to include the latest frame\n",
    "            next_state = shift_frames(self.state, self.img_transformer.transform(next_frame))\n",
    "            \n",
    "            # Save total return\n",
    "            if done:\n",
    "                print(\"Total reward:\", self.total_reward, \"Worker:\", self.name)\n",
    "                self.returns_list.append(self.total_reward)\n",
    "                if len(self.returns_list) > 0 and len(self.returns_list) % 100 == 0:\n",
    "                    print(\"*** Total average reward (last 100):\", np.mean(self.returns_list[-100:]), \"Collected so far:\", len(self.returns_list))\n",
    "                    self.total_reward = 0.\n",
    "            else:\n",
    "                self.total_reward += reward\n",
    "            \n",
    "            # Save step (state,action,reward,next_state,done)\n",
    "            step = Step(self.state, action, reward, next_state, done)\n",
    "            steps.append(step)\n",
    "            \n",
    "            # Increase local and global counters\n",
    "            global_step = next(self.global_counter)\n",
    "            \n",
    "            if done:\n",
    "                self.state = repeat_frame(self.img_transformer.transform(self.env.reset()))\n",
    "                break\n",
    "            else:\n",
    "                self.state = next_state\n",
    "            \n",
    "        return steps, global_step\n",
    "    \n",
    "    def update(self, steps, sess):\n",
    "        \"\"\"\n",
    "        Updates global policy and value networks using the local networks' gradients\n",
    "        \"\"\"\n",
    "\n",
    "        # In order to accumulate the total return\n",
    "        # We will use V_hat(s') to predict the future returns\n",
    "        # But we will use the actual rewards if we have them\n",
    "        # Ex. if we have s1, s2, s3 with rewards r1, r2, r3\n",
    "        # Then G(s3) = r3 + V(s4)\n",
    "        #      G(s2) = r2 + r3 + V(s4)\n",
    "        #      G(s1) = r1 + r2 + r3 + V(s4)\n",
    "        reward = 0.0\n",
    "        if not steps[-1].done:\n",
    "            reward = self.get_value_prediction(steps[-1].next_state, sess)\n",
    "            \n",
    "        # Accumulate minibatch samples\n",
    "        states = []\n",
    "        advantages = []\n",
    "        value_targets = []\n",
    "        actions = []\n",
    "        \n",
    "        # loop through steps in reverse order\n",
    "        for step in reversed(steps):\n",
    "            reward = step.reward + self.discount_factor * reward\n",
    "            advantage = reward - self.get_value_prediction(step.state, sess)\n",
    "            # Accumulate updates\n",
    "            states.append(step.state)\n",
    "            actions.append(step.action)\n",
    "            advantages.append(advantage)\n",
    "            value_targets.append(reward)\n",
    "\n",
    "        feed_dict = {\n",
    "              self.policy_net.states: np.array(states),\n",
    "              self.policy_net.advantage: advantages,\n",
    "              self.policy_net.actions: actions,\n",
    "              self.value_net.states: np.array(states),\n",
    "              self.value_net.targets: value_targets,\n",
    "            }\n",
    "\n",
    "        # Train the global estimators using local gradients\n",
    "        global_step, pnet_loss, vnet_loss, _, _ = sess.run([\n",
    "              self.global_step,\n",
    "              self.policy_net.loss,\n",
    "              self.value_net.loss,\n",
    "              self.pnet_train_op,\n",
    "              self.vnet_train_op,\n",
    "            ], feed_dict)\n",
    "        \n",
    "        # Theoretically could plot these later\n",
    "        return pnet_loss, vnet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import shutil\n",
    "import threading\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Breakout-v0\"\n",
    "MAX_GLOBAL_STEPS = 5e6\n",
    "STEPS_PER_UPDATE = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Env():\n",
    "    return gym.envs.make(ENV_NAME)\n",
    "\n",
    "# Depending on the game we may have a limited action space\n",
    "if ENV_NAME == \"Pong-v0\" or ENV_NAME == \"Breakout-v0\":\n",
    "    NUM_ACTIONS = 4 # env.action_space.n returns a bigger number\n",
    "else:\n",
    "    env = Env()\n",
    "    NUM_ACTIONS = env.action_space.n\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def smooth(x):\n",
    "    # last 100\n",
    "    n = len(x)\n",
    "    y = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        start = max(0, i - 99)\n",
    "        y[i] = float(x[start:(i+1)].sum()) / (i - start + 1)\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of workers\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "\n",
    "    # Keeps track of the number of updates we've performed\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/global_step\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    # Global policy and value nets\n",
    "    with tf.variable_scope(\"global\") as vs:\n",
    "        policy_net, value_net = create_networks(NUM_ACTIONS)\n",
    "        \n",
    "    # Global step iterator\n",
    "    global_counter = itertools.count()\n",
    "    \n",
    "    # Save returns\n",
    "    returns_list = []\n",
    "    \n",
    "    # Create workers\n",
    "    workers = []\n",
    "    for worker_id in range(NUM_WORKERS):\n",
    "        worker = Worker(\n",
    "          name=\"worker_{}\".format(worker_id),\n",
    "          env=Env(),\n",
    "          policy_net=policy_net,\n",
    "          value_net=value_net,\n",
    "          global_counter=global_counter,\n",
    "          returns_list=returns_list,\n",
    "          discount_factor = 0.99,\n",
    "          max_global_steps=MAX_GLOBAL_STEPS)\n",
    "        workers.append(worker)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    # Start worker threads\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_fn = lambda: worker.run(sess, coord, STEPS_PER_UPDATE)\n",
    "        t = threading.Thread(target=worker_fn)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "        \n",
    "    # Wait for all workers to finish\n",
    "    coord.join(worker_threads, stop_grace_period_secs=300)\n",
    "    \n",
    "    # Plot the smoothed returns\n",
    "    x = np.array(returns_list)\n",
    "    y = smooth(x)\n",
    "    plt.plot(x, label='orig')\n",
    "    plt.plot(y, label='smoothed')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shared/fc1/Relu:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet = PolicyNetwork(4)\n",
    "pnet.debug_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'policy_network/fully_connected/BiasAdd:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'policy_network/Softmax:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'policy_network/Categorical/sample/Reshape_1:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'actions:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'policy_network/Neg:0' shape=(?,) dtype=float32>,\n",
       " <tf.Tensor 'policy_network/GatherV2:0' shape=(?,) dtype=float32>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.logits,pnet.probs,pnet.sample_action,pnet.actions,pnet.entropy,pnet.selected_action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-d5124e3b7ab6>:74: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'value_network/fully_connected/BiasAdd:0' shape=(?, 1) dtype=float32>,\n",
       " <tf.Tensor 'value_network/vhat:0' shape=(?,) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnet = ValueNetwork()\n",
    "vnet.debug_vhat,vnet.vhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
