{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"./opensrc/ssd_keras-master_20190822/\") \n",
    "sys.path.append(\"./opensrc/ssd_keras-master_20190822/models\") \n",
    "sys.path.append(\"./opensrc/ssd_keras-master_20190822/keras_loss_function\") \n",
    "sys.path.append(\"./opensrc/ssd_keras-master_20190822/keras_layers\") \n",
    "sys.path.append(\"./opensrc/ssd_keras-master_20190822/ssd_encoder_decoder\") \n",
    "sys.path.append(\"./opensrc/ssd_keras-master_20190822/data_generator\") \n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale boxes\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{level index: } &\\ell = 1, \\dots, L \\\\\n",
    "\\text{scale of boxes: } &s_\\ell = s_\\text{min} + \\frac{s_\\text{max} - s_\\text{min}}{L - 1} (\\ell - 1) \\\\\n",
    "\\text{aspect ratio: } &r \\in \\{1, 2, 3, 1/2, 1/3\\}\\\\\n",
    "\\text{additional scale: } & s'_\\ell = \\sqrt{s_\\ell s_{\\ell + 1}} \\text{ when } r = 1 \\text{thus, 6 boxes in total.}\\\\\n",
    "\\text{width: } &w_\\ell^r = s_\\ell \\sqrt{r} \\\\\n",
    "\\text{height: } &h_\\ell^r = s_\\ell / \\sqrt{r} \\\\\n",
    "\\text{center location: } & (x^i_\\ell, y^j_\\ell) = (\\frac{i+0.5}{m}, \\frac{j+0.5}{n})\n",
    "\\end{aligned}\n",
    "\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/SSD-box-scales.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the convolutionalized VGG-16 weights\n",
    "\n",
    "In order to train an SSD300 or SSD512 from scratch, download the weights of the fully convolutionalized VGG-16 model trained to convergence on ImageNet classification here:\n",
    "\n",
    "[`VGG_ILSVRC_16_layers_fc_reduced.h5`](https://drive.google.com/open?id=1sBmajn6vOE7qJ8GnxUJt4fGPuffVUZox).\n",
    "\n",
    "As with all other weights files below, this is a direct port of the corresponding `.caffemodel` file that is provided in the repository of the original Caffe implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, the optimizer of choice would be Adam (commented out below), but since the original implementation uses plain SGD with momentum, we'll do the same in order to reproduce the original training. Adam is generally the superior optimizer, so if your goal is not to have everything exactly as in the original training, feel free to switch to Adam. You might need to adjust the learning rate scheduler below slightly in case you use Adam.\n",
    "\n",
    "Note that the learning rate that is being set here doesn't matter, because further below we'll pass a learning rate scheduler to the training function, which will overwrite any learning rate set here, i.e. what matters are the learning rates that are defined by the learning rate scheduler.\n",
    "\n",
    "`SSDLoss` is a custom Keras loss function that implements the multi-task that consists of a log loss for classification and a smooth L1 loss for localization. `neg_pos_ratio` and `alpha` are set as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "weights_path = 'data/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSDLoss Class \n",
    "- neg_pos_ratio (int, optional): The maximum ratio of negative (i.e. background)\n",
    "                to positive ground truth boxes to include in the loss computation.\n",
    "                There are no actual background ground truth boxes of course, but `y_true`\n",
    "                contains anchor boxes labeled with the background class. Since\n",
    "                the number of background boxes in `y_true` will usually exceed\n",
    "                the number of positive boxes by far, it is necessary to balance\n",
    "                their influence on the loss. Defaults to 3 following the paper.\n",
    "- n_neg_min (int, optional): The minimum number of negative ground truth boxes to\n",
    "                enter the loss computation *per batch*. This argument can be used to make\n",
    "                sure that the model learns from a minimum number of negatives in batches\n",
    "                in which there are very few, or even none at all, positive ground truth\n",
    "                boxes. It defaults to 0 and if used, it should be set to a value that\n",
    "                stands in reasonable proportion to the batch size used for training.\n",
    "- alpha (float, optional): A factor to weight the localization loss in the\n",
    "                computation of the total loss. Defaults to 1.0 following the paper                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def compute_loss(self, y_true, y_pred):\n",
    "    # Compute the loss of the SSD model prediction against the ground truth.\n",
    "    '''\n",
    "    y_true (array): A Numpy array of shape `(batch_size, #boxes, #classes + 12)`,\n",
    "                #boxes : the total number of boxes that the model predicts per image.\n",
    "                `#classes + 12` : `[classes one-hot encoded, 4 ground truth box coordinate offsets,\n",
    "                                    8 arbitrary entries]`\n",
    "                8 arbitrary entries : not used             \n",
    "                \n",
    "    y_pred (Keras tensor): The model prediction. The shape is identical\n",
    "                to that of `y_true`, \n",
    "                i.e. `(batch_size, #boxes, #classes + 12)`.\n",
    "                The last axis must contain entries in the format\n",
    "                `[classes one-hot encoded, 4 predicted box coordinate offsets, 8 arbitrary entries]`.                \n",
    "    '''\n",
    "    self.neg_pos_ratio = tf.constant(self.neg_pos_ratio)\n",
    "    self.n_neg_min = tf.constant(self.n_neg_min)\n",
    "    self.alpha = tf.constant(self.alpha)\n",
    "\n",
    "    batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
    "    n_boxes = tf.shape(y_pred)[1] # Output dtype: tf.int32, note that `n_boxes` in this context\n",
    "    #denotes the total number of boxes per image, not the number of boxes per cell.\n",
    "\n",
    "    # 1: Compute the losses for class and box predictions for every box.\n",
    "    # Output shape: (batch_size, n_boxes)\n",
    "    # y_true[:,:,:-12] one-hot encoded classes\n",
    "    classification_loss = tf.to_float(self.log_loss(y_true[:,:,:-12], y_pred[:,:,:-12])) \n",
    "    # Output shape: (batch_size, n_boxes)\n",
    "    localization_loss = tf.to_float(self.smooth_L1_loss(y_true[:,:,-12:-8], y_pred[:,:,-12:-8])) \n",
    "    \n",
    "    # 2: Compute the classification losses for the positive and negative targets.\n",
    "    # Create masks for the positive and negative ground truth classes.\n",
    "    negatives = y_true[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
    "    positives = tf.to_float(tf.reduce_max(y_true[:,:,1:-12], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
    "    # Count the number of positive boxes (classes 1 to n) in y_true across the whole batch.\n",
    "    n_positive = tf.reduce_sum(positives)\n",
    "    \n",
    "    pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "    # Compute the classification loss for the negative default boxes (if there are any).\n",
    "    # First, compute the classification loss for all negative boxes.\n",
    "    neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
    "    # The number of non-zero loss entries in `neg_class_loss_all`\n",
    "    n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) \n",
    "    \n",
    "    # What's the point of `n_neg_losses`? For the next step, \n",
    "    # which will be to compute which negative boxes enter the classification loss,\n",
    "    # we don't just want to know how many negative ground truth boxes there are, but for how many of those there \n",
    "    # actually is a positive (i.e. non-zero) loss.\n",
    "    # This is necessary because `tf.nn.top-k()` in the function below will pick the top k boxes with\n",
    "    # the highest losses no matter what, even if it receives a vector where all losses are zero. \n",
    "    # In the unlikely event that all negative classification losses ARE actually zero though,\n",
    "    # this behavior might lead to `tf.nn.top-k()` returning the indices of positive boxes\n",
    "    # ,leading to an incorrect negative classification loss computation\n",
    "    # ,and hence an incorrect overall loss computation.\n",
    "    # We therefore need to make sure that `n_negative_keep`\n",
    "    # , which assumes the role of the `k` argument in `tf.nn.top-k()`,\n",
    "    # is at most the number of negative boxes for which there is a positive classification loss.\n",
    "\n",
    "    # Compute the number of negative examples we want to account for in the loss.\n",
    "    # We'll keep at most `self.neg_pos_ratio` times the number of positives in `y_true`\n",
    "    # , but at least `self.n_neg_min` (unless `n_neg_loses` is smaller).\n",
    "    n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.to_int32(n_positive), self.n_neg_min), n_neg_losses)\n",
    "    \n",
    "    # In the unlikely case when either (1) there are no negative ground truth boxes at all\n",
    "    # or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`.\n",
    "    def f1():\n",
    "        return tf.zeros([batch_size])\n",
    "    # Otherwise compute the negative loss.\n",
    "    def f2():\n",
    "        # Now we'll identify the top-k (where k == `n_negative_keep`) boxes with the highest confidence loss that\n",
    "        # belong to the background class in the ground truth data. \n",
    "        # Note that this doesn't necessarily mean that the model predicted the wrong class for those boxes\n",
    "        # , it just means that the loss for those boxes is the highest.\n",
    "\n",
    "        # To do this, we reshape `neg_class_loss_all` to 1D...\n",
    "        neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
    "        # ...and then we get the indices for the `n_negative_keep` boxes with the highest loss out of those...\n",
    "        values, indices = tf.nn.top_k(neg_class_loss_all_1D,\n",
    "                                      k=n_negative_keep,\n",
    "                                      sorted=False) # We don't need them sorted.\n",
    "        # ...and with these indices we'll create a mask...\n",
    "        # Tensor of shape (batch_size * n_boxes,)\n",
    "        negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
    "                                       updates=tf.ones_like(indices, dtype=tf.int32),\n",
    "                                       shape=tf.shape(neg_class_loss_all_1D)) \n",
    "        # Tensor of shape (batch_size, n_boxes)\n",
    "        negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) \n",
    "        # ...and use it to keep only those boxes and mask all other classification losses\n",
    "        # Tensor of shape (batch_size,)\n",
    "        neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1)\n",
    "        return neg_class_loss\n",
    "    \n",
    "    neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
    "    \n",
    "    class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
    "    \n",
    "    # 3: Compute the localization loss for the positive targets.\n",
    "    # We don't compute a localization loss for negative predicted boxes \n",
    "    # (obviously: there are no ground truth boxes they would correspond to).\n",
    "    loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "    \n",
    "    # 4: Compute the total loss.\n",
    "    total_loss = (class_loss + self.alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
    "    # Keras has the annoying habit of dividing the loss by the batch size, which sucks in our case\n",
    "    # because the relevant criterion to average our loss over is the number of positive boxes in the batch\n",
    "    # (by which we're dividing in the line above), not the batch size. So in order to revert Keras' averaging\n",
    "    # over the batch size, we'll have to multiply by it.\n",
    "    total_loss = total_loss * tf.to_float(batch_size)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.zeros((10,20,21+12))\n",
    "y_true[:,:,0] = np.arange(0,10*20,1).reshape(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.],\n",
       "       [ 20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,\n",
       "         31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
       "       [ 40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,\n",
       "         51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.],\n",
       "       [ 60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,\n",
       "         71.,  72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.],\n",
       "       [ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,\n",
       "         91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.],\n",
       "       [100., 101., 102., 103., 104., 105., 106., 107., 108., 109., 110.,\n",
       "        111., 112., 113., 114., 115., 116., 117., 118., 119.],\n",
       "       [120., 121., 122., 123., 124., 125., 126., 127., 128., 129., 130.,\n",
       "        131., 132., 133., 134., 135., 136., 137., 138., 139.],\n",
       "       [140., 141., 142., 143., 144., 145., 146., 147., 148., 149., 150.,\n",
       "        151., 152., 153., 154., 155., 156., 157., 158., 159.],\n",
       "       [160., 161., 162., 163., 164., 165., 166., 167., 168., 169., 170.,\n",
       "        171., 172., 173., 174., 175., 176., 177., 178., 179.],\n",
       "       [180., 181., 182., 183., 184., 185., 186., 187., 188., 189., 190.,\n",
       "        191., 192., 193., 194., 195., 196., 197., 198., 199.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*cIE7bbicMOokWQ6w41I-NA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*OTVm8L9RoAKtwl3XEQNkzA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/1200/1*up-gIJ9rPkHXUGRoqWuULQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "n_predictor_layers = 6 # The number of predictor conv layers in the network is 6 for the original SSD300.\n",
    "n_classes += 1 # Account for the background class.\n",
    "l2_reg = l2_regularization # Make the internal name shorter.\n",
    "img_height, img_width, img_channels = image_size[0], image_size[1], image_size[2]\n",
    "\n",
    "############################################################################\n",
    "# Build the network.\n",
    "############################################################################\n",
    "\n",
    "x = Input(shape=(img_height, img_width, img_channels))\n",
    "\n",
    "# The following identity layer is only needed so that the subsequent lambda layers can be optional.\n",
    "x1 = Lambda(identity_layer, output_shape=(img_height, img_width, img_channels), name='identity_layer')(x)\n",
    "\n",
    "# VGG16 \n",
    "conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                 kernel_regularizer=l2(l2_reg), name='conv1_1')(x1)\n",
    "...\n",
    "...\n",
    "...  \n",
    "pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_3)\n",
    "\n",
    "fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n",
    "\n",
    "fc7 = Conv2D(1024, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n",
    "\n",
    "conv6_1 = Conv2D(256, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n",
    "conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n",
    "conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n",
    "\n",
    "conv7_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n",
    "conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n",
    "conv7_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n",
    "\n",
    "conv8_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv8_1')(conv7_2)\n",
    "conv8_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv8_2')(conv8_1)\n",
    "\n",
    "conv9_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv9_1')(conv8_2)\n",
    "conv9_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', \n",
    "                                         kernel_regularizer=l2(l2_reg), name='conv9_2')(conv9_1)\n",
    "\n",
    "# Feed conv4_3 into the L2 normalization layer\n",
    "conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)\n",
    "\n",
    "\n",
    "### Build the convolutional predictor layers on top of the base network\n",
    "\n",
    "# We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * \n",
    "# n_classes`\n",
    "# Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`\n",
    "'''\n",
    "n_boxes[0] * n_classes : classification \n",
    "'''\n",
    "conv4_3_norm_mbox_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                                kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(conv4_3_norm)\n",
    "fc7_mbox_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                       kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(fc7)\n",
    "conv6_2_mbox_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv6_2)\n",
    "conv7_2_mbox_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv7_2)\n",
    "conv8_2_mbox_conf = Conv2D(n_boxes[4] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_conf')(conv8_2)\n",
    "conv9_2_mbox_conf = Conv2D(n_boxes[5] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_conf')(conv9_2)\n",
    "\n",
    "# We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`\n",
    "# Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`\n",
    "'''\n",
    "n_boxes[0] * 4 : localization predictor\n",
    "'''\n",
    "conv4_3_norm_mbox_loc = Conv2D(n_boxes[0] * 4, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                               kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_loc')(conv4_3_norm)\n",
    "fc7_mbox_loc = Conv2D(n_boxes[1] * 4, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                      kernel_regularizer=l2(l2_reg), name='fc7_mbox_loc')(fc7)\n",
    "conv6_2_mbox_loc = Conv2D(n_boxes[2] * 4, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                          kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_loc')(conv6_2)\n",
    "conv7_2_mbox_loc = Conv2D(n_boxes[3] * 4, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                          kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_loc')(conv7_2)\n",
    "conv8_2_mbox_loc = Conv2D(n_boxes[4] * 4, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                          kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_loc')(conv8_2)\n",
    "conv9_2_mbox_loc = Conv2D(n_boxes[5] * 4, (3, 3), padding='same', kernel_initializer='he_normal', \n",
    "                          kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_loc')(conv9_2)\n",
    "\n",
    "\n",
    "### Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)\n",
    "'''\n",
    "anchor box(default box), generate the anchor boxes on each layer (each cell(WxH) in layer)\n",
    "priorbox[i] = [xmin, ymin, xmax, ymax, varxc, varyc, varw, varh]\n",
    "'''\n",
    "# Output shape of anchors: `(batch, height, width, n_boxes, 8)`\n",
    "conv4_3_norm_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], \n",
    "                                         aspect_ratios=aspect_ratios[0],\n",
    "                                         two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[0], \n",
    "                                         this_offsets=offsets[0], clip_boxes=clip_boxes,\n",
    "                                         variances=variances, coords=coords, normalize_coords=normalize_coords, \n",
    "                                         name='conv4_3_norm_mbox_priorbox')(conv4_3_norm_mbox_loc)\n",
    "fc7_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], \n",
    "                                        aspect_ratios=aspect_ratios[1],\n",
    "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[1], \n",
    "                                        this_offsets=offsets[1], clip_boxes=clip_boxes,\n",
    "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, \n",
    "                                        name='fc7_mbox_priorbox')(fc7_mbox_loc)\n",
    "conv6_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], \n",
    "                                        aspect_ratios=aspect_ratios[2],\n",
    "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[2], \n",
    "                                        this_offsets=offsets[2], clip_boxes=clip_boxes,\n",
    "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, \n",
    "                                        name='conv6_2_mbox_priorbox')(conv6_2_mbox_loc)\n",
    "conv7_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], \n",
    "                                        aspect_ratios=aspect_ratios[3],\n",
    "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[3], \n",
    "                                        this_offsets=offsets[3], clip_boxes=clip_boxes,\n",
    "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, \n",
    "                                        name='conv7_2_mbox_priorbox')(conv7_2_mbox_loc)\n",
    "conv8_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[4], next_scale=scales[5], \n",
    "                                        aspect_ratios=aspect_ratios[4],\n",
    "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[4], \n",
    "                                        this_offsets=offsets[4], clip_boxes=clip_boxes,\n",
    "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, \n",
    "                                        name='conv8_2_mbox_priorbox')(conv8_2_mbox_loc)\n",
    "conv9_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[5], next_scale=scales[6], \n",
    "                                        aspect_ratios=aspect_ratios[5],\n",
    "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[5], \n",
    "                                        this_offsets=offsets[5], clip_boxes=clip_boxes,\n",
    "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, \n",
    "                                        name='conv9_2_mbox_priorbox')(conv9_2_mbox_loc)\n",
    "\n",
    "'''\n",
    "'''\n",
    "'''\n",
    "'''\n",
    "### Concatenate the predictions from the different layers\n",
    "\n",
    "# Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n",
    "# so we want to concatenate along axis 1, the number of boxes per layer\n",
    "# Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)\n",
    "mbox_conf = Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n",
    "                                                   fc7_mbox_conf_reshape,\n",
    "                                                   conv6_2_mbox_conf_reshape,\n",
    "                                                   conv7_2_mbox_conf_reshape,\n",
    "                                                   conv8_2_mbox_conf_reshape,\n",
    "                                                   conv9_2_mbox_conf_reshape])\n",
    "\n",
    "# Output shape of `mbox_loc`: (batch, n_boxes_total, 4)\n",
    "mbox_loc = Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_mbox_loc_reshape,\n",
    "                                                 fc7_mbox_loc_reshape,\n",
    "                                                 conv6_2_mbox_loc_reshape,\n",
    "                                                 conv7_2_mbox_loc_reshape,\n",
    "                                                 conv8_2_mbox_loc_reshape,\n",
    "                                                 conv9_2_mbox_loc_reshape])\n",
    "\n",
    "# Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)\n",
    "'''\n",
    "# priorbox[i] = [xmin, ymin, xmax, ymax, varxc, varyc, varw, varh]\n",
    "'''\n",
    "mbox_priorbox = Concatenate(axis=1, name='mbox_priorbox')([conv4_3_norm_mbox_priorbox_reshape,\n",
    "                                                           fc7_mbox_priorbox_reshape,\n",
    "                                                           conv6_2_mbox_priorbox_reshape,\n",
    "                                                           conv7_2_mbox_priorbox_reshape,\n",
    "                                                           conv8_2_mbox_priorbox_reshape,\n",
    "                                                           conv9_2_mbox_priorbox_reshape])\n",
    "\n",
    "# The box coordinate predictions will go into the loss function just the way they are,\n",
    "# but for the class predictions, we'll apply a softmax activation layer first\n",
    "mbox_conf_softmax = Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n",
    "\n",
    "# Concatenate the class and box predictions and the anchors to one large predictions vector\n",
    "# Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
    "predictions = Concatenate(axis=2, name='predictions')([mbox_conf_softmax, mbox_loc, mbox_priorbox])\n",
    "\n",
    "if mode == 'training':\n",
    "    model = Model(inputs=x, outputs=predictions)\n",
    "elif mode == 'inference':\n",
    "    decoded_predictions = DecodeDetections(confidence_thresh=confidence_thresh,\n",
    "                                           iou_threshold=iou_threshold,\n",
    "                                           top_k=top_k,\n",
    "                                           nms_max_output_size=nms_max_output_size,\n",
    "                                           coords=coords,\n",
    "                                           normalize_coords=normalize_coords,\n",
    "                                           img_height=img_height,\n",
    "                                           img_width=img_width,\n",
    "                                           name='decoded_predictions')(predictions)\n",
    "    model = Model(inputs=x, outputs=decoded_predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnchorBox(Default Box, Prior Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]]\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "\n",
    "anchor_img_height = 300\n",
    "anchor_img_width = 300\n",
    "anchor_this_scale = scales_pascal[0]\n",
    "anchor_next_scale = scales_pascal[1]\n",
    "anchor_aspect_ratios = aspect_ratios[0]\n",
    "anchor_two_boxes_for_ar1 = True\n",
    "anchor_this_steps = steps[0]\n",
    "anchor_this_offsets = offsets[0]\n",
    "anchor_clip_boxes = False\n",
    "anchor_variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "anchor_coords = 'centroids'\n",
    "anchor_normalize_coords = True\n",
    "anchor_n_boxes = len(anchor_aspect_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "anchor_size = min(anchor_img_height, anchor_img_width)\n",
    "print(anchor_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the box widths and and heights for all aspect ratios\n",
    "wh_list = []\n",
    "for ar in anchor_aspect_ratios:\n",
    "    box_height = anchor_this_scale * anchor_size / np.sqrt(ar)\n",
    "    box_width = anchor_this_scale * anchor_size * np.sqrt(ar)\n",
    "    wh_list.append((box_width, box_height))\n",
    "wh_list = np.array(wh_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.        , 30.        ],\n",
       "       [42.42640687, 21.21320344],\n",
       "       [21.21320344, 42.42640687]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "AnchorBox input_shape (None, 38, 38, 16)\n",
    "x._keras_shape (None, 38, 38, 16)\n",
    "AnchorBox input_shape (None, 19, 19, 24)\n",
    "x._keras_shape (None, 19, 19, 24)\n",
    "AnchorBox input_shape (None, 10, 10, 24)\n",
    "x._keras_shape (None, 10, 10, 24)\n",
    "AnchorBox input_shape (None, 5, 5, 24)\n",
    "x._keras_shape (None, 5, 5, 24)\n",
    "AnchorBox input_shape (None, 3, 3, 16)\n",
    "x._keras_shape (None, 3, 3, 16)\n",
    "AnchorBox input_shape (None, 1, 1, 16)\n",
    "x._keras_shape (None, 1, 1, 16)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, feature_map_height, feature_map_width, feature_map_channels = (None, 38, 38, 16)\n",
    "# batch_size, feature_map_height, feature_map_width, feature_map_channels = (None, 19, 19, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_this_steps 8\n",
      "step_height 8 step_width 8\n"
     ]
    }
   ],
   "source": [
    "print(\"anchor_this_steps {}\".format(anchor_this_steps))\n",
    "step_height = anchor_this_steps\n",
    "step_width = anchor_this_steps\n",
    "print(\"step_height {} step_width {}\".format(step_height,step_width))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_this_offsets 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"anchor_this_offsets {}\".format(anchor_this_offsets))\n",
    "offset_height = anchor_this_offsets\n",
    "offset_width = anchor_this_offsets\n",
    "offset_height,offset_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
    "cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
    "cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
    "cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
    "cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "# cx_grid represents (length of [x_1,...,x_feature_map_width]*feature_map_width,[x_1,...,x_feature_map_width],1)\n",
    "# cy_grid represents (length of [y_1,...,y_feature_map_height]*feature_map_height,[y_1,...,y_feature_map_height],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset_height * step_height 4.0\n",
      "(offset_height + feature_map_height - 1) * step_height 300.0\n",
      "feature_map_height 38\n",
      "cy 38\n",
      "offset_width * step_width 4.0\n",
      "(offset_width + feature_map_width - 1) * step_width 300.0\n",
      "feature_map_width 38\n",
      "cy 38\n"
     ]
    }
   ],
   "source": [
    "print(\"offset_height * step_height {}\".format(offset_height * step_height))\n",
    "print(\"(offset_height + feature_map_height - 1) * step_height {}\".format((offset_height + feature_map_height - 1) * step_height))\n",
    "print(\"feature_map_height {}\".format(feature_map_height))\n",
    "print(\"cy {}\".format(len(cy)))\n",
    "\n",
    "print(\"offset_width * step_width {}\".format(offset_width * step_width))\n",
    "print(\"(offset_width + feature_map_width - 1) * step_width {}\".format((offset_width + feature_map_width - 1) * step_width))\n",
    "print(\"feature_map_width {}\".format(feature_map_width))\n",
    "print(\"cy {}\".format(len(cx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
    "# where the last dimension will contain `(cx, cy, w, h)`\n",
    "# cx_grid is (feature_map_width,feature_map_width,1)\n",
    "# cy_grid is (feature_map_height,feature_map_height,1)\n",
    "boxes_tensor = np.zeros((feature_map_height, feature_map_width, anchor_n_boxes, 4))\n",
    "\n",
    "boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, anchor_n_boxes)) # Set cx\n",
    "boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, anchor_n_boxes)) # Set cy\n",
    "boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
    "boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
    "\n",
    "# boxes_tensor[:, :, :, 0] shape is (feature_map_width,feature_map_width,anchor_n_boxes)\n",
    "# cx_grid's shape (feature_map_width,feature_map_width,1) is expanded to the \n",
    "# (feature_map_height,feature_map_height,anchor_n_boxes) to align with length of width,height list\n",
    "# (length of anchor_n_boxes) \n",
    "# boxes_tensor[:, :, :, 1] shape is (feature_map_height,feature_map_height,anchor_n_boxes)\n",
    "# boxes_tensor[:, :, :, 2] shape is (feature_map_height,feature_map_height,anchor_n_boxes)\n",
    "# boxes_tensor[:, :, :, 3] shape is (feature_map_width,feature_map_width,anchor_n_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38, 38, 3, 4), (38, 38, 3), (38, 38, 3), (38, 38, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_tensor.shape,boxes_tensor[:, :, :, 2].shape,boxes_tensor[:, :, :, 0].shape,boxes_tensor[:, :, :, 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.w3resource.com/w3r_images/numpy-manipulation-tile-function-image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coordinates(tensor, start_index, conversion, border_pixels='half'):\n",
    "    '''\n",
    "    Convert coordinates for axis-aligned 2D boxes between two coordinate formats.\n",
    "\n",
    "    Creates a copy of `tensor`, i.e. does not operate in place. Currently there are\n",
    "    three supported coordinate formats that can be converted from and to each other:\n",
    "        1) (xmin, xmax, ymin, ymax) - the 'minmax' format\n",
    "        2) (xmin, ymin, xmax, ymax) - the 'corners' format\n",
    "        2) (cx, cy, w, h) - the 'centroids' format\n",
    "   \n",
    "    '''\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "    elif border_pixels == 'include':\n",
    "        d = 1\n",
    "    elif border_pixels == 'exclude':\n",
    "        d = -1\n",
    "\n",
    "    ind = start_index\n",
    "    tensor1 = np.copy(tensor).astype(np.float)\n",
    "    if conversion == 'minmax2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+1]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+2] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+2] + d # Set h\n",
    "    elif conversion == 'centroids2minmax':\n",
    "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
    "        tensor1[..., ind+1] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
    "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
    "    elif conversion == 'corners2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+2]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+1] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+2] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+1] + d # Set h\n",
    "    elif conversion == 'centroids2corners':\n",
    "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
    "        tensor1[..., ind+1] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
    "        tensor1[..., ind+2] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
    "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
    "    elif (conversion == 'minmax2corners') or (conversion == 'corners2minmax'):\n",
    "        tensor1[..., ind+1] = tensor[..., ind+2]\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1]\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\")\n",
    "\n",
    "    return tensor1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`\n",
    "boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
    "if anchor_normalize_coords:\n",
    "    boxes_tensor[:, :, :, [0, 2]] /= anchor_img_width\n",
    "    boxes_tensor[:, :, :, [1, 3]] /= anchor_img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
    "if anchor_coords == 'centroids':\n",
    "    # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
    "    boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01333333, 0.01333333, 0.01333333],\n",
       "        [0.04      , 0.04      , 0.04      ],\n",
       "        [0.06666667, 0.06666667, 0.06666667],\n",
       "        ...,\n",
       "        [0.94666667, 0.94666667, 0.94666667],\n",
       "        [0.97333333, 0.97333333, 0.97333333],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[0.01333333, 0.01333333, 0.01333333],\n",
       "        [0.04      , 0.04      , 0.04      ],\n",
       "        [0.06666667, 0.06666667, 0.06666667],\n",
       "        ...,\n",
       "        [0.94666667, 0.94666667, 0.94666667],\n",
       "        [0.97333333, 0.97333333, 0.97333333],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[0.01333333, 0.01333333, 0.01333333],\n",
       "        [0.04      , 0.04      , 0.04      ],\n",
       "        [0.06666667, 0.06666667, 0.06666667],\n",
       "        ...,\n",
       "        [0.94666667, 0.94666667, 0.94666667],\n",
       "        [0.97333333, 0.97333333, 0.97333333],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.01333333, 0.01333333, 0.01333333],\n",
       "        [0.04      , 0.04      , 0.04      ],\n",
       "        [0.06666667, 0.06666667, 0.06666667],\n",
       "        ...,\n",
       "        [0.94666667, 0.94666667, 0.94666667],\n",
       "        [0.97333333, 0.97333333, 0.97333333],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[0.01333333, 0.01333333, 0.01333333],\n",
       "        [0.04      , 0.04      , 0.04      ],\n",
       "        [0.06666667, 0.06666667, 0.06666667],\n",
       "        ...,\n",
       "        [0.94666667, 0.94666667, 0.94666667],\n",
       "        [0.97333333, 0.97333333, 0.97333333],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[0.01333333, 0.01333333, 0.01333333],\n",
       "        [0.04      , 0.04      , 0.04      ],\n",
       "        [0.06666667, 0.06666667, 0.06666667],\n",
       "        ...,\n",
       "        [0.94666667, 0.94666667, 0.94666667],\n",
       "        [0.97333333, 0.97333333, 0.97333333],\n",
       "        [1.        , 1.        , 1.        ]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized coordinates between 0 and 1\n",
    "boxes_tensor[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_variances [0.1, 0.1, 0.2, 0.2]\n",
      " variances_tensor (38, 38, 3, 4) [0.1 0.1 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape\n",
    "# as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.\n",
    "print(\"anchor_variances {}\".format(anchor_variances))\n",
    "variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
    "variances_tensor += anchor_variances # Long live broadcasting\n",
    "print(\" variances_tensor {} {}\".format(variances_tensor.shape,variances_tensor[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[numpy.zeros_like](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
    "boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38, 38, 3, 8), (38, 38, 3), (38, 38, 3))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_tensor.shape,boxes_tensor[:,:,:,0].shape,boxes_tensor[:,:,:,4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x =  (None, 38, 38, 16)\n",
    "# Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along\n",
    "# The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n",
    "boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
    "boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print all anchorBox parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "wh_list [[30.         30.        ]\n",
      " [42.42640687 21.21320344]\n",
      " [21.21320344 42.42640687]]\n",
      "feature_map_height 38, feature_map_width 38, feature_map_channels 16\n",
      "anchor_this_steps 8\n",
      "step_height 8 step_width 8\n",
      "anchor_this_offsets 0.5\n",
      "offset_height 0.5 ,offset_width 0.5\n",
      "offset_height * step_height 4.0\n",
      "(offset_height + feature_map_height - 1) * step_height 300.0\n",
      "feature_map_height 38\n",
      "cy [  4.  12.  20.  28.  36.  44.  52.  60.  68.  76.  84.  92. 100. 108.\n",
      " 116. 124. 132. 140. 148. 156. 164. 172. 180. 188. 196. 204. 212. 220.\n",
      " 228. 236. 244. 252. 260. 268. 276. 284. 292. 300.] 38\n",
      "offset_width * step_width 4.0\n",
      "(offset_width + feature_map_width - 1) * step_width 300.0\n",
      "feature_map_width 38\n",
      "cx [  4.  12.  20.  28.  36.  44.  52.  60.  68.  76.  84.  92. 100. 108.\n",
      " 116. 124. 132. 140. 148. 156. 164. 172. 180. 188. 196. 204. 212. 220.\n",
      " 228. 236. 244. 252. 260. 268. 276. 284. 292. 300.] 38\n",
      "cx_grid (38, 38, 1) cy_grid (38, 38, 1)\n",
      "cx_grid[0] [[  4.]\n",
      " [ 12.]\n",
      " [ 20.]\n",
      " [ 28.]\n",
      " [ 36.]\n",
      " [ 44.]\n",
      " [ 52.]\n",
      " [ 60.]\n",
      " [ 68.]\n",
      " [ 76.]\n",
      " [ 84.]\n",
      " [ 92.]\n",
      " [100.]\n",
      " [108.]\n",
      " [116.]\n",
      " [124.]\n",
      " [132.]\n",
      " [140.]\n",
      " [148.]\n",
      " [156.]\n",
      " [164.]\n",
      " [172.]\n",
      " [180.]\n",
      " [188.]\n",
      " [196.]\n",
      " [204.]\n",
      " [212.]\n",
      " [220.]\n",
      " [228.]\n",
      " [236.]\n",
      " [244.]\n",
      " [252.]\n",
      " [260.]\n",
      " [268.]\n",
      " [276.]\n",
      " [284.]\n",
      " [292.]\n",
      " [300.]] \n",
      "cy_grid[0] [[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]] \n",
      "300\n",
      "wh_list [[ 60.          60.        ]\n",
      " [ 84.85281374  42.42640687]\n",
      " [ 42.42640687  84.85281374]\n",
      " [103.92304845  34.64101615]\n",
      " [ 34.64101615 103.92304845]]\n",
      "feature_map_height 19, feature_map_width 19, feature_map_channels 24\n",
      "anchor_this_steps 16\n",
      "step_height 16 step_width 16\n",
      "anchor_this_offsets 0.5\n",
      "offset_height 0.5 ,offset_width 0.5\n",
      "offset_height * step_height 8.0\n",
      "(offset_height + feature_map_height - 1) * step_height 296.0\n",
      "feature_map_height 19\n",
      "cy [  8.  24.  40.  56.  72.  88. 104. 120. 136. 152. 168. 184. 200. 216.\n",
      " 232. 248. 264. 280. 296.] 19\n",
      "offset_width * step_width 8.0\n",
      "(offset_width + feature_map_width - 1) * step_width 296.0\n",
      "feature_map_width 19\n",
      "cx [  8.  24.  40.  56.  72.  88. 104. 120. 136. 152. 168. 184. 200. 216.\n",
      " 232. 248. 264. 280. 296.] 19\n",
      "cx_grid (19, 19, 1) cy_grid (19, 19, 1)\n",
      "cx_grid[0] [[  8.]\n",
      " [ 24.]\n",
      " [ 40.]\n",
      " [ 56.]\n",
      " [ 72.]\n",
      " [ 88.]\n",
      " [104.]\n",
      " [120.]\n",
      " [136.]\n",
      " [152.]\n",
      " [168.]\n",
      " [184.]\n",
      " [200.]\n",
      " [216.]\n",
      " [232.]\n",
      " [248.]\n",
      " [264.]\n",
      " [280.]\n",
      " [296.]] \n",
      "cy_grid[0] [[8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]\n",
      " [8.]] \n",
      "300\n",
      "wh_list [[111.         111.        ]\n",
      " [156.97770542  78.48885271]\n",
      " [ 78.48885271 156.97770542]\n",
      " [192.25763964  64.08587988]\n",
      " [ 64.08587988 192.25763964]]\n",
      "feature_map_height 10, feature_map_width 10, feature_map_channels 24\n",
      "anchor_this_steps 32\n",
      "step_height 32 step_width 32\n",
      "anchor_this_offsets 0.5\n",
      "offset_height 0.5 ,offset_width 0.5\n",
      "offset_height * step_height 16.0\n",
      "(offset_height + feature_map_height - 1) * step_height 304.0\n",
      "feature_map_height 10\n",
      "cy [ 16.  48.  80. 112. 144. 176. 208. 240. 272. 304.] 10\n",
      "offset_width * step_width 16.0\n",
      "(offset_width + feature_map_width - 1) * step_width 304.0\n",
      "feature_map_width 10\n",
      "cx [ 16.  48.  80. 112. 144. 176. 208. 240. 272. 304.] 10\n",
      "cx_grid (10, 10, 1) cy_grid (10, 10, 1)\n",
      "cx_grid[0] [[ 16.]\n",
      " [ 48.]\n",
      " [ 80.]\n",
      " [112.]\n",
      " [144.]\n",
      " [176.]\n",
      " [208.]\n",
      " [240.]\n",
      " [272.]\n",
      " [304.]] \n",
      "cy_grid[0] [[16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]] \n",
      "300\n",
      "wh_list [[162.         162.        ]\n",
      " [229.1025971  114.55129855]\n",
      " [114.55129855 229.1025971 ]\n",
      " [280.59223083  93.53074361]\n",
      " [ 93.53074361 280.59223083]]\n",
      "feature_map_height 5, feature_map_width 5, feature_map_channels 24\n",
      "anchor_this_steps 64\n",
      "step_height 64 step_width 64\n",
      "anchor_this_offsets 0.5\n",
      "offset_height 0.5 ,offset_width 0.5\n",
      "offset_height * step_height 32.0\n",
      "(offset_height + feature_map_height - 1) * step_height 288.0\n",
      "feature_map_height 5\n",
      "cy [ 32.  96. 160. 224. 288.] 5\n",
      "offset_width * step_width 32.0\n",
      "(offset_width + feature_map_width - 1) * step_width 288.0\n",
      "feature_map_width 5\n",
      "cx [ 32.  96. 160. 224. 288.] 5\n",
      "cx_grid (5, 5, 1) cy_grid (5, 5, 1)\n",
      "cx_grid[0] [[ 32.]\n",
      " [ 96.]\n",
      " [160.]\n",
      " [224.]\n",
      " [288.]] \n",
      "cy_grid[0] [[32.]\n",
      " [32.]\n",
      " [32.]\n",
      " [32.]\n",
      " [32.]] \n",
      "300\n",
      "wh_list [[213.         213.        ]\n",
      " [301.22748879 150.61374439]\n",
      " [150.61374439 301.22748879]]\n",
      "feature_map_height 3, feature_map_width 3, feature_map_channels 16\n",
      "anchor_this_steps 100\n",
      "step_height 100 step_width 100\n",
      "anchor_this_offsets 0.5\n",
      "offset_height 0.5 ,offset_width 0.5\n",
      "offset_height * step_height 50.0\n",
      "(offset_height + feature_map_height - 1) * step_height 250.0\n",
      "feature_map_height 3\n",
      "cy [ 50. 150. 250.] 3\n",
      "offset_width * step_width 50.0\n",
      "(offset_width + feature_map_width - 1) * step_width 250.0\n",
      "feature_map_width 3\n",
      "cx [ 50. 150. 250.] 3\n",
      "cx_grid (3, 3, 1) cy_grid (3, 3, 1)\n",
      "cx_grid[0] [[ 50.]\n",
      " [150.]\n",
      " [250.]] \n",
      "cy_grid[0] [[50.]\n",
      " [50.]\n",
      " [50.]] \n",
      "300\n",
      "wh_list [[264.         264.        ]\n",
      " [373.35238047 186.67619023]\n",
      " [186.67619023 373.35238047]]\n",
      "feature_map_height 1, feature_map_width 1, feature_map_channels 16\n",
      "anchor_this_steps 300\n",
      "step_height 300 step_width 300\n",
      "anchor_this_offsets 0.5\n",
      "offset_height 0.5 ,offset_width 0.5\n",
      "offset_height * step_height 150.0\n",
      "(offset_height + feature_map_height - 1) * step_height 150.0\n",
      "feature_map_height 1\n",
      "cy [150.] 1\n",
      "offset_width * step_width 150.0\n",
      "(offset_width + feature_map_width - 1) * step_width 150.0\n",
      "feature_map_width 1\n",
      "cx [150.] 1\n",
      "cx_grid (1, 1, 1) cy_grid (1, 1, 1)\n",
      "cx_grid[0] [[150.]] \n",
      "cy_grid[0] [[150.]] \n"
     ]
    }
   ],
   "source": [
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]]\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "\n",
    "anchor_img_height = 300\n",
    "anchor_img_width = 300\n",
    "anchor_this_scale = scales_pascal[0]\n",
    "anchor_next_scale = scales_pascal[1]\n",
    "anchor_aspect_ratios = aspect_ratios[0]\n",
    "anchor_two_boxes_for_ar1 = True\n",
    "anchor_this_steps = steps[0]\n",
    "anchor_this_offsets = offsets[0]\n",
    "anchor_clip_boxes = False\n",
    "anchor_variances = variances\n",
    "anchor_coords = 'centroids'\n",
    "anchor_normalize_coords = True\n",
    "\n",
    "anchor_input_shape = [(None, 38, 38, 16),(None, 19, 19, 24),(None, 10, 10, 24),(None, 5, 5, 24)\\\n",
    "                     ,(None, 3, 3, 16),(None, 1, 1, 16)]\n",
    "\n",
    "for _curlayer in range(len(scales_pascal)-1):\n",
    "    anchor_this_scale = scales_pascal[_curlayer]\n",
    "    anchor_next_scale = scales_pascal[_curlayer+1]\n",
    "    anchor_aspect_ratios = aspect_ratios[_curlayer]\n",
    "    anchor_this_steps = steps[_curlayer]\n",
    "    anchor_this_offsets = offsets[_curlayer]\n",
    "    \n",
    "    anchor_size = min(anchor_img_height, anchor_img_width)\n",
    "    print(anchor_size)\n",
    "    \n",
    "    # Compute the box widths and and heights for all aspect ratios\n",
    "    wh_list = []\n",
    "    for ar in anchor_aspect_ratios:\n",
    "        box_height = anchor_this_scale * anchor_size / np.sqrt(ar)\n",
    "        box_width = anchor_this_scale * anchor_size * np.sqrt(ar)\n",
    "        wh_list.append((box_width, box_height))\n",
    "    wh_list = np.array(wh_list)\n",
    "    print(\"wh_list {}\".format(wh_list))\n",
    "    \n",
    "    batch_size, feature_map_height, feature_map_width, feature_map_channels = anchor_input_shape[_curlayer]\n",
    "    print(\"feature_map_height {}, feature_map_width {}, feature_map_channels {}\".format(feature_map_height, feature_map_width, feature_map_channels))\n",
    "    print(\"anchor_this_steps {}\".format(anchor_this_steps))\n",
    "    step_height = anchor_this_steps\n",
    "    step_width = anchor_this_steps\n",
    "    print(\"step_height {} step_width {}\".format(step_height,step_width))    \n",
    "    \n",
    "    print(\"anchor_this_offsets {}\".format(anchor_this_offsets))\n",
    "    offset_height = anchor_this_offsets\n",
    "    offset_width = anchor_this_offsets\n",
    "    print(\"offset_height {} ,offset_width {}\".format(offset_height,offset_width))\n",
    "    \n",
    "    # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
    "    cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
    "    cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
    "    cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
    "    cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "    cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "    print(\"offset_height * step_height {}\".format(offset_height * step_height))\n",
    "    print(\"(offset_height + feature_map_height - 1) * step_height {}\".format((offset_height + feature_map_height - 1) * step_height))\n",
    "    print(\"feature_map_height {}\".format(feature_map_height))\n",
    "    print(\"cy {} {}\".format(cy,len(cy)))\n",
    "\n",
    "    print(\"offset_width * step_width {}\".format(offset_width * step_width))\n",
    "    print(\"(offset_width + feature_map_width - 1) * step_width {}\".format((offset_width + feature_map_width - 1) * step_width))\n",
    "    print(\"feature_map_width {}\".format(feature_map_width))\n",
    "    print(\"cx {} {}\".format(cx,len(cx)))\n",
    "    print(\"cx_grid {} cy_grid {}\".format(cx_grid.shape,cy_grid.shape))\n",
    "    print(\"cx_grid[0] {} \".format(cx_grid[0]))\n",
    "    print(\"cy_grid[0] {} \".format(cy_grid[0]))\n",
    "#     break\n",
    "#     print(\"cx_grid {},cy_grid {}\".format(cx_grid,cy_grid))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```txt\n",
    "\n",
    "    A generator to generate batches of samples and corresponding labels indefinitely.\n",
    "\n",
    "    Can shuffle the dataset consistently after each complete pass.\n",
    "\n",
    "    Currently provides three methods to parse annotation data: A general-purpose CSV parser,\n",
    "    an XML parser for the Pascal VOC datasets, and a JSON parser for the MS COCO datasets.\n",
    "    If the annotations of your dataset are in a format that is not supported by these parsers,\n",
    "    you could just add another parser method and still use this generator.\n",
    "\n",
    "    Can perform image transformations for data conversion and data augmentation,\n",
    "    for details please refer to the documentation of the `generate()` method.\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    " load_images_into_memory=False\n",
    " hdf5_dataset_path=None\n",
    " filenames=None\n",
    " filenames_type='text'\n",
    " images_dir=None\n",
    " labels=None\n",
    " image_ids=None\n",
    " eval_neutral=None\n",
    " labels_output_format=('class_id', 'xmin', 'ymin', 'xmax', 'ymax')\n",
    " verbose=True\n",
    "'''\n",
    "Initializes the data generator. You can either load a dataset directly here in the constructor,\n",
    "e.g. an HDF5 dataset, or you can use one of the parser methods to read in a dataset.\n",
    "\n",
    "Arguments:\n",
    "load_images_into_memory (bool, optional): If `True`, the entire dataset will be loaded into memory.\n",
    "    This enables noticeably faster data generation than loading batches of images into memory ad hoc.\n",
    "    Be sure that you have enough memory before you activate this option.\n",
    "    \n",
    "hdf5_dataset_path (str, optional): The full file path of an HDF5 file that contains a dataset in the\n",
    "    format that the `create_hdf5_dataset()` method produces. If you load such an HDF5 dataset, you\n",
    "    don't need to use any of the parser methods anymore, the HDF5 dataset already contains all relevant\n",
    "    data.\n",
    "    \n",
    "filenames (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
    "    a filepath. If a list/tuple is passed, it must contain the file names (full paths) of the\n",
    "    images to be used. Note that the list/tuple must contain the paths to the images,\n",
    "    not the images themselves. If a filepath string is passed, it must point either to\n",
    "    (1) a pickled file containing a list/tuple as described above. In this case the `filenames_type`\n",
    "    argument must be set to `pickle`.\n",
    "    Or\n",
    "    (2) a text file. Each line of the text file contains the file name (basename of the file only,\n",
    "    not the full directory path) to one image and nothing else. In this case the `filenames_type`\n",
    "    argument must be set to `text` and you must pass the path to the directory that contains the\n",
    "    images in `images_dir`.\n",
    "    \n",
    "filenames_type (string, optional): In case a string is passed for `filenames`, this indicates what\n",
    "    type of file `filenames` is. It can be either 'pickle' for a pickled file or 'text' for a\n",
    "    plain text file.\n",
    "    \n",
    "images_dir (string, optional): In case a text file is passed for `filenames`, the full paths to\n",
    "    the images will be composed from `images_dir` and the names in the text file, i.e. this\n",
    "    should be the directory that contains the images to which the text file refers.\n",
    "    If `filenames_type` is not 'text', then this argument is irrelevant.\n",
    "    \n",
    "labels (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
    "    the path to a pickled file containing a list/tuple. The list/tuple must contain Numpy arrays\n",
    "    that represent the labels of the dataset.\n",
    "    \n",
    "image_ids (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
    "    the path to a pickled file containing a list/tuple. The list/tuple must contain the image\n",
    "    IDs of the images in the dataset.\n",
    "    \n",
    "eval_neutral (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
    "    the path to a pickled file containing a list/tuple. The list/tuple must contain for each image\n",
    "    a list that indicates for each ground truth object in the image whether that object is supposed\n",
    "    to be treated as neutral during an evaluation.\n",
    "    \n",
    "labels_output_format (list, optional): A list of five strings representing the desired order of the five\n",
    "    items class ID, xmin, ymin, xmax, ymax in the generated ground truth data (if any). The expected\n",
    "    strings are 'xmin', 'ymin', 'xmax', 'ymax', 'class_id'.\n",
    "    \n",
    "verbose (bool, optional): If `True`, prints out the progress for some constructor operations that may\n",
    "    take a bit longer.\n",
    "'''\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_labels_output_format = labels_output_format\n",
    "gen_labels_format={'class_id': labels_output_format.index('class_id') # 0,\n",
    "                    'xmin': labels_output_format.index('xmin')# 1,\n",
    "                    'ymin': labels_output_format.index('ymin')# 2,\n",
    "                    'xmax': labels_output_format.index('xmax')# 3,\n",
    "                    'ymax': labels_output_format.index('ymax')# 4} # This dictionary is for internal use.\n",
    "\n",
    "gen_dataset_size = 0 # As long as we haven't loaded anything yet, the dataset size is zero.\n",
    "gen_load_images_into_memory = load_images_into_memory\n",
    "gen_images = None # The only way that this list will not stay `None` is if `load_images_into_memory == True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "labels_output_format = ['class_id','xmin']\n",
    "labels_output_format.index('xmin')\n",
    ">> 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def parse_xml(self,\n",
    "              images_dirs,\n",
    "              image_set_filenames,\n",
    "              annotations_dirs=[],\n",
    "              classes=['background',\n",
    "                       'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                       'bottle', 'bus', 'car', 'cat',\n",
    "                       'chair', 'cow', 'diningtable', 'dog',\n",
    "                       'horse', 'motorbike', 'person', 'pottedplant',\n",
    "                       'sheep', 'sofa', 'train', 'tvmonitor'],\n",
    "              include_classes = 'all',\n",
    "              exclude_truncated=False,\n",
    "              exclude_difficult=False,\n",
    "              ret=False,\n",
    "              verbose=True):\n",
    "'''\n",
    "This is an XML parser for the Pascal VOC datasets. It might be applicable to other datasets with minor changes to\n",
    "the code, but in its current form it expects the data format and XML tags of the Pascal VOC datasets.\n",
    "\n",
    "Arguments:\n",
    "images_dirs (list): A list of strings, where each string is the path of a directory that\n",
    "    contains images that are to be part of the dataset. This allows you to aggregate multiple datasets\n",
    "    into one (e.g. one directory that contains the images for Pascal VOC 2007, another that contains\n",
    "    the images for Pascal VOC 2012, etc.).\n",
    "    \n",
    "image_set_filenames (list): A list of strings, where each string is the path of the text file with the image\n",
    "    set to be loaded. Must be one file per image directory given. These text files define what images in the\n",
    "    respective image directories are to be part of the dataset and simply contains one image ID per line\n",
    "    and nothing else.\n",
    "    \n",
    "annotations_dirs (list, optional): A list of strings, where each string is the path of a directory that\n",
    "    contains the annotations (XML files) that belong to the images in the respective image directories given.\n",
    "    The directories must contain one XML file per image and the name of an XML file must be the image ID\n",
    "    of the image it belongs to. The content of the XML files must be in the Pascal VOC format.\n",
    "    \n",
    "classes (list, optional): A list containing the names of the object classes as found in the\n",
    "    `name` XML tags. Must include the class `background` as the first list item. The order of this list\n",
    "    defines the class IDs.\n",
    "    \n",
    "include_classes (list, optional): Either 'all' or a list of integers containing the class IDs that\n",
    "    are to be included in the dataset. If 'all', all ground truth boxes will be included in the dataset.\n",
    "    \n",
    "exclude_truncated (bool, optional): If `True`, excludes boxes that are labeled as 'truncated'.\n",
    "\n",
    "exclude_difficult (bool, optional): If `True`, excludes boxes that are labeled as 'difficult'.\n",
    "\n",
    "ret (bool, optional): Whether or not to return the outputs of the parser.\n",
    "\n",
    "verbose (bool, optional): If `True`, prints out the progress for operations that may take a bit longer.\n",
    "\n",
    "Returns:\n",
    "None by default, optionally lists for whichever are available of images, image filenames, labels, image IDs,\n",
    "and a list indicating which boxes are annotated with the label \"difficult\".\n",
    "'''\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir,\n",
    "                                     VOC_2012_images_dir],\n",
    "                        image_set_filenames=[VOC_2007_trainval_image_set_filename,\n",
    "                                             VOC_2012_trainval_image_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir,\n",
    "                                          VOC_2012_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation and Encoding Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SSDInputEncoder:\n",
    "'''\n",
    "Transforms ground truth labels for object detection in images\n",
    "(2D bounding box coordinates and class labels) to the format required for\n",
    "training an SSD model.\n",
    "\n",
    "In the process of encoding the ground truth labels, a template of anchor boxes\n",
    "is being built, which are subsequently matched to the ground truth boxes\n",
    "via an intersection-over-union threshold criterion.\n",
    "'''\n",
    "\n",
    "def __init__(self,\n",
    "             img_height,\n",
    "             img_width,\n",
    "             n_classes,\n",
    "             predictor_sizes,\n",
    "             min_scale=0.1,\n",
    "             max_scale=0.9,\n",
    "             scales=None,\n",
    "             aspect_ratios_global=[0.5, 1.0, 2.0],\n",
    "             aspect_ratios_per_layer=None,\n",
    "             two_boxes_for_ar1=True,\n",
    "             steps=None,\n",
    "             offsets=None,\n",
    "             clip_boxes=False,\n",
    "             variances=[0.1, 0.1, 0.2, 0.2],\n",
    "             matching_type='multi',\n",
    "             pos_iou_threshold=0.5,\n",
    "             neg_iou_limit=0.3,\n",
    "             border_pixels='half',\n",
    "             coords='centroids',\n",
    "             normalize_coords=True,\n",
    "             background_id=0):\n",
    "   \n",
    "```\n",
    "\n",
    "Arguments:  \n",
    "- img_height (int): The height of the input images.\n",
    "    \n",
    "- img_width (int): The width of the input images.\n",
    "    \n",
    "- n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n",
    "    \n",
    "- predictor_sizes (list): A list of int-tuples of the format `(height, width)`\n",
    "        containing the output heights and widths of the convolutional predictor layers.\n",
    "        \n",
    "- min_scale (float, optional): The smallest scaling factor for the size of the anchor boxes as a fraction\n",
    "        of the shorter side of the input images. Note that you should set the scaling factors\n",
    "        such that the resulting anchor box sizes correspond to the sizes of the objects you are trying\n",
    "        to detect. Must be >0.\n",
    "        \n",
    "- max_scale (float, optional): The largest scaling factor for the size of the anchor boxes as a fraction\n",
    "        of the shorter side of the input images. All scaling factors between the smallest and the\n",
    "        largest will be linearly interpolated. Note that the second to last of the linearly interpolated\n",
    "        scaling factors will actually be the scaling factor for the last predictor layer, while the last\n",
    "        scaling factor is used for the second box for aspect ratio 1 in the last predictor layer\n",
    "        if `two_boxes_for_ar1` is `True`. Note that you should set the scaling factors\n",
    "        such that the resulting anchor box sizes correspond to the sizes of the objects you are trying\n",
    "        to detect. Must be greater than or equal to `min_scale`.\n",
    "        \n",
    "- scales (list, optional): A list of floats >0 containing scaling factors per convolutional predictor layer.\n",
    "        This list must be one element longer than the number of predictor layers. The first `k` elements are the\n",
    "        scaling factors for the `k` predictor layers, while the last element is used for the second box\n",
    "        for aspect ratio 1 in the last predictor layer if `two_boxes_for_ar1` is `True`. This additional\n",
    "        last scaling factor must be passed either way, even if it is not being used. If a list is passed,\n",
    "        this argument overrides `min_scale` and `max_scale`. All scaling factors must be greater than zero.\n",
    "        Note that you should set the scaling factors such that the resulting anchor box sizes correspond to\n",
    "        the sizes of the objects you are trying to detect.\n",
    "        \n",
    "- aspect_ratios_global (list, optional): The list of aspect ratios for which anchor boxes are to be\n",
    "        generated. This list is valid for all prediction layers. Note that you should set the aspect ratios such\n",
    "        that the resulting anchor box shapes roughly correspond to the shapes of the objects you are trying to \n",
    "        detect.\n",
    "- aspect_ratios_per_layer (list, optional): A list containing one aspect ratio list for each prediction layer.\n",
    "        If a list is passed, it overrides `aspect_ratios_global`. Note that you should set the aspect ratios such\n",
    "        that the resulting anchor box shapes very roughly correspond to the shapes of the objects you are trying \n",
    "        to detect.\n",
    "        \n",
    "- two_boxes_for_ar1 (bool, optional): Only relevant for aspect ratios lists that contain 1. Will be ignored \n",
    "        otherwise.\n",
    "        If `True`, two anchor boxes will be generated for aspect ratio 1. The first will be generated\n",
    "        using the scaling factor for the respective layer, the second one will be generated using\n",
    "        geometric mean of said scaling factor and next bigger scaling factor.\n",
    "        \n",
    "- steps (list, optional): `None` or a list with as many elements as there are predictor layers. \n",
    "        The elements can be either ints/floats or tuples of two ints/floats. These numbers represent for each \n",
    "        predictor layer how many pixels apart the anchor box center points should be vertically and horizontally \n",
    "        along the spatial grid over the image. If the list contains ints/floats, then that value will be used for \n",
    "        both spatial dimensions.\n",
    "        If the list contains tuples of two ints/floats, then they represent `(step_height, step_width)`.\n",
    "        If no steps are provided, then they will be computed such that the anchor box center points will form an\n",
    "        equidistant grid within the image dimensions.\n",
    "        \n",
    "- offsets (list, optional): `None` or a list with as many elements as there are predictor layers. \n",
    "        The elements can be either floats or tuples of two floats. These numbers represent for each predictor \n",
    "        layer how many pixels from the top and left boarders of the image the top-most and left-most anchor box \n",
    "        center points should be as a fraction of `steps`. The last bit is important: The offsets are not absolute \n",
    "        pixel values, but fractions of the step size specified in the `steps` argument. If the list contains \n",
    "        floats, then that value will be used for both spatial dimensions. If the list contains tuples of two \n",
    "        floats, then they represent `(vertical_offset, horizontal_offset)`. If no offsets are provided, then they \n",
    "        will default to 0.5 of the step size.\n",
    "\n",
    "- clip_boxes (bool, optional): If `True`, limits the anchor box coordinates to stay within image boundaries.\n",
    "\n",
    "- variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n",
    "        its respective variance value.\n",
    "        \n",
    "- matching_type (str, optional): Can be either 'multi' or 'bipartite'. In 'bipartite' mode, each ground truth box \n",
    "        will be matched only to the one anchor box with the highest IoU overlap. In 'multi' mode, in addition to \n",
    "        the aforementioned bipartite matching, all anchor boxes with an IoU overlap greater than or equal to the \n",
    "        `pos_iou_threshold` will be matched to a given ground truth box.\n",
    "        \n",
    "- pos_iou_threshold (float, optional): The intersection-over-union similarity threshold that must be met in order \n",
    "        to match a given ground truth box to a given anchor box.\n",
    "        \n",
    "- neg_iou_limit (float, optional): The maximum allowed intersection-over-union similarity of an anchor box with \n",
    "        any ground truth box to be labeled a negative (i.e. background) box. If an anchor box is neither a \n",
    "        positive, nor a negative box, it will be ignored during training.\n",
    "        \n",
    "- border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "        Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong to the boxes. If 'exclude', \n",
    "        the border pixels do not belong to the boxes. If 'half', then one of each of the two horizontal and \n",
    "        vertical borders belong to the boxex, but not the other.\n",
    "        \n",
    "- coords (str, optional): The box coordinate format to be used internally by the model (i.e. this is not the input \n",
    "        format of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center \n",
    "        coordinates, width, and height), 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the \n",
    "        format `(xmin, ymin, xmax, ymax)`.\n",
    "\n",
    "- normalize_coords (bool, optional): If `True`, the encoder uses relative instead of absolute coordinates.\n",
    "      This means instead of using absolute tartget coordinates, the encoder will scale all coordinates to be \n",
    "      within [0,1].  This way learning becomes independent of the input image size.\n",
    "      \n",
    "- background_id (int, optional): Determines which class ID is for the background class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 32 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "# predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "#                    model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "#                    model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "#                    model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "#                    model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "#                    model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "predictor_sizes = [(38, 38, 16),(19, 19, 24),(10, 10, 24),(5, 5, 24)\\\n",
    "                     ,(3, 3, 16),(1, 1, 16)]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "class SSDInputEncoder:\n",
    "    def __init__()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_sizes = [(38, 38, 16),(19, 19, 24),(10, 10, 24),(5, 5, 24)\\\n",
    "                     ,(3, 3, 16),(1, 1, 16)]\n",
    "\n",
    "predictor_sizes = np.array(predictor_sizes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Set or compute members.\n",
    "##################################################################################\n",
    "min_scale=0.1,\n",
    "max_scale=0.9\n",
    "\n",
    "encoder_img_height = img_height\n",
    "encoder_img_width = img_width\n",
    "encoder_n_classes = n_classes + 1 # + 1 for the background class\n",
    "encoder_predictor_sizes = predictor_sizes\n",
    "encoder_min_scale = min_scale\n",
    "encoder_max_scale = max_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_scales = scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "\n",
    "encoder_aspect_ratios = aspect_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_two_boxes_for_ar1 = two_boxes_for_ar1 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_steps = steps = steps = [8, 16, 32, 64, 100, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_offsets = offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_clip_boxes = clip_boxes = False\n",
    "encoder_variances = variances = [0.1, 0.1, 0.2, 0.2]\n",
    "encoder_matching_type = matching_type = 'multi'\n",
    "encoder_pos_iou_threshold = pos_iou_threshold = 0.5\n",
    "encoder_neg_iou_limit = neg_iou_limit = 0.5\n",
    "encoder_border_pixels = border_pixels='half'\n",
    "encoder_coords = coords = 'centroids'\n",
    "encoder_normalize_coords = normalize_coords = True\n",
    "encoder_background_id = background_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_n_boxes = []\n",
    "for aspect_ratios in encoder_aspect_ratios:\n",
    "    if (1 in aspect_ratios) & two_boxes_for_ar1:\n",
    "        encoder_n_boxes.append(len(aspect_ratios) + 1)\n",
    "    else:\n",
    "        encoder_n_boxes.append(len(aspect_ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 6, 6, 4, 4]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_n_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Compute the anchor boxes for each predictor layer.\n",
    "##################################################################################\n",
    "\n",
    "# Compute the anchor boxes for each predictor layer. We only have to do this once\n",
    "# since the anchor boxes depend only on the model configuration, not on the input data.\n",
    "# For each predictor layer (i.e. for each scaling factor) the tensors for that layer's\n",
    "# anchor boxes will have the shape `(feature_map_height, feature_map_width, n_boxes, 4)`.\n",
    "\n",
    "encoder_boxes_list = [] # This will store the anchor boxes for each predicotr layer.\n",
    "\n",
    "# The following lists just store diagnostic information. Sometimes it's handy to have the\n",
    "# boxes' center points, heights, widths, etc. in a list.\n",
    "encoder_wh_list_diag = [] # Box widths and heights for each predictor layer\n",
    "encoder_steps_diag = [] # Horizontal and vertical distances between any two boxes for each predictor layer\n",
    "encoder_offsets_diag = [] # Offsets for each predictor layer\n",
    "encoder_centers_diag = [] # Anchor box center points as `(cy, cx)` for each predictor layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "#  class SSDInputEncoder:\n",
    " end of def __init__()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_anchor_boxes_for_layer  \n",
    "Returns:\n",
    "- A 4D Numpy tensor of shape `(feature_map_height, feature_map_width, n_boxes_per_cell, 4)`   \n",
    "    where the last dimension contains `(xmin, xmax, ymin, ymax)` \n",
    "    for each anchor box in each cell of the feature map.  \n",
    "    `(xmin, xmax, ymin, ymax)` is normalized between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all predictor layers and compute the anchor boxes for each one.\n",
    "for i in range(len(self.predictor_sizes)):\n",
    "    boxes, center, wh, step, offset = generate_anchor_boxes_for_layer(feature_map_size=encoder_predictor_sizes[i],\n",
    "                                                                           aspect_ratios=encoder_aspect_ratios[i],\n",
    "                                                                           this_scale=encoder_scales[i],\n",
    "                                                                           next_scale=encoder_scales[i+1],\n",
    "                                                                           this_steps=encoder_steps[i],\n",
    "                                                                           this_offsets=encoder_offsets[i],\n",
    "                                                                           diagnostics=True)\n",
    "    encoder_boxes_list.append(boxes)\n",
    "    encoder_wh_list_diag.append(wh)\n",
    "    encoder_steps_diag.append(step)\n",
    "    encoder_offsets_diag.append(offset)\n",
    "    encoder_centers_diag.append(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts ground truth bounding box data into a suitable format to train an SSD model.\n",
    "\n",
    "Arguments:\n",
    "- ground_truth_labels (list): A python list of length `batch_size` that contains one 2D Numpy array for each batch \n",
    "        image. Each such array has `k` rows for the `k` ground truth bounding boxes belonging to the respective \n",
    "        image, and the data for each ground truth bounding box has the format `(class_id, xmin, ymin, xmax, ymax)` \n",
    "        (i.e. the 'corners' coordinate format), and `class_id` must be an integer greater than 0 for all boxes as \n",
    "        class ID 0 is reserved for the background class.\n",
    "- diagnostics (bool, optional): If `True`, not only the encoded ground truth tensor will be returned,\n",
    "        but also a copy of it with anchor box coordinates in place of the ground truth coordinates.\n",
    "        This can be very useful if you want to visualize which anchor boxes got matched to which ground truth\n",
    "        boxes.\n",
    "\n",
    "Returns:\n",
    "\n",
    "    `y_encoded`, a 3D numpy array of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)` that serves as the\n",
    "    ground truth label tensor for training, where `#boxes` is the total number of boxes predicted by the\n",
    "    model per image, and the classes are one-hot-encoded. The four elements after the class vecotrs in\n",
    "    the last axis are the box coordinates, the next four elements after that are just dummy elements, and\n",
    "    the last four elements are the variances.\n",
    "        \n",
    "\n",
    "```python\n",
    "\n",
    "class SSDInputEncoder:\n",
    "    def __call__(self, ground_truth_labels, diagnostics=False):\n",
    "        # Mapping to define which indices represent which coordinates in the ground truth.\n",
    "        class_id = 0\n",
    "        xmin = 1\n",
    "        ymin = 2\n",
    "        xmax = 3\n",
    "        ymax = 4\n",
    "        \n",
    "        batch_size = len(ground_truth_labels)\n",
    "        \n",
    "        ##################################################################################\n",
    "        # Generate the template for y_encoded.\n",
    "        ##################################################################################\n",
    "        '(batch_size, #boxes, #classes + 12)'\n",
    "        the template into which to encode the ground truth labels for training. The last axis has length \n",
    "        '#classes + 12' because the model output contains not only the '4 predicted box coordinate offsets'\n",
    "        , but also the '4 coordinates for the anchor boxes' and the '4 variance values'.\n",
    "          \n",
    "        y_encoded = self.generate_encoding_template(batch_size=batch_size, diagnostics=False)\n",
    "        \n",
    "        ##################################################################################\n",
    "        # Match ground truth boxes to anchor boxes.\n",
    "        ##################################################################################\n",
    "\n",
    "        # Match the ground truth boxes to the anchor boxes. Every anchor box that does not have\n",
    "        # a ground truth match and for which the maximal IoU overlap with any ground truth box is less\n",
    "        # than or equal to `neg_iou_limit` will be a negative (background) box.\n",
    "        '(batch, feature_map_height*feature_map_width*n_boxes,4(predicted coord,anchor boxes)+4(dummy  \n",
    "            'coord)+4(variance))'  \n",
    "        y_encoded[:, :, self.background_id] = 1 # All boxes are background boxes by default.\n",
    "        n_boxes = y_encoded.shape[1] # The total number of boxes that the model predicts per batch item\n",
    "        class_vectors = np.eye(self.n_classes) # An identity matrix that we'll use as one-hot class vectors\n",
    "        \n",
    "        for i in range(batch_size): # For each batch item...\n",
    "            labels = ground_truth_labels[i].astype(np.float) # The labels for this batch item\n",
    "            # Maybe normalize the box coordinates.\n",
    "            if self.normalize_coords:# True\n",
    "                labels[:,[ymin,ymax]] /= self.img_height # Normalize ymin and ymax relative to the image height\n",
    "                labels[:,[xmin,xmax]] /= self.img_width # Normalize xmin and xmax relative to the image width\n",
    "            # Maybe convert the box coordinate format.\n",
    "            if self.coords == 'centroids':#(xmin, ymin, xmax, ymax) -> (cx,cy,w,h)\n",
    "                labels = convert_coordinates(labels, start_index=xmin, conversion='corners2centroids',  \n",
    "                                             border_pixels=self.border_pixels)   \n",
    "            classes_one_hot = class_vectors[labels[:, class_id].astype(np.int)] # The one-hot class IDs\n",
    "                                                         #for the ground truth boxes of this batch item\n",
    "            labels_one_hot = np.concatenate([classes_one_hot, labels[:, [xmin,ymin,xmax,ymax]]], axis=-1)\n",
    "            # The one-hot version of the labels for this batch item\n",
    "\n",
    "            # Compute the IoU similarities between all anchor boxes and all ground truth boxes \n",
    "            # for this batch item.\n",
    "            # This is a matrix of shape `(num_ground_truth_boxes, num_anchor_boxes)`.\n",
    "            # iou returns  values in [0,1]\n",
    "            similarities = iou(labels[:,[xmin,ymin,xmax,ymax]], y_encoded[i,:,-12:-8], coords=self.coords,\n",
    "                               mode='outer_product', border_pixels=self.border_pixels)\n",
    "\n",
    "            # First: Do bipartite matching, i.e. match each ground truth box to the one anchor box\n",
    "            # with the highest IoU.\n",
    "            # This ensures that each ground truth box will have at least one good match.\n",
    "\n",
    "            # For each ground truth box, get the anchor box to match with it.\n",
    "            # match_bipartite_greedy returns matches[ground_truth_index] = anchor_index list\n",
    "            bipartite_matches = match_bipartite_greedy(weight_matrix=similarities)\n",
    "            \n",
    "            # Write the ground truth data to the matched anchor boxes.\n",
    "            'bipartite_matches : anchorboxes indices list <-assign ground truth class, 4 coords '\n",
    "            y_encoded[i, bipartite_matches, :-8] = labels_one_hot\n",
    "            \n",
    "            # Set the columns of the matched anchor boxes to zero to indicate that they were matched.\n",
    "            similarities[:, bipartite_matches] = 0\n",
    "            \n",
    "            # Second: Maybe do 'multi' matching, where each remaining anchor box will be matched to its most \n",
    "            # similar ground truth box with an IoU of at least `pos_iou_threshold`, \n",
    "            # or not matched if there is no such ground truth box.\n",
    "\n",
    "            if self.matching_type == 'multi':\n",
    "\n",
    "                # Get all matches that satisfy the IoU threshold.\n",
    "                matches = match_multi(weight_matrix=similarities, threshold=self.pos_iou_threshold)\n",
    "\n",
    "                # Write the ground truth data to the matched anchor boxes.\n",
    "                y_encoded[i, matches[1], :-8] = labels_one_hot[matches[0]]\n",
    "\n",
    "                # Set the columns of the matched anchor boxes to zero to indicate that they were matched.\n",
    "                similarities[:, matches[1]] = 0\n",
    "            \n",
    "            # Third: Now after the matching is done, all negative (background) anchor boxes that have  \n",
    "            # an IoU of `neg_iou_limit` or more with any ground truth box will be set to netral,  \n",
    "            # i.e. they will no longer be background boxes. These anchors are \"too close\" to a  \n",
    "            # ground truth box to be valid background boxes.\n",
    "\n",
    "            max_background_similarities = np.amax(similarities, axis=0)\n",
    "            neutral_boxes = np.nonzero(max_background_similarities >= self.neg_iou_limit)[0]\n",
    "            y_encoded[i, neutral_boxes, self.background_id] = 0  \n",
    "            \n",
    "            if self.coords == 'centroids':\n",
    "                # cx(gt) - cx(anchor), cy(gt) - cy(anchor)\n",
    "                y_encoded[:,:,[-12,-11]] -= y_encoded[:,:,[-8,-7]] \n",
    "                # (cx(gt) - cx(anchor)) / w(anchor) / cx_variance, \n",
    "                # (cy(gt) - cy(anchor)) / h(anchor) / cy_variance\n",
    "                y_encoded[:,:,[-12,-11]] /= y_encoded[:,:,[-6,-5]] * y_encoded[:,:,[-4,-3]]\n",
    "                # w(gt) / w(anchor), h(gt) / h(anchor)\n",
    "                y_encoded[:,:,[-10,-9]] /= y_encoded[:,:,[-6,-5]] \n",
    "                # ln(w(gt) / w(anchor)) / w_variance, \n",
    "                # ln(h(gt) / h(anchor)) / h_variance (ln == natural logarithm)\n",
    "                y_encoded[:,:,[-10,-9]] = np.log(y_encoded[:,:,[-10,-9]]) / y_encoded[:,:,[-2,-1]] \n",
    "            return y_encoded\n",
    "        \n",
    "\n",
    "    def generate_encoding_template(self, batch_size, diagnostics=False):\n",
    "    '''\n",
    "    Arguments:\n",
    "        batch_size (int): The batch size.\n",
    "        diagnostics (bool, optional): See the documnentation for `generate_anchor_boxes()`. The diagnostic \n",
    "        output here is similar, just for all predictor conv layers.\n",
    "\n",
    "    Returns:\n",
    "        A Numpy array of shape `(batch_size, #boxes, #classes + 12)`, the template into which to encode\n",
    "        the ground truth labels for training. The last axis has length `#classes + 12` because the model\n",
    "        output contains not only the 4 predicted box coordinate offsets, but also the 4 coordinates for\n",
    "        the anchor boxes and the 4 variance values.\n",
    "    '''\n",
    "        # Tile the anchor boxes for each predictor layer across all batch items.        \n",
    "        boxes_batch = []\n",
    "        for boxes in encoder_boxes_list:\n",
    "            # Prepend one dimension to `self.boxes_list` to account for the batch size and tile it along.\n",
    "            # The result will be a 5D tensor of shape \n",
    "            '(batch_size, feature_map_height, feature_map_width , _boxes, #4)'\n",
    "            boxes = np.expand_dims(boxes, axis=0)\n",
    "            boxes = np.tile(boxes, (batch_size, 1, 1, 1, 1))\n",
    "\n",
    "            # Now reshape the 5D tensor above into a 3D tensor of shape\n",
    "            '(batch, feature_map_height * feature_map_width * n_boxes, 4)'\n",
    "            #. The resulting order of the tensor content will be identical to the order obtained from the \n",
    "            # reshaping operation in our Keras model (we're using the Tensorflow backend, and tf.reshape() and \n",
    "            # np.reshape() use the same default index order, which is C-like index ordering)\n",
    "            boxes = np.reshape(boxes, (batch_size, -1, 4))\n",
    "            boxes_batch.append(boxes)\n",
    "\n",
    "        '(batch, feature_map_height * feature_map_width * n_boxes, 4)'\n",
    "        # Concatenate the anchor tensors from the individual layers to one.\n",
    "        boxes_tensor = np.concatenate(boxes_batch, axis=1)\n",
    "        \n",
    "        # 3: Create a template tensor to hold the one-hot class encodings of shape \n",
    "        '(batch, #boxes, #classes)'\n",
    "        # It will contain all zeros for now, the classes will be set in the matching process that follows\n",
    "        classes_tensor = np.zeros((batch_size, boxes_tensor.shape[1], encoder_n_classes))\n",
    "        \n",
    "        # 4: Create a tensor to contain the variances. This tensor has the same shape \n",
    "        # as `boxes_tensor` and simply contains the same 4 variance values for every position \n",
    "        # in the last axis.\n",
    "        variances_tensor = np.zeros_like(boxes_tensor)\n",
    "        variances_tensor += encoder_variances # Long live broadcasting\n",
    "        \n",
    "        # 4: Concatenate the classes, boxes and variances tensors to get our final template for y_encoded. \n",
    "        # We also need another tensor of the shape of `boxes_tensor` as a space filler so that \n",
    "        # `y_encoding_template` has the same shape as the SSD model output tensor. \n",
    "        # The content of this tensor is irrelevant, we'll just use `boxes_tensor` a second time.\n",
    "        y_encoding_template = np.concatenate((classes_tensor, boxes_tensor, boxes_tensor, variances_tensor),\\\n",
    "                                             axis=2)\n",
    "        \n",
    "        return y_encoding_template\n",
    "            \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if self.coords == 'centroids':\n",
    "    # cx(gt) - cx(anchor), cy(gt) - cy(anchor)\n",
    "    y_encoded[:,:,[-12,-11]] -= y_encoded[:,:,[-8,-7]] \n",
    "    # (cx(gt) - cx(anchor)) / w(anchor) / cx_variance, \n",
    "    # (cy(gt) - cy(anchor)) / h(anchor) / cy_variance\n",
    "    y_encoded[:,:,[-12,-11]] /= y_encoded[:,:,[-6,-5]] * y_encoded[:,:,[-4,-3]]\n",
    "    # w(gt) / w(anchor), h(gt) / h(anchor)\n",
    "    y_encoded[:,:,[-10,-9]] /= y_encoded[:,:,[-6,-5]] \n",
    "    # ln(w(gt) / w(anchor)) / w_variance, \n",
    "    # ln(h(gt) / h(anchor)) / h_variance (ln == natural logarithm)\n",
    "    y_encoded[:,:,[-10,-9]] = np.log(y_encoded[:,:,[-10,-9]]) / y_encoded[:,:,[-2,-1]] \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*cIE7bbicMOokWQ6w41I-NA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_encoded.shape (10, 20, 33)\n",
      "matches (5,)\n",
      "[0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, #boxes, #classes + 12)\n",
    "y_encoded = np.zeros((10,20,21+12))\n",
    "print(\"y_encoded.shape {}\".format(y_encoded.shape))\n",
    "\n",
    "num_ground_truth_boxes = 5\n",
    "matches = np.zeros(num_ground_truth_boxes, dtype=np.int)\n",
    "print(\"matches {}\".format(matches.shape))\n",
    "matches[2] = 1\n",
    "print(matches)\n",
    "bipartite_matches = matches\n",
    "\n",
    "y_encoded[0, bipartite_matches, :-8] = 2\n",
    "y_encoded[0,[0],:-8] = 1\n",
    "y_encoded[0,[1],:-8] = 2\n",
    "y_encoded[0,[2],:-8] = 3\n",
    "y_encoded[0,[0,1,0]]\n",
    "y_encoded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "# boxes1 - labels[:,[xmin,ymin,xmax,ymax]], : ground truth \n",
    "# boxes2 - y_encoded[i,:,-12:-8] : anchor boxes\n",
    "def iou(boxes1, boxes2, coords='centroids', mode='outer_product', border_pixels='half'):\n",
    "    '''\n",
    "    Computes the intersection-over-union similarity (also known as Jaccard similarity)  \n",
    "    of two sets of axis-aligned 2D rectangular boxes.  \n",
    "\n",
    "    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.  \n",
    "\n",
    "    In 'outer_product' mode, returns an `(m,n)` matrix with the IoUs for all possible  \n",
    "    combinations of the boxes in `boxes1` and `boxes2`.  \n",
    "\n",
    "    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation  \n",
    "    of the `mode` argument for details.  \n",
    "\n",
    "    Arguments:\n",
    "        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates   \n",
    "            for one box in the format specified by `coords` or a 2D Numpy array of shape `(m, 4)`  \n",
    "            containing the coordinates for `m` boxes.  \n",
    "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.  \n",
    "        \n",
    "        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the  \n",
    "            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates   \n",
    "            for `n` boxes.\n",
    "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.  \n",
    "            \n",
    "        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids'  \n",
    "            for the format `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`,  \n",
    "            or 'corners' for the format `(xmin, ymin, xmax, ymax)`.  \n",
    "            \n",
    "        mode (str, optional): Can be one of 'outer_product' and 'element-wise'.  \n",
    "            In 'outer_product' mode, returns an `(m,n)` matrix with the IoU overlaps  \n",
    "            for all possible combinations of the `m` boxes in `boxes1` with the `n` boxes in `boxes2`.\n",
    "            In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n",
    "            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes,  \n",
    "            then this returns an array of length `m` where the i-th position contains \n",
    "            the IoU overlap of `boxes1[i]` with `boxes2[i]`.  \n",
    "            \n",
    "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
    "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
    "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
    "            to the boxex, but not the other.\n",
    "\n",
    "    Returns:\n",
    "        A 1D or 2D Numpy array (refer to the `mode` argument for details) of  \n",
    "        dtype float containing values in [0,1],the Jaccard similarity of the boxes in  \n",
    "        `boxes1` and `boxes2`. 0 means there is no overlap between two given  \n",
    "        boxes, 1 means their coordinates are identical.\n",
    "    '''\n",
    "    # Convert the coordinates if necessary.\n",
    "    if coords == 'centroids':\n",
    "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2corners')\n",
    "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2corners')\n",
    "        coords = 'corners'\n",
    "\n",
    "    # Compute the IoU.\n",
    "    # Compute the interesection areas.\n",
    "    intersection_areas = intersection_area_(boxes1, boxes2, coords=coords, mode=mode)\n",
    "\n",
    "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
    "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
    "    \n",
    "    # Compute the union areas.\n",
    "\n",
    "    # Set the correct coordinate indices for the respective formats.\n",
    "    if coords == 'corners':\n",
    "        xmin = 0\n",
    "        ymin = 1\n",
    "        xmax = 2\n",
    "        ymax = 3\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "        \n",
    "    if mode == 'outer_product':\n",
    "\n",
    "        boxes1_areas = np.tile(np.expand_dims((boxes1[:,xmax] - boxes1[:,xmin] + d) * (boxes1[:,ymax] - \n",
    "                                               boxes1[:,ymin] + d), axis=1), reps=(1,n))\n",
    "        boxes2_areas = np.tile(np.expand_dims((boxes2[:,xmax] - boxes2[:,xmin] + d) * (boxes2[:,ymax] - \n",
    "                                               boxes2[:,ymin] + d), axis=0), reps=(m,1))\n",
    "        \n",
    "    \n",
    "    union_areas = boxes1_areas + boxes2_areas - intersection_areas\n",
    "\n",
    "    return intersection_areas / union_areas\n",
    "    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def match_bipartite_greedy(weight_matrix):\n",
    "    '''\n",
    "    Returns a bipartite matching according to the given weight matrix.\n",
    "\n",
    "    The algorithm works as follows:\n",
    "\n",
    "    Let the first axis of `weight_matrix` represent ground truth boxes\n",
    "    and the second axis anchor boxes.\n",
    "    The ground truth box that has the greatest similarity with any\n",
    "    anchor box will be matched first, then out of the remaining ground\n",
    "    truth boxes, the ground truth box that has the greatest similarity\n",
    "    with any of the remaining anchor boxes will be matched second, and\n",
    "    so on. That is, the ground truth boxes will be matched in descending\n",
    "    order by maximum similarity with any of the respectively remaining\n",
    "    anchor boxes.\n",
    "    The runtime complexity is O(m^2 * n), where `m` is the number of\n",
    "    ground truth boxes and `n` is the number of anchor boxes.\n",
    "\n",
    "    Arguments:\n",
    "        weight_matrix (array): A 2D Numpy array that represents the weight matrix\n",
    "            for the matching process. If `(m,n)` is the shape of the weight matrix,\n",
    "            it must be `m <= n`. The weights can be integers or floating point\n",
    "            numbers. The matching process will maximize, i.e. larger weights are\n",
    "            preferred over smaller weights.\n",
    "\n",
    "    Returns:\n",
    "        A 1D Numpy array of length `weight_matrix.shape[0]` that represents\n",
    "        the matched index along the second axis of `weight_matrix` for each index\n",
    "        along the first axis.\n",
    "    '''\n",
    "    \n",
    "    weight_matrix = np.copy(weight_matrix) # We'll modify this array.\n",
    "    num_ground_truth_boxes = weight_matrix.shape[0]\n",
    "    all_gt_indices = list(range(num_ground_truth_boxes)) # Only relevant for fancy-indexing below.\n",
    "\n",
    "    # This 1D array will contain for each ground truth box the index of\n",
    "    # the matched anchor box.\n",
    "    matches = np.zeros(num_ground_truth_boxes, dtype=np.int)\n",
    "    \n",
    "    # In each iteration of the loop below, exactly one ground truth box\n",
    "    # will be matched to one anchor box.\n",
    "    for _ in range(num_ground_truth_boxes):\n",
    "\n",
    "        # Find the maximal anchor-ground truth pair in two steps: First, reduce\n",
    "        # over the anchor boxes and then reduce over the ground truth boxes.\n",
    "        anchor_indices = np.argmax(weight_matrix, axis=1) # Reduce along the anchor box axis.\n",
    "        overlaps = weight_matrix[all_gt_indices, anchor_indices]\n",
    "        ground_truth_index = np.argmax(overlaps) # Reduce along the ground truth box axis.\n",
    "        anchor_index = anchor_indices[ground_truth_index]\n",
    "        matches[ground_truth_index] = anchor_index # Set the match.\n",
    "\n",
    "        # Set the row of the matched ground truth box and the column of the matched\n",
    "        # anchor box to all zeros. This ensures that those boxes will not be matched again,\n",
    "        # because they will never be the best matches for any other boxes.\n",
    "        weight_matrix[ground_truth_index] = 0\n",
    "        weight_matrix[:,anchor_index] = 0\n",
    "\n",
    "    return matches\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "'confidence_thresh=0.01'\n",
    "'iou_threshold=0.45'\n",
    "'top_k=200'\n",
    "'nms_max_output_size=400'\n",
    "'coords=centroids'\n",
    "'normalize_coords=True'\n",
    "returns '`[class_id, confidence, xmin, ymin, xmax, ymax]`.'\n",
    "decoded_predictions = DecodeDetections(confidence_thresh=confidence_thresh,\n",
    "                                               iou_threshold=iou_threshold,\n",
    "                                               top_k=top_k,\n",
    "                                               nms_max_output_size=nms_max_output_size,\n",
    "                                               coords=coords,\n",
    "                                               normalize_coords=normalize_coords,\n",
    "                                               img_height=img_height,\n",
    "                                               img_width=img_width,\n",
    "                                               name='decoded_predictions')(predictions)\n",
    "model = Model(inputs=x, outputs=decoded_predictions)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DecodeDetections(Layer):\n",
    "    '''\n",
    "    A Keras layer to decode the raw SSD prediction output.\n",
    "\n",
    "    Input shape:\n",
    "        3D tensor of shape `(batch_size, n_boxes, n_classes + 12)`.\n",
    "\n",
    "    Output shape:\n",
    "        3D tensor of shape `(batch_size, top_k, 6)`.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 confidence_thresh=0.01,\n",
    "                 iou_threshold=0.45,\n",
    "                 top_k=200,\n",
    "                 nms_max_output_size=400,\n",
    "                 coords='centroids',\n",
    "                 normalize_coords=True,\n",
    "                 img_height=None,\n",
    "                 img_width=None,\n",
    "                 **kwargs):\n",
    "'''\n",
    "All default argument values follow the Caffe implementation.\n",
    "\n",
    "Arguments:\n",
    "- confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence\n",
    "                    in a specific positive class in order to be considered for the non-maximum suppression \n",
    "                    stage for the respective class.\n",
    "                    A lower value will result in a larger part of the selection process being done by \n",
    "                    the non-maximum suppression stage, while a larger value will result in a larger part\n",
    "                    of the selection process happening in the confidence thresholding stage.\n",
    "        \n",
    "- iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater\n",
    "                    than `iou_threshold` with a locally maximal box will be removed from\n",
    "                    the set of predictions for a given class, where 'maximal' refers to the box score.\n",
    "                    \n",
    "- top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
    "                    non-maximum suppression stage.\n",
    "                    \n",
    "- nms_max_output_size (int, optional): The maximum number of predictions that will be left \n",
    "                    after performing non-maximum suppression.\n",
    "                    \n",
    "- coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n",
    "                    i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height).\n",
    "                    Other coordinate formats are currently not supported.\n",
    "                    \n",
    "- normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates \n",
    "                    (i.e. coordinates in [0,1])\n",
    "                    and you wish to transform these relative coordinates back to absolute coordinates. \n",
    "                    If the model outputs relative coordinates, but you do not want to convert them back to \n",
    "                    absolute coordinates, set this to `False`.\n",
    "                    Do not set this to `True` if the model already outputs absolute coordinates, \n",
    "                    as that would result in incorrect coordinates. Requires `img_height` and `img_width`\n",
    "                    if set to `True`.\n",
    "- img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
    "- img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
    "'''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*cIE7bbicMOokWQ6w41I-NA.png)\n",
    "\n",
    "```python\n",
    "class DecodeDetections(Layer):\n",
    "    y_pred contains '(batch, feature_map_height * feature_map_width * n_boxes, \n",
    "                    ' n_classes + 4(prediction)+4(anchor)+4(variance))'\n",
    "    def call(self, y_pred, mask=None):\n",
    "    '''\n",
    "    Returns:\n",
    "        3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n",
    "        to always yield `top_k` predictions per batch item. The last axis contains\n",
    "        the coordinates for each predicted box in the format\n",
    "        `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
    "    '''\n",
    "    \n",
    "    #####################################################################################\n",
    "    # 1. Convert the box coordinates from predicted anchor box offsets to predicted\n",
    "    #    absolute coordinates\n",
    "    #####################################################################################\n",
    "\n",
    "    # Convert anchor box offsets to image offsets.\n",
    "    # cx = cx_pred * cx_variance * w_anchor + cx_anchor\n",
    "    cx = y_pred[...,-12] * y_pred[...,-4] * y_pred[...,-6] + y_pred[...,-8] \n",
    "    # cy = cy_pred * cy_variance * h_anchor + cy_anchor\n",
    "    cy = y_pred[...,-11] * y_pred[...,-3] * y_pred[...,-5] + y_pred[...,-7] \n",
    "    # w = exp(w_pred * variance_w) * w_anchor\n",
    "    w = tf.exp(y_pred[...,-10] * y_pred[...,-2]) * y_pred[...,-6]\n",
    "    # h = exp(h_pred * variance_h) * h_anchor\n",
    "    h = tf.exp(y_pred[...,-9] * y_pred[...,-1]) * y_pred[...,-5] \n",
    "    \n",
    "    # Convert 'centroids' to 'corners'.\n",
    "    xmin = cx - 0.5 * w\n",
    "    ymin = cy - 0.5 * h\n",
    "    xmax = cx + 0.5 * w\n",
    "    ymax = cy + 0.5 * h\n",
    "    \n",
    "    # If the model predicts box coordinates relative to the image dimensions and they are supposed\n",
    "    # to be converted back to absolute coordinates, do that.\n",
    "    def normalized_coords():\n",
    "        xmin1 = tf.expand_dims(xmin * self.tf_img_width, axis=-1)\n",
    "        ymin1 = tf.expand_dims(ymin * self.tf_img_height, axis=-1)\n",
    "        xmax1 = tf.expand_dims(xmax * self.tf_img_width, axis=-1)\n",
    "        ymax1 = tf.expand_dims(ymax * self.tf_img_height, axis=-1)\n",
    "        return xmin1, ymin1, xmax1, ymax1\n",
    "    def non_normalized_coords():\n",
    "        return tf.expand_dims(xmin, axis=-1), tf.expand_dims(ymin, axis=-1), tf.expand_dims(xmax, axis=-1),\n",
    "                tf.expand_dims(ymax, axis=-1)\n",
    "        \n",
    "    xmin, ymin, xmax, ymax = tf.cond(self.tf_normalize_coords, normalized_coords, non_normalized_coords)\n",
    "\n",
    "    # Concatenate the one-hot class confidences and \n",
    "    # the converted box coordinates to form the decoded predictions tensor.\n",
    "    y_pred = tf.concat(values=[y_pred[...,:-12], xmin, ymin, xmax, ymax], axis=-1)\n",
    "\n",
    "    #####################################################################################\n",
    "    # 2. Perform confidence thresholding, per-class non-maximum suppression, and\n",
    "    #    top-k filtering.\n",
    "    #####################################################################################\n",
    "\n",
    "    batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
    "    n_boxes = tf.shape(y_pred)[1]\n",
    "    n_classes = y_pred.shape[2] - 4\n",
    "    class_indices = tf.range(1, n_classes)\n",
    "    \n",
    "    # Create a function that filters the predictions for the given batch item. Specifically, it performs:\n",
    "    # - confidence thresholding\n",
    "    # - non-maximum suppression (NMS)\n",
    "    # - top-k filtering\n",
    "    def filter_predictions(batch_item):\n",
    "\n",
    "        # Create a function that filters the predictions for one single class.\n",
    "        def filter_single_class(index):\n",
    "\n",
    "            # From a tensor of shape (n_boxes, n_classes + 4 coordinates) extract\n",
    "            # a tensor of shape (n_boxes, 1 + 4 coordinates) that contains the\n",
    "            # confidnece values for just one class, determined by `index`.\n",
    "            confidences = tf.expand_dims(batch_item[..., index], axis=-1)\n",
    "            class_id = tf.fill(dims=tf.shape(confidences), value=tf.to_float(index))\n",
    "            box_coordinates = batch_item[...,-4:]\n",
    "\n",
    "            single_class = tf.concat([class_id, confidences, box_coordinates], axis=-1)\n",
    "\n",
    "            # Apply confidence thresholding with respect to the class defined by `index`.\n",
    "            threshold_met = single_class[:,1] > self.tf_confidence_thresh\n",
    "            single_class = tf.boolean_mask(tensor=single_class,\n",
    "                                           mask=threshold_met)\n",
    "\n",
    "            # If any boxes made the threshold, perform NMS.\n",
    "            def perform_nms():\n",
    "                scores = single_class[...,1]\n",
    "\n",
    "                # `tf.image.non_max_suppression()` needs the box coordinates\n",
    "                # in the format `(ymin, xmin, ymax, xmax)`.\n",
    "                xmin = tf.expand_dims(single_class[...,-4], axis=-1)\n",
    "                ymin = tf.expand_dims(single_class[...,-3], axis=-1)\n",
    "                xmax = tf.expand_dims(single_class[...,-2], axis=-1)\n",
    "                ymax = tf.expand_dims(single_class[...,-1], axis=-1)\n",
    "                boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\n",
    "\n",
    "                maxima_indices = tf.image.non_max_suppression(boxes=boxes,\n",
    "                                                              scores=scores,\n",
    "                                                              max_output_size=self.tf_nms_max_output_size,\n",
    "                                                              iou_threshold=self.iou_threshold,\n",
    "                                                              name='non_maximum_suppresion')\n",
    "                maxima = tf.gather(params=single_class,\n",
    "                                   indices=maxima_indices,\n",
    "                                   axis=0)\n",
    "                return maxima\n",
    "\n",
    "            def no_confident_predictions():\n",
    "                return tf.constant(value=0.0, shape=(1,6))\n",
    "\n",
    "            single_class_nms = tf.cond(tf.equal(tf.size(single_class), 0),\n",
    "                                       no_confident_predictions, perform_nms)\n",
    "\n",
    "            # Make sure `single_class` is exactly `self.nms_max_output_size` elements long.\n",
    "            padded_single_class = tf.pad(tensor=single_class_nms,\n",
    "                                         paddings=[[0, self.tf_nms_max_output_size - \n",
    "                                                    tf.shape(single_class_nms)[0]], [0, 0]],\n",
    "                                         mode='CONSTANT',\n",
    "                                         constant_values=0.0)\n",
    "\n",
    "            return padded_single_class\n",
    "        '(#n_classes -1, # zeropadded tf_nms_max_output_size,\\\n",
    "         6([class_id, confidence, xmin, ymin, xmax, ymax]) )'\n",
    "        # Iterate `filter_single_class()` over all class indices.\n",
    "        filtered_single_classes = tf.map_fn(fn=lambda i: filter_single_class(i),\n",
    "                                            elems=tf.range(1,n_classes),\n",
    "                                            dtype=tf.float32,\n",
    "                                            parallel_iterations=128,\n",
    "                                            back_prop=False,\n",
    "                                            swap_memory=False,\n",
    "                                            infer_shape=True,\n",
    "                                            name='loop_over_classes')\n",
    "\n",
    "        # Concatenate the filtered results for all individual classes to one tensor.\n",
    "        filtered_predictions = tf.reshape(tensor=filtered_single_classes, shape=(-1,6))\n",
    "\n",
    "        # Perform top-k filtering for this batch item or pad it in case there are\n",
    "        # fewer than `self.top_k` boxes left at this point. Either way, produce a\n",
    "        # tensor of length `self.top_k`. By the time we return the final results tensor\n",
    "        # for the whole batch, all batch items must have the same number of predicted\n",
    "        # boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`\n",
    "        # predictions are left after the filtering process above, we pad the missing\n",
    "        # predictions with zeros as dummy entries.\n",
    "        def top_k():\n",
    "            return tf.gather(params=filtered_predictions,\n",
    "                             indices=tf.nn.top_k(filtered_predictions[:, 1], k=self.tf_top_k,\n",
    "                                                 sorted=True).indices,\n",
    "                             axis=0)\n",
    "        def pad_and_top_k():\n",
    "            padded_predictions = tf.pad(tensor=filtered_predictions,\n",
    "                                        paddings=[[0, self.tf_top_k \n",
    "                                                   - tf.shape(filtered_predictions)[0]], [0, 0]],\n",
    "                                        mode='CONSTANT',\n",
    "                                        constant_values=0.0)\n",
    "            return tf.gather(params=padded_predictions,\n",
    "                   indices=tf.nn.top_k(padded_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
    "                             axis=0)\n",
    "\n",
    "        top_k_boxes = tf.cond(tf.greater_equal(tf.shape(filtered_predictions)[0], self.tf_top_k), top_k, \n",
    "                              pad_and_top_k)\n",
    "\n",
    "        return top_k_boxes\n",
    "    \n",
    "    # Iterate `filter_predictions()` over all batch items.\n",
    "    output_tensor = tf.map_fn(fn=lambda x: filter_predictions(x),\n",
    "                              elems=y_pred,\n",
    "                              dtype=None,\n",
    "                              parallel_iterations=128,\n",
    "                              back_prop=False,\n",
    "                              swap_memory=False,\n",
    "                              infer_shape=True,\n",
    "                              name='loop_over_batch')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.zeros((10,20,25))\n",
    "y_pred[:,:,0] = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "# confidence = y_pred[...,0]\n",
    "confidences = np.expand_dims(y_pred[...,0], axis=-1)\n",
    "print(confidences.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
