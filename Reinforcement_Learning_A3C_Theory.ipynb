{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C \n",
    "- Asynchronous Advantage Actor -Critic\n",
    "- Actor : Policy\n",
    "- Critic : Value\n",
    "- Advantage = G- V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\pi(a|s,\\theta_p) = NeuralNet(input:s,weight:\\theta_p)\\\\\n",
    "V(a|s,\\theta_v) = NeuralNet(input:s,weight:\\theta_v)\n",
    "\\end{equation*}\n",
    "\n",
    "- For policy loss, backwards from policy gradient, For Value loss, use squared error\n",
    "\\begin{equation*}\n",
    "L_p = -(G- V(s))log\\pi(a|s,\\theta_p)\\\\\n",
    "L_v = (G-V(s,\\theta_v))^2\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pseudocode\n",
    "\\begin{equation*}\n",
    "\\theta_p = \\theta_p - learningrate* dL_p/d\\theta_p \\\\\n",
    "\\theta_v = \\theta_v - learningrate* dL_v/d\\theta_v\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N-Step return \n",
    "- instead of using TD(0), we use the N-step return\n",
    "\\begin{equation*}\n",
    "V(s)= r + \\gamma r' + \\gamma^2 r'' + \\gamma^3 V(s''')\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy Regularization\n",
    "- Definition of entropy\n",
    "\\begin{equation*}\n",
    "H = - \\sum_{k=1}^n \\pi_k log\\pi_k\n",
    "\\end{equation*}\n",
    "\n",
    "- New loss (C: regularization constant)\n",
    "\\begin{equation*}\n",
    "L_p'= L_p +CH\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cn.bing.com/th?id=OIP.TlKyrDc2rVlD0tNkkderjAHaGs&pid=Api&rs=1&p=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A3C simply achieves stability using a different method(parallel agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP is a collection of 5 things  \n",
    "\n",
    "- states\n",
    "- actions\n",
    "- rewards\n",
    "- state trasition probabilities\n",
    "- discount factor(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property\n",
    "\n",
    "$p[s(t+1),r(t+1)|s(t),a(t),...,s(1),a(1)] $ = $p[s(t+1),r(t+1)|s(t),a(t)] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount Factor\n",
    "\n",
    "$G(t) = \\sum_{\\tau = 0}^{\\infty}\\gamma^{\\tau}R(t+\\tau+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State value and state action value\n",
    "\n",
    "$V_{\\pi}(s) = E_{\\pi}[G(t)|S_{t}=s]$  \n",
    "$Q_{\\pi}(s,a) = E_{\\pi}[G(t)|S_{t}=s,A_{t}=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi(s) = argmax_{a}{(Q(s,a))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_{\\pi}(s) = \\sum_{a}^{}\\pi(a|s)\\sum_{s'}^{}\\sum_{r}^{}p(s',r|s,a)(r+\\gamma V_{\\pi}(s'))$  \n",
    "\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/TD_MC_DP_backups.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "- prediction problem: given a policy , find the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "```python\n",
    "while not converged:\n",
    "    step1) policy evaluation of current policy\n",
    "    step2) policy improvement(take the argmax Q(s,a))\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "- Q-learning\n",
    "\n",
    "$V_{k+1}(s) = max_{a}\\sum_{s'}^{}\\sum_{r}^{}p(s',r|s,a)(r+\\gamma V_{\\pi}(s'))$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Programming Summary\n",
    "\n",
    "- it is not practical\n",
    "- state space may be very large\n",
    "- doesn't learn from experience\n",
    "- MC and TD learning, no model of the environment needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unlike DP, MC is all about learning from experience\n",
    "\n",
    "$V(s) = E[G(t)|S(t)=s]\\approx \\frac{1}{N}\\sum_{N}^{i=1}G_{i,s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Control\n",
    "\n",
    "1. Initailize random policy\n",
    "2. while not converged:\n",
    "    - a. play an episode,calculate retruns for each state\n",
    "    - b. do policy improvements based on current Q(s,a)take argmax\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC: sample returns based on an episode\n",
    "- TD: estimate returns based on current value function estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0)  \n",
    "\n",
    "$V(S_{t}) = V(S_{t})+\\alpha[r+\\gamma V(S_{t+1})-V(S_{t})]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Control\n",
    "\n",
    "#### SARSA\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r+\\gamma Q(s',a') - Q(s,a)]$  \n",
    "$a' = argmax_{a}[Q(s',a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "- off policy \n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r+\\gamma *max_{a'}Q(s',a') - Q(s,a)]$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
