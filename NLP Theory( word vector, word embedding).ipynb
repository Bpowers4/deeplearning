{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is wrod embedding?  \n",
    "![](https://cn.bing.com/th?id=OIP.sXNXYfAqfLUeiDXPCo130wHaCl&pid=Api&rs=1&p=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Why do we need word embedding?  \n",
    "- Text is represented as a string inside the computer \n",
    "- Deep Learning expects numbers as input\n",
    "- One Solution is one hot encode each word  \n",
    "  Apple = [1,0,0], Banana = [0,1,0] ...\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is feature vector?  \n",
    "\n",
    "|  Name | Meets prerequisites | Independent Learner|\n",
    "|---| ----| ----|\n",
    "| Alice | 10 | 10 |\n",
    "| Bob | 1 | 1 |\n",
    "| Carol | 8 | 2 |\n",
    "| David | 3 | 9 |\n",
    "\n",
    "- each row is a feature vector\n",
    "- we want the feature vector to place things in a meaningful position releative to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "- These feature vectors corresponding to words, we'll call them word vectors  \n",
    "\n",
    "|Name| Gender| Age| Place|\n",
    "|---|---|---|---|\n",
    "|King| +1 | +1| 0|\n",
    "|Queen| -1 | +1| 0|\n",
    "|Prince| +0.9 | -1| 0|\n",
    "|Princess| -0.8 | -0.9| 0.0001|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automation  \n",
    "- we need some automated way to create word vectors\n",
    "- the simplest way is by counting\n",
    "\n",
    "#### In Morden times of automation\n",
    "- we use unsupervised learning, the features don't necessarily have to make sense to humans\n",
    "- we call them latent vectors or hidden vectors\n",
    "- 3 popular methods: Word2Vec, GloVe, FastText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogies\n",
    "- Vec(\"king\") - Vec(\"man\")  = Vec(\"Queen\") - Vec(\"woman\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this have to do with embedding?\n",
    "- A word embedding is simply a matrix of stacked word vectors\n",
    "\n",
    "![](https://cn.bing.com/th?id=OIP.NjSf8RtyVppKoqWemId6bQHaDl&pid=Api&rs=1&p=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventions\n",
    "- V = # of rows  = vocabulary size  = # of distinct words\n",
    "- D = embedding dimension = feature vector size\n",
    "\n",
    "word embedding \n",
    "V x D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Trick\n",
    "- What's an easier way to get a row of matrix?\n",
    "- to get the kth row of a matrix W -> W[k]\n",
    "- In tensorflow: tf.nn.embedding_lookup\n",
    "- In theano: W[k]\n",
    "- In kears: Embedding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings\n",
    "- we will do somthing like transfer learning, where we set the embedding layer with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ![](https://cn.bing.com/th?id=OIP.1BtuTRvy3ken7RQqGb2RIwHaFb&pid=Api&w=1024&h=750&rs=1&p=0)\n",
    "\n",
    "- pretrained embedding[input] -> Embedding Layer W[input] -> Dense Layer:input.dot(W) +b -> Dense Layer: input.dot(W) +b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "- What if your dataset contains words that doesn't exist in the pre-trained embedding?\n",
    "- we can initialize these to random( or 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pre-trained embeddings?\n",
    "- model.fit(X,Y) automatically updates all parameters in the model\n",
    "- it might be necessary to train embeddings that were initialized randomly(not found in pre-trained embedding)\n",
    "- Typically, we don't bother to fine-tune\n",
    "- sometimes, fine-tune led to worse results\n",
    "\n",
    "#### Keras\n",
    "- to fine-tune\n",
    "embedding_layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
