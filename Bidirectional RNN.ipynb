{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN  \n",
    "![](https://cn.bing.com/th?id=OIP.tZtg4-7QAkaUdPyBbNBlXwHaEt&pid=Api&rs=1&p=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if each RNN unit's latent dimension is M, \\begin{equation*}h_t\\end{equation*} should be of size 2M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-One Problem\n",
    "- The backward RNN has only seen one last word!\n",
    "\n",
    "\\begin{equation*}\n",
    "out = [\\vec{h_T}, \\overleftarrow{h_1}] \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "- This is default behavior in Keras if return_sequence= False\n",
    "\n",
    "- Another solution \n",
    "    - take the max over all hidden states\n",
    "\\begin{equation*}\n",
    "out = \\max_{t}  h_t\\\\\n",
    "\\end{equation*}\n",
    "    - What if we took the softmax instead of max? keep this in mind for Attention\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When not to use a Bidirectional RNN\n",
    "- not to use for predicting the future\n",
    "- it doesn't make sense for inputs to come from even further in the future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Bidirectional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM\n",
    "    from keras.layers import CuDNNGRU as GRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 8\n",
    "D = 2\n",
    "M = 3\n",
    "\n",
    "X = np.random.randn(1, T, D)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(T, D))\n",
    "# rnn = Bidirectional(LSTM(M, return_state=True, return_sequences=True))\n",
    "rnn = Bidirectional(LSTM(M, return_state=True, return_sequences=False))\n",
    "x = rnn(input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o: [[-0.17028646 -0.11698733 -0.26957956 -0.14286296  0.13157322 -0.11919126]]\n",
      "o.shape: (1, 6)\n",
      "h1: [[-0.17028646 -0.11698733 -0.26957956]]\n",
      "c1: [[-0.45001054 -0.26141956 -0.66415256]]\n",
      "h2: [[-0.14286296  0.13157322 -0.11919126]]\n",
      "c2: [[-0.30573738  0.30624184 -0.21949151]]\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_, outputs=x)\n",
    "o, h1, c1, h2, c2 = model.predict(X)\n",
    "print(\"o:\", o)\n",
    "print(\"o.shape:\", o.shape)\n",
    "print(\"h1:\", h1)\n",
    "print(\"c1:\", c1)\n",
    "print(\"h2:\", h2)\n",
    "print(\"c2:\", c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toxic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import keras.backend as K\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM\n",
    "    from keras.layers import CuDNNGRU as GRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "\n",
    "# create an LSTM network with a single LSTM\n",
    "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_)\n",
    "# x = LSTM(15, return_sequences=True)(x)\n",
    "x = Bidirectional(LSTM(15, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "output = Dense(len(possible_labels), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(input_, output)\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data,\n",
    "  targets,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification with Bidirectional RNNs\n",
    "- RNNs can be used for images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An image as a sequence of pixels\n",
    "\n",
    "#### Architecture\n",
    "- Let's pretend the image is a sequence of word vectors( T x D matrix)\n",
    "- Pretend height = T , width = D\n",
    "- Rotate the image and run a Bidirectional RNN on both, we go in all 4 directions\n",
    "\n",
    "[tensorflow example](http://easy-tensorflow.com/tf-tutorials/recurrent-neural-networks/bidirectional-rnn-for-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- given LSTM latent dimensionality  = M, what is the output size?\n",
    "- input (N x H x W) -> bi-LSTM(N x H x 2M) -> maxpool -> N x 2M\n",
    "- rotate (N x H x W) -> bi-LSTM(N x H x 2M) -> maxpool -> N x 2M\n",
    "- concat output ( N x 4M)\n",
    "- Dense + softmax to get prediction ( N x K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get data\n",
    "X, Y = get_mnist()\n",
    "\n",
    "# config\n",
    "D = 28\n",
    "M = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input is an image of size 28x28\n",
    "input_ = Input(shape=(D, D))\n",
    "\n",
    "# up-down\n",
    "rnn1 = Bidirectional(LSTM(M, return_sequences=True))\n",
    "x1 = rnn1(input_) # output is N x D x 2M\n",
    "x1 = GlobalMaxPooling1D()(x1) # output is N x 2M\n",
    "\n",
    "# left-right\n",
    "rnn2 = Bidirectional(LSTM(M, return_sequences=True))\n",
    "\n",
    "# custom layer\n",
    "permutor = Lambda(lambda t: K.permute_dimensions(t, pattern=(0, 2, 1)))\n",
    "\n",
    "x2 = permutor(input_)\n",
    "x2 = rnn2(x2) # output is N x D x 2M\n",
    "x2 = GlobalMaxPooling1D()(x2) # output is N x 2M\n",
    "\n",
    "# put them together\n",
    "concatenator = Concatenate(axis=1)\n",
    "x = concatenator([x1, x2]) # output is N x 4M\n",
    "\n",
    "# final dense layer\n",
    "output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=output)\n",
    "\n",
    "# testing\n",
    "# o = model.predict(X)\n",
    "# print(\"o.shape:\", o.shape)\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train\n",
    "print('Training model...')\n",
    "r = model.fit(X, Y, batch_size=32, epochs=10, validation_split=0.3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
