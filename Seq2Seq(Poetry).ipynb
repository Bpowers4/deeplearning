{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence \n",
    "- the translation will contain a different number of words than the input\n",
    "![](https://isaacchanghau.github.io/img/nlp/seq2seq-neuralconver/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- No outputs because not making predictions\n",
    "- Only keep final state \\begin{equation*}h_t \\end{equation*}\n",
    "- return_sequence = False in Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- New RNN unit with its own weights\n",
    "- Must have the same vector size(M) \n",
    "- Pass start of sentence token into the X input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "y_1\\ becames\\ x_2 \\\\\n",
    "y_2\\ becames\\ x_3 \\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dense layer comes after the LSTM with output size V\n",
    "- Language modelling \n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_t | w_{t-1},w_{t-2},...)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Solve the problem of mapping \n",
    "\\begin{equation*}\n",
    "an\\ input\\ of\\ length\\ T_x \\ != \\ output\\ length\\ T_y\\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What tasks look like machine translation?\n",
    "- Story and Question concatenated to form input sequence\n",
    "- Input sequence is transformed by encoder into a thought vector\n",
    "- Thought vector is decoded to form an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbots\n",
    "- I personally don't think Seq2Seq is well suited to Chatbots\n",
    "- Real conversations involve an idea spanning multiple back and forth statements by the agents involved can even return to a previous topic much later\n",
    "- conversation with only binary question-answer pairs is awkward\n",
    "- Seq2Seq only learns to memorize the request-response pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Forcing\n",
    "- Pass in the true target sequence into the bottom of the decoder RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "- Keras must have constant-sized inputs\n",
    "- Decoder input length during training is Ty\n",
    "- Decoder input length during prediction is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras impl\n",
    "- create 2 different models\n",
    "\n",
    "emb = Embedding()  \n",
    "lstm = LSTM()  \n",
    "dense = Dense()  \n",
    "  \n",
    "input1 = Input(lenght = Ty)  \n",
    "model1  = Model(input1, dense(lstm(emb(input1))))  \n",
    "  \n",
    "input2 = Input(lenght = 1)  \n",
    "model2  = Model(input1, dense(lstm(emb(input2))))  \n",
    "  \n",
    "h = encoder model output x = SOS  \n",
    "for t in range(Ty)  \n",
    "    x,h = model2.predict(x,h)  \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code(Poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "import keras.backend as K\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM\n",
    "    from keras.layers import CuDNNGRU as GRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2000\n",
    "LATENT_DIM = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "# load in the data\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "for line in open('./large_files/robert_frost.txt'):\n",
    "    line = line.rstrip()\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    input_line = '<sos> ' + line\n",
    "    target_line = line + ' <eos>'\n",
    "\n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)\n",
    "    \n",
    "    if count > 10:\n",
    "        break\n",
    "    count += 1\n",
    "    \n",
    "all_lines = input_texts + target_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> Two roads diverged in a yellow wood,',\n",
       " '<sos> And sorry I could not travel both',\n",
       " '<sos> And be one traveler, long I stood',\n",
       " '<sos> And looked down one as far as I could',\n",
       " '<sos> To where it bent in the undergrowth;',\n",
       " '<sos> Then took the other, as just as fair,',\n",
       " '<sos> And having perhaps the better claim',\n",
       " '<sos> Because it was grassy and wanted wear,',\n",
       " '<sos> Though as for that the passing there',\n",
       " '<sos> Had worn them really about the same,',\n",
       " '<sos> And both that morning equally lay',\n",
       " '<sos> In leaves no step had trodden black.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sentences (strings) into integers\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, [1, 14, 15, 16, 6, 17, 18, 19])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences),input_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 10\n",
      "Found 64 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# find max seq length\n",
    "max_sequence_length_from_data = max(len(s) for s in input_sequences)\n",
    "print('Max sequence length:', max_sequence_length_from_data)\n",
    "\n",
    "\n",
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<eos>': 3,\n",
       " '<sos>': 1,\n",
       " 'a': 17,\n",
       " 'about': 55,\n",
       " 'and': 2,\n",
       " 'as': 4,\n",
       " 'be': 23,\n",
       " 'because': 43,\n",
       " 'bent': 32,\n",
       " 'better': 41,\n",
       " 'black.': 64,\n",
       " 'both': 9,\n",
       " 'claim': 42,\n",
       " 'could': 8,\n",
       " 'diverged': 16,\n",
       " 'down': 28,\n",
       " 'equally': 58,\n",
       " 'fair,': 38,\n",
       " 'far': 29,\n",
       " 'for': 49,\n",
       " 'grassy': 45,\n",
       " 'had': 13,\n",
       " 'having': 39,\n",
       " 'i': 7,\n",
       " 'in': 6,\n",
       " 'it': 11,\n",
       " 'just': 37,\n",
       " 'lay': 59,\n",
       " 'leaves': 60,\n",
       " 'long': 25,\n",
       " 'looked': 27,\n",
       " 'morning': 57,\n",
       " 'no': 61,\n",
       " 'not': 21,\n",
       " 'one': 10,\n",
       " 'other,': 36,\n",
       " 'passing': 50,\n",
       " 'perhaps': 40,\n",
       " 'really': 54,\n",
       " 'roads': 15,\n",
       " 'same,': 56,\n",
       " 'sorry': 20,\n",
       " 'step': 62,\n",
       " 'stood': 26,\n",
       " 'that': 12,\n",
       " 'the': 5,\n",
       " 'them': 53,\n",
       " 'then': 34,\n",
       " 'there': 51,\n",
       " 'though': 48,\n",
       " 'to': 30,\n",
       " 'took': 35,\n",
       " 'travel': 22,\n",
       " 'traveler,': 24,\n",
       " 'trodden': 63,\n",
       " 'two': 14,\n",
       " 'undergrowth;': 33,\n",
       " 'wanted': 46,\n",
       " 'was': 44,\n",
       " 'wear,': 47,\n",
       " 'where': 31,\n",
       " 'wood,': 19,\n",
       " 'worn': 52,\n",
       " 'yellow': 18}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (12, 10)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "max_sequence_length = min(max_sequence_length_from_data, MAX_SEQUENCE_LENGTH)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print('Shape of data tensor:', input_sequences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 12 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "count = 0\n",
    "with open(os.path.join('./large_files/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        \n",
    "        if count > 10:\n",
    "            break\n",
    "        count += 1\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_targets (12, 10, 65)\n"
     ]
    }
   ],
   "source": [
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "# one-hot the targets (can't use sparse cross-entropy)\n",
    "one_hot_targets = np.zeros((len(input_sequences), max_sequence_length, num_words))\n",
    "print(\"one_hot_targets {}\".format(one_hot_targets.shape))\n",
    "for i, target_sequence in enumerate(target_sequences):\n",
    "    for t, word in enumerate(target_sequence):\n",
    "        if word > 0:\n",
    "            one_hot_targets[i, t, word] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8322e6745e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'target_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "target_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  # trainable=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Tensor(\"input_8:0\", shape=(?, 10), dtype=float32) Tensor(\"input_9:0\", shape=(?, 25), dtype=float32) Tensor(\"input_10:0\", shape=(?, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Building model...')\n",
    "\n",
    "# create an LSTM network with a single LSTM\n",
    "input_ = Input(shape=(max_sequence_length,))\n",
    "\n",
    "initial_h = Input(shape=(LATENT_DIM,))\n",
    "initial_c = Input(shape=(LATENT_DIM,))\n",
    "\n",
    "print(input_,initial_h,initial_c)\n",
    "x = embedding_layer(input_)\n",
    "lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "x, _, _ = lstm(x, initial_state=[initial_h, initial_c]) # don't need the states here\n",
    "dense = Dense(num_words, activation='softmax')\n",
    "output = dense(x)\n",
    "\n",
    "model = Model([input_, initial_h, initial_c], output)\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  # optimizer='rmsprop',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  # optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, <tf.Tensor 'dense_1/truediv:0' shape=(?, 10, 65) dtype=float32>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "z (12, 25)\n"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "z = np.zeros((len(input_sequences), LATENT_DIM))\n",
    "print(\"z {}\".format(z.shape))\n",
    "r = model.fit(\n",
    "  [input_sequences, z, z],\n",
    "  one_hot_targets,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sampling model\n",
    "input2 = Input(shape=(1,)) # we'll only input one word at a time\n",
    "x = embedding_layer(input2)\n",
    "x, h, c = lstm(x, initial_state=[initial_h, initial_c]) # now we need states to feed back in\n",
    "output2 = dense(x)\n",
    "sampling_model = Model([input2, initial_h, initial_c], [output2, h, c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'lstm_1_2/transpose_1:0' shape=(?, ?, 25) dtype=float32>,\n",
       " 65,\n",
       " 50,\n",
       " <tf.Tensor 'dense_1_2/truediv:0' shape=(?, 1, 65) dtype=float32>,\n",
       " <tf.Tensor 'strided_slice:0' shape=(65,) dtype=float32>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,num_words, EMBEDDING_DIM,output2,output2[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse word2idx dictionary to get back words\n",
    "# during prediction\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "\n",
    "\n",
    "def sample_line():\n",
    "    # initial inputs\n",
    "    np_input = np.array([[ word2idx['<sos>'] ]])\n",
    "    h = np.zeros((1, LATENT_DIM))\n",
    "    c = np.zeros((1, LATENT_DIM))\n",
    "\n",
    "    # so we know when to quit\n",
    "    eos = word2idx['<eos>']\n",
    "\n",
    "    # store the output here\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_sequence_length):\n",
    "        o, h, c = sampling_model.predict([np_input, h, c])\n",
    "\n",
    "        # print(\"o.shape:\", o.shape, o[0,0,:10])\n",
    "        # idx = np.argmax(o[0,0])\n",
    "        probs = o[0,0]\n",
    "        if np.argmax(probs) == 0:\n",
    "            print(\"wtf\")\n",
    "        probs[0] = 0\n",
    "        probs /= probs.sum()\n",
    "        idx = np.random.choice(len(probs), p=probs)\n",
    "        if idx == eos:\n",
    "            break\n",
    "\n",
    "        # accuulate output\n",
    "        output_sentence.append(idx2word.get(idx, '<WTF %s>' % idx))\n",
    "\n",
    "        # make the next input into model\n",
    "        np_input[0,0] = idx\n",
    "\n",
    "    return ' '.join(output_sentence)\n",
    "\n",
    "# generate a 4 line poem\n",
    "while True:\n",
    "    for _ in range(4):\n",
    "        print(sample_line())\n",
    "\n",
    "    ans = input(\"---generate another? [Y/n]---\")\n",
    "    if ans and ans[0].lower().startswith('n'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1), array([[1]]), 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_input = np.array([[ word2idx['<sos>'] ]])\n",
    "np_input.shape,np_input,np_input[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
