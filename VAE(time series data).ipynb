{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./src/\") \n",
    "\n",
    "import mnist_data\n",
    "import plot_utils\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filelists):\n",
    "\n",
    "    try:\n",
    "        savedir = './data'\n",
    "\n",
    "        mergeddata = []\n",
    "        for file in filelists:\n",
    "            filename = file\n",
    "            stocksavedir = savedir+'/'+filename\n",
    "\n",
    "            np_data = []\n",
    "            for _inputname in ['np_train','np_test']:\n",
    "                with gzip.open( stocksavedir+'/'+_inputname+'.npy.gz', 'r') as infile:\n",
    "                    np_data.append(np.load(infile))\n",
    "\n",
    "            df_data = []\n",
    "            for _inputname in ['df_train','df_test']:\n",
    "                df_data.append(pd.read_pickle(stocksavedir+'/'+_inputname+'.pkl'))\n",
    "            \n",
    "            mergeddata.append([np_data[0],np_data[1],df_data[0],df_data[1]])\n",
    "\n",
    "        return mergeddata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2079, 4323)\n",
      "(891, 4323)\n",
      "(2079, 30)\n"
     ]
    }
   ],
   "source": [
    "filelists = ['DJI']\n",
    "mergeddata = readData(filelists)\n",
    "np_train = mergeddata[0][0]\n",
    "print(np_train.shape)\n",
    "np_test = mergeddata[0][1]\n",
    "print(np_test.shape)\n",
    "df_train = mergeddata[0][2]\n",
    "# print(df_train.head())\n",
    "print(df_train.shape)\n",
    "# df_train['signal_5ma'][:10]\n",
    "df_train['id'] = 0\n",
    "df_train.loc[(df_train['signal_5ma'] == 9) , ['id']] = 1\n",
    "df_train.loc[(df_train['signal_5ma'] == 8) , ['id']] = 2\n",
    "df_train.loc[(df_train['signal_5ma'] == -9) , ['id']] = 3\n",
    "df_train.loc[(df_train['signal_5ma'] == 0) , ['id']] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae:\n",
    "    def __init__(self,session,learning_rate,dense_layers,dense_funcs,dim_z,dim_out):\n",
    "\n",
    "        self.session = session\n",
    "        self.dim_in = dim_out\n",
    "        self.dim_out = dim_out\n",
    "        self.dim_z = dim_z\n",
    "        \n",
    "        self.lrate = learning_rate\n",
    "        self.dense_layers = dense_layers\n",
    "        self.dense_funcs = dense_funcs\n",
    "        \n",
    "        self.model_name = 'vae'\n",
    "        self.logs_dir = './model'\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self):\n",
    "        \n",
    "        self.x_hat = tf.placeholder(tf.float32, shape=[None, self.dim_in], name='input_denoising')\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.dim_out], name='target')\n",
    "\n",
    "        # dropout\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        # input for PMLR\n",
    "        self.z_in = tf.placeholder(tf.float32, shape=[None, dim_z], name='latent_variable')\n",
    "        \n",
    "        self.encoder_dense_layers = self.dense_layers+[self.dim_z]\n",
    "        self.encoder_dense_funcs = self.dense_funcs\n",
    "        print(\"encoder_dense_layers {}\".format(self.encoder_dense_layers))\n",
    "        \n",
    "        self.mu,self.sigma = self.encoder(self.x_hat,self.encoder_dense_layers,self.encoder_dense_funcs,self.keep_prob)\n",
    "        \n",
    "        print(\"mu:{} sigma:{}\".format(self.mu.shape,self.sigma.shape))\n",
    "        # sampling by re-parameterization technique\n",
    "        self.z = self.mu + self.sigma * tf.random_normal(tf.shape(self.mu), 0, 1, dtype=tf.float32)\n",
    "        print(\"encoder output z:{}\".format(self.z.shape))\n",
    "\n",
    "        self.decoder_dense_layers = self.dense_layers[::-1]+[self.dim_out]\n",
    "        self.decoder_activ = self.encoder_dense_funcs[::-1]\n",
    "        print(\"decoder_dense_layers {}\".format(self.decoder_dense_layers))\n",
    "        \n",
    "        # decoding\n",
    "        self.y = self.decoder(self.z, self.decoder_dense_layers,self.decoder_activ, self.keep_prob)\n",
    "        self.y = tf.clip_by_value(self.y, 1e-8, 1 - 1e-8)\n",
    "        print(\"decoder output y:{}\".format(self.y.shape))\n",
    "        # loss\n",
    "        self.marginal_likelihood = tf.reduce_sum(self.x * tf.log(self.y) + (1 - self.x) * tf.log(1 - self.y), 1)\n",
    "        self.KL_divergence = 0.5 * tf.reduce_sum(tf.square(self.mu) + tf.square(self.sigma) - tf.log(1e-8 + tf.square(self.sigma)) - 1, 1)\n",
    "\n",
    "        self.marginal_likelihood = tf.reduce_mean(self.marginal_likelihood)\n",
    "        self.KL_divergence = tf.reduce_mean(self.KL_divergence)\n",
    "\n",
    "        self.ELBO = self.marginal_likelihood - self.KL_divergence\n",
    "\n",
    "        self.loss = -self.ELBO\n",
    "        \n",
    "        self.decoded = self.sample_decoder(self.z_in, self.dim_out)\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer(self.lrate).minimize(self.loss)\n",
    "        self.neg_marginal_likelihood = -1*self.marginal_likelihood\n",
    "        \n",
    "        self.global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "        \n",
    "        \n",
    "    def save(self, step):\n",
    "        model_name = self.model_name + \".model\"\n",
    "                \n",
    "        model_logs_dir = self.logs_dir\n",
    "        if not os.path.exists(model_logs_dir):\n",
    "            os.makedirs(model_logs_dir)\n",
    "            \n",
    "        self.saver.save(\n",
    "            self.session,\n",
    "            os.path.join(model_logs_dir, model_name),\n",
    "            global_step=step\n",
    "        )\n",
    "    def load(self):\n",
    "    \n",
    "        ckpt = tf.train.get_checkpoint_state(self.logs_dir)\n",
    "        \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            # print('ckpt_name {} self.logs_dir {}'.format(ckpt_name,self.logs_dir))\n",
    "            self.saver.restore(self.session, os.path.join(self.logs_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "\n",
    "        else:\n",
    "            print(\" [*] Failed to find a \" +self.logs_dir+ \" checkpoint\")\n",
    "            return False, 0\n",
    "    \n",
    "    def sample_decoder(self,z, dim_img):\n",
    "\n",
    "        # decoding\n",
    "        y = self.decoder(z, self.decoder_dense_layers,self.decoder_activ, 1.0, reuse=True)\n",
    "        return y\n",
    "    \n",
    "    def encoder(self,x, dense_layers,dense_func,keep_prob):\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "\n",
    "            # initializers\n",
    "            w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            b_init = tf.constant_initializer(0.)\n",
    "\n",
    "            # dense layers\n",
    "            dense_input_shape = x.get_shape()[1]\n",
    "            dense_input = x\n",
    "            for _i,_n_hidden in enumerate(dense_layers[:-1]):\n",
    "                net_w = tf.get_variable('w'+str(_i),[dense_input_shape,_n_hidden],initializer=w_init)\n",
    "                net_b = tf.get_variable('b'+str(_i),[_n_hidden],initializer = b_init)\n",
    "                net_h = tf.matmul(dense_input,net_w) + net_b \n",
    "                if dense_func[_i] == 'elu':\n",
    "                    actfunc = tf.nn.elu\n",
    "                elif dense_func[_i] == 'tanh':\n",
    "                    actfunc = tf.nn.tanh\n",
    "                net_h = actfunc(net_h)\n",
    "                net_h = tf.nn.dropout(net_h,keep_prob)\n",
    "                dense_input_shape = net_h.get_shape()[1]\n",
    "                dense_input = net_h\n",
    "\n",
    "            # final output\n",
    "            final_w = tf.get_variable(\"w_final\",[dense_input.get_shape()[1],dense_layers[-1]*2],initializer = w_init)\n",
    "            final_b = tf.get_variable(\"b_final\",[dense_layers[-1]*2],initializer=b_init)\n",
    "            gaussian_params = tf.matmul(dense_input,final_w) + final_b\n",
    "\n",
    "            mean = gaussian_params[:,:dense_layers[-1]]\n",
    "            stddev = 1e-6 +tf.nn.softplus(gaussian_params[:,dense_layers[-1]:])\n",
    "            print(\"gaussian encoder mean:{} \".format(mean.shape))\n",
    "\n",
    "\n",
    "        return mean, stddev\n",
    "    \n",
    "    # Bernoulli as decoder\n",
    "    # bernoulli_decoder(z, [500,500,784],['tanh','elu'], keep_prob, reuse=False)\n",
    "    def decoder(self,z, dense_layers,dense_func, keep_prob, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(\"decoder\", reuse=reuse):\n",
    "            # initializers\n",
    "            w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            b_init = tf.constant_initializer(0.)\n",
    "\n",
    "            # dense layers\n",
    "            dense_input_shape = z.get_shape()[1]\n",
    "            dense_input = z\n",
    "            for _i,_n_hidden in enumerate(dense_layers[:-1]):\n",
    "                net_w = tf.get_variable('w'+str(_i),[dense_input_shape,_n_hidden],initializer=w_init)\n",
    "                net_b = tf.get_variable('b'+str(_i),[_n_hidden],initializer = b_init)\n",
    "                net_h = tf.matmul(dense_input,net_w) + net_b \n",
    "                if dense_func[_i] == 'elu':\n",
    "                    actfunc = tf.nn.elu\n",
    "                elif dense_func[_i] == 'tanh':\n",
    "                    actfunc = tf.nn.tanh\n",
    "                net_h = actfunc(net_h)\n",
    "                net_h = tf.nn.dropout(net_h,keep_prob)\n",
    "                dense_input_shape = net_h.get_shape()[1]\n",
    "                dense_input = net_h\n",
    "\n",
    "            # final output\n",
    "            final_w = tf.get_variable(\"w_final\",[dense_input.get_shape()[1],dense_layers[-1]],initializer = w_init)\n",
    "            final_b = tf.get_variable(\"b_final\",[dense_layers[-1]],initializer=b_init)\n",
    "            y = tf.sigmoid(tf.matmul(dense_input,final_w)+final_b)\n",
    "\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_dense_layers [1500, 700, 300, 2]\n",
      "gaussian encoder mean:(?, 2) \n",
      "mu:(?, 2) sigma:(?, 2)\n",
      "encoder output z:(?, 2)\n",
      "decoder_dense_layers [300, 700, 1500, 4323]\n",
      "decoder output y:(?, 4323)\n",
      "epoch 0: L_tot 2891.58 L_likelihood 2884.16 L_divergence 7.42\n",
      "epoch 1: L_tot 2863.19 L_likelihood 2857.99 L_divergence 5.19\n",
      "epoch 2: L_tot 2863.99 L_likelihood 2860.22 L_divergence 3.77\n",
      "epoch 3: L_tot 2853.23 L_likelihood 2849.44 L_divergence 3.79\n",
      "epoch 4: L_tot 2830.20 L_likelihood 2826.40 L_divergence 3.80\n",
      "epoch 5: L_tot 2830.28 L_likelihood 2826.33 L_divergence 3.95\n",
      "epoch 6: L_tot 2851.33 L_likelihood 2847.38 L_divergence 3.95\n",
      "epoch 7: L_tot 2809.61 L_likelihood 2805.52 L_divergence 4.10\n",
      "epoch 8: L_tot 2791.03 L_likelihood 2787.49 L_divergence 3.54\n",
      "epoch 9: L_tot 2796.70 L_likelihood 2793.30 L_divergence 3.39\n",
      "epoch 10: L_tot 2818.64 L_likelihood 2815.01 L_divergence 3.63\n",
      "epoch 11: L_tot 2799.84 L_likelihood 2796.44 L_divergence 3.40\n",
      "epoch 12: L_tot 2814.86 L_likelihood 2811.35 L_divergence 3.51\n",
      "epoch 13: L_tot 2784.50 L_likelihood 2781.33 L_divergence 3.17\n",
      "epoch 14: L_tot 2806.18 L_likelihood 2803.22 L_divergence 2.96\n",
      "epoch 15: L_tot 2796.83 L_likelihood 2793.36 L_divergence 3.47\n",
      "epoch 16: L_tot 2806.20 L_likelihood 2803.25 L_divergence 2.95\n",
      "epoch 17: L_tot 2799.16 L_likelihood 2795.98 L_divergence 3.18\n",
      "epoch 18: L_tot 2781.03 L_likelihood 2778.34 L_divergence 2.68\n",
      "epoch 19: L_tot 2803.19 L_likelihood 2800.16 L_divergence 3.03\n",
      "epoch 20: L_tot 2790.26 L_likelihood 2787.00 L_divergence 3.26\n",
      "epoch 21: L_tot 2821.55 L_likelihood 2818.64 L_divergence 2.91\n",
      "epoch 22: L_tot 2776.88 L_likelihood 2773.61 L_divergence 3.26\n",
      "epoch 23: L_tot 2820.51 L_likelihood 2817.48 L_divergence 3.03\n",
      "epoch 24: L_tot 2774.83 L_likelihood 2771.97 L_divergence 2.86\n",
      "epoch 25: L_tot 2799.66 L_likelihood 2797.00 L_divergence 2.67\n",
      "epoch 26: L_tot 2792.90 L_likelihood 2789.56 L_divergence 3.34\n",
      "epoch 27: L_tot 2792.22 L_likelihood 2789.26 L_divergence 2.95\n",
      "epoch 28: L_tot 2790.93 L_likelihood 2788.50 L_divergence 2.42\n",
      "epoch 29: L_tot 2792.34 L_likelihood 2789.48 L_divergence 2.85\n",
      "epoch 30: L_tot 2788.35 L_likelihood 2785.37 L_divergence 2.98\n",
      "epoch 31: L_tot 2799.77 L_likelihood 2796.89 L_divergence 2.88\n",
      "epoch 32: L_tot 2760.67 L_likelihood 2757.85 L_divergence 2.82\n",
      "epoch 33: L_tot 2789.83 L_likelihood 2786.93 L_divergence 2.90\n",
      "epoch 34: L_tot 2788.53 L_likelihood 2785.31 L_divergence 3.22\n",
      "epoch 35: L_tot 2812.92 L_likelihood 2810.16 L_divergence 2.76\n",
      "epoch 36: L_tot 2783.13 L_likelihood 2780.80 L_divergence 2.33\n",
      "epoch 37: L_tot 2817.68 L_likelihood 2814.84 L_divergence 2.84\n",
      "epoch 38: L_tot 2796.57 L_likelihood 2794.09 L_divergence 2.48\n",
      "epoch 39: L_tot 2797.59 L_likelihood 2795.07 L_divergence 2.52\n",
      "epoch 40: L_tot 2811.82 L_likelihood 2809.41 L_divergence 2.41\n",
      "epoch 41: L_tot 2836.01 L_likelihood 2833.06 L_divergence 2.95\n",
      "epoch 42: L_tot 2801.51 L_likelihood 2798.94 L_divergence 2.56\n",
      "epoch 43: L_tot 2817.19 L_likelihood 2814.67 L_divergence 2.52\n",
      "epoch 44: L_tot 2793.31 L_likelihood 2790.59 L_divergence 2.72\n",
      "epoch 45: L_tot 2797.57 L_likelihood 2794.07 L_divergence 3.50\n",
      "epoch 46: L_tot 2789.80 L_likelihood 2787.23 L_divergence 2.56\n",
      "epoch 47: L_tot 2782.16 L_likelihood 2779.11 L_divergence 3.06\n",
      "epoch 48: L_tot 2781.17 L_likelihood 2778.41 L_divergence 2.76\n",
      "epoch 49: L_tot 2793.75 L_likelihood 2791.18 L_divergence 2.56\n",
      "epoch 50: L_tot 2795.22 L_likelihood 2792.93 L_divergence 2.29\n",
      "epoch 51: L_tot 2815.35 L_likelihood 2812.64 L_divergence 2.71\n",
      "epoch 52: L_tot 2761.44 L_likelihood 2758.23 L_divergence 3.21\n",
      "epoch 53: L_tot 2785.27 L_likelihood 2782.37 L_divergence 2.89\n",
      "epoch 54: L_tot 2816.83 L_likelihood 2813.92 L_divergence 2.91\n",
      "epoch 55: L_tot 2791.15 L_likelihood 2788.89 L_divergence 2.26\n",
      "epoch 56: L_tot 2751.61 L_likelihood 2748.50 L_divergence 3.12\n",
      "epoch 57: L_tot 2798.19 L_likelihood 2795.65 L_divergence 2.54\n",
      "epoch 58: L_tot 2798.76 L_likelihood 2796.28 L_divergence 2.47\n",
      "epoch 59: L_tot 2823.84 L_likelihood 2821.43 L_divergence 2.41\n",
      "epoch 60: L_tot 2775.38 L_likelihood 2772.90 L_divergence 2.49\n",
      "epoch 61: L_tot 2783.87 L_likelihood 2781.57 L_divergence 2.29\n",
      "epoch 62: L_tot 2792.00 L_likelihood 2789.56 L_divergence 2.44\n",
      "epoch 63: L_tot 2791.02 L_likelihood 2788.36 L_divergence 2.66\n",
      "epoch 64: L_tot 2793.66 L_likelihood 2791.07 L_divergence 2.59\n",
      "epoch 65: L_tot 2792.32 L_likelihood 2789.34 L_divergence 2.98\n",
      "epoch 66: L_tot 2791.46 L_likelihood 2788.40 L_divergence 3.06\n",
      "epoch 67: L_tot 2798.10 L_likelihood 2795.29 L_divergence 2.81\n",
      "epoch 68: L_tot 2785.86 L_likelihood 2782.99 L_divergence 2.86\n",
      "epoch 69: L_tot 2804.93 L_likelihood 2801.25 L_divergence 3.69\n",
      "epoch 70: L_tot 2780.34 L_likelihood 2778.01 L_divergence 2.32\n",
      "epoch 71: L_tot 2788.63 L_likelihood 2785.88 L_divergence 2.75\n",
      "epoch 72: L_tot 2809.47 L_likelihood 2806.68 L_divergence 2.79\n",
      "epoch 73: L_tot 2789.72 L_likelihood 2786.23 L_divergence 3.49\n",
      "epoch 74: L_tot 2786.33 L_likelihood 2783.67 L_divergence 2.66\n",
      "epoch 75: L_tot 2785.94 L_likelihood 2783.41 L_divergence 2.53\n",
      "epoch 76: L_tot 2801.33 L_likelihood 2798.88 L_divergence 2.45\n",
      "epoch 77: L_tot 2800.88 L_likelihood 2798.27 L_divergence 2.61\n",
      "epoch 78: L_tot 2779.07 L_likelihood 2776.03 L_divergence 3.04\n",
      "epoch 79: L_tot 2794.83 L_likelihood 2791.54 L_divergence 3.29\n",
      "epoch 80: L_tot 2785.48 L_likelihood 2782.79 L_divergence 2.69\n",
      "epoch 81: L_tot 2773.25 L_likelihood 2770.97 L_divergence 2.28\n",
      "epoch 82: L_tot 2777.20 L_likelihood 2774.31 L_divergence 2.88\n",
      "epoch 83: L_tot 2804.93 L_likelihood 2802.15 L_divergence 2.78\n",
      "epoch 84: L_tot 2784.94 L_likelihood 2782.32 L_divergence 2.61\n",
      "epoch 85: L_tot 2809.12 L_likelihood 2806.46 L_divergence 2.66\n",
      "epoch 86: L_tot 2771.85 L_likelihood 2769.03 L_divergence 2.82\n",
      "epoch 87: L_tot 2763.46 L_likelihood 2760.47 L_divergence 2.99\n",
      "epoch 88: L_tot 2787.02 L_likelihood 2784.11 L_divergence 2.91\n",
      "epoch 89: L_tot 2775.11 L_likelihood 2772.30 L_divergence 2.81\n",
      "epoch 90: L_tot 2790.56 L_likelihood 2787.98 L_divergence 2.58\n",
      "epoch 91: L_tot 2802.65 L_likelihood 2800.47 L_divergence 2.19\n",
      "epoch 92: L_tot 2805.09 L_likelihood 2802.30 L_divergence 2.79\n",
      "epoch 93: L_tot 2773.26 L_likelihood 2770.82 L_divergence 2.44\n",
      "epoch 94: L_tot 2794.59 L_likelihood 2791.56 L_divergence 3.03\n",
      "epoch 95: L_tot 2812.49 L_likelihood 2809.77 L_divergence 2.71\n",
      "epoch 96: L_tot 2781.39 L_likelihood 2778.42 L_divergence 2.97\n",
      "epoch 97: L_tot 2790.10 L_likelihood 2787.41 L_divergence 2.69\n",
      "epoch 98: L_tot 2795.24 L_likelihood 2792.74 L_divergence 2.50\n",
      "epoch 99: L_tot 2778.06 L_likelihood 2775.25 L_divergence 2.80\n",
      "epoch 100: L_tot 2814.96 L_likelihood 2812.71 L_divergence 2.24\n",
      "epoch 101: L_tot 2785.92 L_likelihood 2783.43 L_divergence 2.50\n",
      "epoch 102: L_tot 2807.39 L_likelihood 2804.13 L_divergence 3.26\n",
      "epoch 103: L_tot 2805.61 L_likelihood 2802.86 L_divergence 2.75\n",
      "epoch 104: L_tot 2761.34 L_likelihood 2758.46 L_divergence 2.88\n",
      "epoch 105: L_tot 2793.41 L_likelihood 2790.85 L_divergence 2.56\n",
      "epoch 106: L_tot 2781.36 L_likelihood 2778.73 L_divergence 2.63\n",
      "epoch 107: L_tot 2811.56 L_likelihood 2808.29 L_divergence 3.27\n",
      "epoch 108: L_tot 2792.17 L_likelihood 2789.57 L_divergence 2.61\n",
      "epoch 109: L_tot 2780.82 L_likelihood 2777.76 L_divergence 3.05\n",
      "epoch 110: L_tot 2749.94 L_likelihood 2747.18 L_divergence 2.76\n",
      "epoch 111: L_tot 2794.55 L_likelihood 2791.09 L_divergence 3.46\n",
      "epoch 112: L_tot 2791.26 L_likelihood 2788.57 L_divergence 2.68\n",
      "epoch 113: L_tot 2773.28 L_likelihood 2770.55 L_divergence 2.73\n",
      "epoch 114: L_tot 2765.53 L_likelihood 2762.65 L_divergence 2.88\n",
      "epoch 115: L_tot 2786.53 L_likelihood 2784.09 L_divergence 2.44\n",
      "epoch 116: L_tot 2788.36 L_likelihood 2785.95 L_divergence 2.40\n",
      "epoch 117: L_tot 2766.29 L_likelihood 2763.57 L_divergence 2.71\n",
      "epoch 118: L_tot 2804.08 L_likelihood 2800.94 L_divergence 3.14\n",
      "epoch 119: L_tot 2777.43 L_likelihood 2774.82 L_divergence 2.61\n",
      "epoch 120: L_tot 2799.21 L_likelihood 2796.59 L_divergence 2.63\n",
      "epoch 121: L_tot 2785.29 L_likelihood 2782.86 L_divergence 2.42\n",
      "epoch 122: L_tot 2816.68 L_likelihood 2813.33 L_divergence 3.35\n",
      "epoch 123: L_tot 2759.52 L_likelihood 2756.76 L_divergence 2.76\n",
      "epoch 124: L_tot 2791.82 L_likelihood 2789.24 L_divergence 2.57\n",
      "epoch 125: L_tot 2788.10 L_likelihood 2785.27 L_divergence 2.82\n",
      "epoch 126: L_tot 2773.89 L_likelihood 2770.55 L_divergence 3.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 127: L_tot 2775.62 L_likelihood 2772.52 L_divergence 3.10\n",
      "epoch 128: L_tot 2785.39 L_likelihood 2782.66 L_divergence 2.73\n",
      "epoch 129: L_tot 2806.27 L_likelihood 2803.49 L_divergence 2.77\n",
      "epoch 130: L_tot 2773.92 L_likelihood 2771.10 L_divergence 2.82\n",
      "epoch 131: L_tot 2779.49 L_likelihood 2776.31 L_divergence 3.17\n",
      "epoch 132: L_tot 2779.16 L_likelihood 2776.56 L_divergence 2.60\n",
      "epoch 133: L_tot 2789.24 L_likelihood 2786.45 L_divergence 2.79\n",
      "epoch 134: L_tot 2770.86 L_likelihood 2767.98 L_divergence 2.88\n",
      "epoch 135: L_tot 2810.81 L_likelihood 2807.66 L_divergence 3.15\n",
      "epoch 136: L_tot 2775.79 L_likelihood 2772.60 L_divergence 3.19\n",
      "epoch 137: L_tot 2834.87 L_likelihood 2832.37 L_divergence 2.49\n",
      "epoch 138: L_tot 2790.18 L_likelihood 2786.90 L_divergence 3.28\n",
      "epoch 139: L_tot 2771.66 L_likelihood 2768.50 L_divergence 3.15\n",
      "epoch 140: L_tot 2771.07 L_likelihood 2768.47 L_divergence 2.61\n",
      "epoch 141: L_tot 2823.68 L_likelihood 2820.38 L_divergence 3.30\n",
      "epoch 142: L_tot 2792.49 L_likelihood 2789.20 L_divergence 3.29\n",
      "epoch 143: L_tot 2797.65 L_likelihood 2794.48 L_divergence 3.17\n",
      "epoch 144: L_tot 2763.34 L_likelihood 2760.64 L_divergence 2.70\n",
      "epoch 145: L_tot 2775.47 L_likelihood 2772.87 L_divergence 2.60\n",
      "epoch 146: L_tot 2803.40 L_likelihood 2800.46 L_divergence 2.94\n",
      "epoch 147: L_tot 2786.27 L_likelihood 2783.78 L_divergence 2.48\n",
      "epoch 148: L_tot 2788.87 L_likelihood 2785.89 L_divergence 2.97\n",
      "epoch 149: L_tot 2784.29 L_likelihood 2781.43 L_divergence 2.85\n",
      "epoch 150: L_tot 2786.93 L_likelihood 2783.96 L_divergence 2.97\n",
      "epoch 151: L_tot 2784.32 L_likelihood 2781.36 L_divergence 2.96\n",
      "epoch 152: L_tot 2761.64 L_likelihood 2758.52 L_divergence 3.12\n",
      "epoch 153: L_tot 2796.11 L_likelihood 2792.91 L_divergence 3.20\n",
      "epoch 154: L_tot 2815.01 L_likelihood 2811.76 L_divergence 3.25\n",
      "epoch 155: L_tot 2775.72 L_likelihood 2772.71 L_divergence 3.01\n",
      "epoch 156: L_tot 2773.00 L_likelihood 2770.06 L_divergence 2.95\n",
      "epoch 157: L_tot 2765.03 L_likelihood 2762.54 L_divergence 2.48\n",
      "epoch 158: L_tot 2768.91 L_likelihood 2765.07 L_divergence 3.84\n",
      "epoch 159: L_tot 2805.72 L_likelihood 2802.86 L_divergence 2.85\n",
      "epoch 160: L_tot 2767.54 L_likelihood 2764.47 L_divergence 3.07\n",
      "epoch 161: L_tot 2777.87 L_likelihood 2774.39 L_divergence 3.48\n",
      "epoch 162: L_tot 2777.66 L_likelihood 2774.08 L_divergence 3.58\n",
      "epoch 163: L_tot 2791.03 L_likelihood 2788.39 L_divergence 2.64\n",
      "epoch 164: L_tot 2778.12 L_likelihood 2775.30 L_divergence 2.82\n",
      "epoch 165: L_tot 2796.06 L_likelihood 2792.94 L_divergence 3.12\n",
      "epoch 166: L_tot 2766.42 L_likelihood 2763.31 L_divergence 3.11\n",
      "epoch 167: L_tot 2774.46 L_likelihood 2771.13 L_divergence 3.33\n",
      "epoch 168: L_tot 2808.59 L_likelihood 2805.99 L_divergence 2.60\n",
      "epoch 169: L_tot 2777.22 L_likelihood 2774.54 L_divergence 2.68\n",
      "epoch 170: L_tot 2766.55 L_likelihood 2764.00 L_divergence 2.55\n",
      "epoch 171: L_tot 2793.67 L_likelihood 2790.44 L_divergence 3.24\n",
      "epoch 172: L_tot 2779.57 L_likelihood 2776.27 L_divergence 3.30\n",
      "epoch 173: L_tot 2835.81 L_likelihood 2832.35 L_divergence 3.46\n",
      "epoch 174: L_tot 2773.03 L_likelihood 2770.09 L_divergence 2.94\n",
      "epoch 175: L_tot 2800.45 L_likelihood 2797.75 L_divergence 2.69\n",
      "epoch 176: L_tot 2801.55 L_likelihood 2798.31 L_divergence 3.24\n",
      "epoch 177: L_tot 2794.52 L_likelihood 2791.28 L_divergence 3.25\n",
      "epoch 178: L_tot 2786.49 L_likelihood 2783.46 L_divergence 3.04\n",
      "epoch 179: L_tot 2803.72 L_likelihood 2800.60 L_divergence 3.12\n",
      "epoch 180: L_tot 2790.30 L_likelihood 2787.20 L_divergence 3.11\n",
      "epoch 181: L_tot 2767.10 L_likelihood 2763.99 L_divergence 3.10\n",
      "epoch 182: L_tot 2800.29 L_likelihood 2797.17 L_divergence 3.13\n",
      "epoch 183: L_tot 2752.50 L_likelihood 2749.83 L_divergence 2.67\n",
      "epoch 184: L_tot 2798.01 L_likelihood 2794.79 L_divergence 3.22\n",
      "epoch 185: L_tot 2784.47 L_likelihood 2781.27 L_divergence 3.20\n",
      "epoch 186: L_tot 2788.24 L_likelihood 2785.20 L_divergence 3.04\n",
      "epoch 187: L_tot 2803.59 L_likelihood 2800.30 L_divergence 3.30\n",
      "epoch 188: L_tot 2793.90 L_likelihood 2790.68 L_divergence 3.23\n",
      "epoch 189: L_tot 2804.30 L_likelihood 2801.13 L_divergence 3.17\n",
      "epoch 190: L_tot 2783.47 L_likelihood 2780.39 L_divergence 3.08\n",
      "epoch 191: L_tot 2807.48 L_likelihood 2804.56 L_divergence 2.92\n",
      "epoch 192: L_tot 2786.92 L_likelihood 2783.99 L_divergence 2.93\n",
      "epoch 193: L_tot 2777.36 L_likelihood 2774.18 L_divergence 3.18\n",
      "epoch 194: L_tot 2755.25 L_likelihood 2752.58 L_divergence 2.67\n",
      "epoch 195: L_tot 2790.86 L_likelihood 2787.95 L_divergence 2.91\n",
      "epoch 196: L_tot 2764.45 L_likelihood 2761.64 L_divergence 2.81\n",
      "epoch 197: L_tot 2763.96 L_likelihood 2760.27 L_divergence 3.69\n",
      "epoch 198: L_tot 2791.98 L_likelihood 2788.83 L_divergence 3.14\n",
      "epoch 199: L_tot 2817.64 L_likelihood 2814.55 L_divergence 3.08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    learning_rate= 0.0005\n",
    "    dense_layers = [1500,700,300]\n",
    "    dense_funcs = ['elu','elu','tanh']\n",
    "    dim_z = 2#20 # latent vector size\n",
    "    dim_out = np_train.shape[1]\n",
    "    \n",
    "    \n",
    "    autoencoder = vae(sess,learning_rate,dense_layers,dense_funcs,dim_z,dim_out)\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={autoencoder.keep_prob : 0.9})\n",
    "    \n",
    "    # train\n",
    "    n_epochs = 200\n",
    "    batch_size = 40\n",
    "    n_samples = np_train.shape[0]\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    ADD_NOISE = True\n",
    "    train_total_data = np_train\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Random shuffling\n",
    "        np.random.shuffle(train_total_data)\n",
    "        train_data_ = train_total_data\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            offset = (i * batch_size) % (n_samples)\n",
    "            batch_xs_input = train_data_[offset:(offset + batch_size), :]\n",
    "\n",
    "            batch_xs_target = batch_xs_input\n",
    "            \n",
    "            \n",
    "            # add salt & pepper noise\n",
    "            if ADD_NOISE:\n",
    "                batch_xs_input = batch_xs_input * np.random.randint(2, size=batch_xs_input.shape)\n",
    "                batch_xs_input += np.random.randint(2, size=batch_xs_input.shape)\n",
    "            \n",
    "            _, tot_loss, loss_likelihood, loss_divergence = sess.run(\n",
    "                (autoencoder.train_op, autoencoder.loss, autoencoder.neg_marginal_likelihood, autoencoder.KL_divergence),\n",
    "                feed_dict={autoencoder.x_hat: batch_xs_input, autoencoder.x: batch_xs_target, autoencoder.keep_prob : 0.9})\n",
    "            \n",
    "            \n",
    "#         print(\"epoch {}\".format(epoch))\n",
    "        # print cost every epoch\n",
    "        print(\"epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f\" % (epoch, tot_loss, loss_likelihood, loss_divergence))\n",
    "\n",
    "        \n",
    "    autoencoder.save(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def save_scattered_image(z, id, name='scattered_image.jpg'):\n",
    "    N = 2\n",
    "    z_range = 3\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(z[:, 0], z[:, 1], c=np.argmax(id, 1), marker='o', edgecolor='none', cmap=discrete_cmap(N, 'jet'))\n",
    "    plt.colorbar(ticks=range(N))\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-z_range-2, z_range+2])\n",
    "    axes.set_ylim([-z_range-2, z_range+2])\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"./results/time/\" + name)\n",
    "\n",
    "\n",
    "def discrete_cmap(N, base_cmap=None):\n",
    "\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_dense_layers [1500, 700, 300, 2]\n",
      "gaussian encoder mean:(?, 2) \n",
      "mu:(?, 2) sigma:(?, 2)\n",
      "encoder output z:(?, 2)\n",
      "decoder_dense_layers [300, 700, 1500, 4323]\n",
      "decoder output y:(?, 4323)\n",
      "INFO:tensorflow:Restoring parameters from ./model/vae.model-1\n",
      " [*] Success to read vae.model-1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    learning_rate= 0.0005\n",
    "    dense_layers = [1500,700,300]\n",
    "    dense_funcs = ['elu','elu','tanh']\n",
    "    dim_z = 2#20 # latent vector size\n",
    "    dim_out = np_train.shape[1]\n",
    "    \n",
    "    \n",
    "    autoencoder = vae(sess,learning_rate,dense_layers,dense_funcs,dim_z,dim_out)\n",
    "    autoencoder.load()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={autoencoder.keep_prob : 0.9})\n",
    "    \n",
    "    PMLR = True\n",
    "    ADD_NOISE = True\n",
    "    if PMLR and dim_z == 2:\n",
    "        \n",
    "        # Plot for manifold learning result\n",
    "#         PMLR = plot_utils.Plot_Manifold_Learning_Result('./results/time', PMLR_n_img_x, PMLR_n_img_y, IMAGE_SIZE_MNIST, IMAGE_SIZE_MNIST, PMLR_resize_factor, PMLR_z_range)\n",
    "\n",
    "#         x_PMLR = np_train\n",
    "#         id_PMLR = df_train['id'].values\n",
    "#         nb_classes = 4\n",
    "#         targets = id_PMLR\n",
    "#         one_hot_targets = np.eye(nb_classes)[targets]\n",
    "        x_PMLR = np_new\n",
    "        \n",
    "        if ADD_NOISE:\n",
    "            x_PMLR = x_PMLR * np.random.randint(2, size=x_PMLR.shape)\n",
    "            x_PMLR += np.random.randint(2, size=x_PMLR.shape)\n",
    "\n",
    "        z_PMLR = sess.run(autoencoder.z, feed_dict={autoencoder.x_hat: x_PMLR, autoencoder.keep_prob : 1})\n",
    "        save_scattered_image(z_PMLR,one_hot_targets, name=\"/PMLR_map_stock.jpg\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index()\n",
    "df_train.head()\n",
    "id1 = df_train[df_train['id']==1].index\n",
    "id1 = id1.values\n",
    "id3 = df_train[df_train['id']==3].index\n",
    "id3 = id3.values\n",
    "# np_id1 = np_train[id1]\n",
    "# np_id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 4323)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id1 = id1.values\n",
    "# type(id1)\n",
    "np_new = np.concatenate((np_train[id1],np_train[id3]),axis=0)\n",
    "np_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id1_list = np.zeros(len(id1)).astype(int)\n",
    "id3_list = np.ones(len(id3)).astype(int)\n",
    "id_list = np.concatenate((id1_list,id3_list),axis=0)\n",
    "id_list\n",
    "nb_classes = 2\n",
    "targets = id_list\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "# one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_PMLR = df_train['id'].values\n",
    "nb_classes = 2\n",
    "targets = id_PMLR\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 4\n",
    "z_range = 3\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=np.argmax(id, 1), marker='o', edgecolor='none', cmap=discrete_cmap(N, 'jet'))\n",
    "plt.colorbar(ticks=range(N))\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-z_range-2, z_range+2])\n",
    "axes.set_ylim([-z_range-2, z_range+2])\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
