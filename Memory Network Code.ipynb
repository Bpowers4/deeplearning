{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Lambda, Reshape, add, dot, Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils.data_utils import get_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single supporting facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data and open the compressed file using the tarfile library\n",
    "# https://research.fb.com/downloads/babi/\n",
    "path = get_file(\n",
    "  'babi-tasks-v1-2.tar.gz', cache_subdir = \"datasets\",cache_dir='./',\n",
    "  origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "tar = tarfile.open(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant data in the tar file\n",
    "# there's lots more data in there, check it out if you want!\n",
    "challenges = {\n",
    "  # QA1 with 10,000 samples\n",
    "  'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "  # QA2 with 10,000 samples\n",
    "  'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenges['single_supporting_fact_10k'].format('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " \n",
      "am\n",
      " \n",
      "rich\n",
      "!!? ? (\n",
      "why\n",
      "?)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home9second/.virtualenvs/deeplearning/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "sent = 'I am rich!!? ? (why?)'\n",
    "for x in re.split('(\\W+)?', sent):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stories(f):\n",
    "    # data will return a list of triples\n",
    "    # each triple contains:\n",
    "    #   1. a story\n",
    "    #   2. a question about the story\n",
    "    #   3. the answer to the question\n",
    "    data = []\n",
    "\n",
    "    # use this list to keep track of the story so far\n",
    "    story = []\n",
    "    \n",
    "    count = 0\n",
    "    # print a random story, helpful to see the data\n",
    "    printed = False\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8').strip()\n",
    "\n",
    "        # split the line number from the rest of the line\n",
    "        nid, line = line.split(' ', 1)\n",
    "\n",
    "        # see if we should begin a new story\n",
    "        if int(nid) == 1:\n",
    "            story = []\n",
    "\n",
    "        # this line contains a question and answer if it has a tab\n",
    "        #       question<TAB>answer\n",
    "        # it also tells us which line in the story is relevant to the answer\n",
    "        # Note: we actually ignore this fact, since the model will learn\n",
    "        #       which lines are important\n",
    "        # Note: the max line number is not the number of lines of the story\n",
    "        #       since lines with questions do not contain any story\n",
    "        # one story may contain MULTIPLE questions\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "\n",
    "            # numbering each line is very useful\n",
    "            # it's the equivalent of adding a unique token to the front\n",
    "            # of each sentence\n",
    "            story_so_far = [[str(i)] + s for i, s in enumerate(story) if s]\n",
    "\n",
    "            # uncomment if you want to see what a story looks like\n",
    "            # if not printed and np.random.rand() < 0.5:\n",
    "            #     print(\"story_so_far:\", story_so_far)\n",
    "            #     printed = True\n",
    "            data.append((story_so_far, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # just add the line to the current story\n",
    "            story.append(tokenize(line))\n",
    "            \n",
    "        count += 1\n",
    "        if count > 2:\n",
    "            break\n",
    "            \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = challenges['single_supporting_fact_10k']\n",
    "trainfile = tar.extractfile(challenge.format('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mary moved to the bathroom.\n",
      "2 John went to the hallway.\n",
      "3 Where is Mary? \tbathroom\t1\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "story = []\n",
    "printed = False\n",
    "count = 0\n",
    "for line in trainfile:\n",
    "    line = line.decode('utf-8').strip()\n",
    "    print(line)\n",
    "\n",
    "    nid, line = line.split(' ', 1)\n",
    "    if int(nid) == 1:\n",
    "        story = []\n",
    "    if '\\t' in line:\n",
    "        q, a, supporting = line.split('\\t')\n",
    "        q = tokenize(q)\n",
    "\n",
    "        story_so_far = [[str(i)] + s for i, s in enumerate(story) if s]\n",
    "        data.append((story_so_far, q, a))\n",
    "        story.append('')\n",
    "    else:\n",
    "\n",
    "        story.append(tokenize(line))\n",
    "\n",
    "    count += 1\n",
    "    if count > 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['0', 'Mary', 'moved', 'to', 'the', 'bathroom', '.'],\n",
       "   ['1', 'John', 'went', 'to', 'the', 'hallway', '.']],\n",
       "  ['Where', 'is', 'Mary', '?'],\n",
       "  'bathroom')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mary', 'moved', 'to', 'the', 'bathroom', '.'],\n",
       " ['John', 'went', 'to', 'the', 'hallway', '.'],\n",
       " '']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursively flatten a list\n",
    "def should_flatten(el): \n",
    "    return not isinstance(el, (str, bytes))\n",
    "\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if should_flatten(el):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "# test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "stories = train_stories\n",
    "\n",
    "vocab = sorted(set(flatten(stories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '0',\n",
       " '1',\n",
       " '?',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Where',\n",
       " 'bathroom',\n",
       " 'hallway',\n",
       " 'is',\n",
       " 'moved',\n",
       " 'the',\n",
       " 'to',\n",
       " 'went']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert stories from words into lists of word indexes (integers)\n",
    "# pad each sequence so that they are the same length\n",
    "# we will need to re-pad the stories later so that each story\n",
    "# is the same length\n",
    "def vectorize_stories(data, word2idx, story_maxlen, query_maxlen):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([[word2idx[w] for w in s] for s in story])\n",
    "        queries.append([word2idx[w] for w in query])\n",
    "        answers.append([word2idx[answer]])\n",
    "    return (\n",
    "        [pad_sequences(x, maxlen=story_maxlen) for x in inputs],\n",
    "         pad_sequences(queries, maxlen=query_maxlen),\n",
    "         np.array(answers)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is like 'pad_sequences' but for entire stories\n",
    "# we are padding each story with zeros so every story\n",
    "# has the same number of sentences\n",
    "# append an array of zeros of size:\n",
    "# (max_sentences - num sentences in story, max words in sentence)\n",
    "def stack_inputs(inputs, story_maxsents, story_maxlen):\n",
    "    for i, story in enumerate(inputs):\n",
    "        inputs[i] = np.concatenate(\n",
    "          [\n",
    "            story, \n",
    "            np.zeros((story_maxsents - story.shape[0], story_maxlen), 'int')\n",
    "          ]\n",
    "        )\n",
    "    return np.stack(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can get the max length of each story, of each sentence, and of each question\n",
    "story_maxlen = max((len(s) for x, _, _ in stories for s in x))\n",
    "story_maxsents = max((len(x) for x, _, _ in stories))\n",
    "query_maxlen = max(len(x) for _, x, _ in stories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'Mary', 'moved', 'to', 'the', 'bathroom', '.'] 7\n",
      "['1', 'John', 'went', 'to', 'the', 'hallway', '.'] 7\n"
     ]
    }
   ],
   "source": [
    "for x,_,_ in stories:\n",
    "    for s in x:\n",
    "        print(s,len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary of corpus and find size, including a padding element.\n",
    "vocab = sorted(set(flatten(stories)))\n",
    "vocab.insert(0, '<PAD>')\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create an index mapping for the vocabulary.\n",
    "word2idx = {c:i for i, c in enumerate(vocab)}\n",
    "\n",
    "# convert stories from strings to lists of integers\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(\n",
    "train_stories, \n",
    "word2idx,\n",
    "story_maxlen,\n",
    "query_maxlen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2,  6, 11, 13, 12,  8,  1],\n",
       "        [ 3,  5, 14, 13, 12,  9,  1]], dtype=int32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[0].shape#,queries_train, answers_train\n",
    "inputs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_train.shape,  (1, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "# convert inputs into 3-D numpy arrays\n",
    "inputs_train = stack_inputs(inputs_train, story_maxsents, story_maxlen)\n",
    "print(\"inputs_train.shape, \", inputs_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 2  6 11 13 12  8  1]\n",
      " [ 3  5 14 13 12  9  1]]\n"
     ]
    }
   ],
   "source": [
    "for i, story in enumerate(inputs_train):\n",
    "    print(i,story)\n",
    "    inputs_train[i] = np.concatenate(\n",
    "      [\n",
    "        story, \n",
    "        np.zeros((story_maxsents - story.shape[0], story_maxlen), 'int')\n",
    "      ]\n",
    "    )\n",
    "inputs_train = np.stack(inputs_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2,  6, 11, 13, 12,  8,  1],\n",
       "        [ 3,  5, 14, 13, 12,  9,  1]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_maxsents,story.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 7), dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.zeros((story_maxsents - story.shape[0], story_maxlen), 'int')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6]])\n",
    "c = np.concatenate([a, b])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to get the data since\n",
    "# we want to load both the single supporting fact data\n",
    "# and the two supporting fact data later\n",
    "def get_data(challenge_type):\n",
    "    # input should either be 'single_supporting_fact_10k' or 'two_supporting_facts_10k'\n",
    "    challenge = challenges[challenge_type]\n",
    "\n",
    "\n",
    "    # returns a list of triples of:\n",
    "    # (story, question, answer)\n",
    "    # story is a list of sentences\n",
    "    # question is a sentence\n",
    "    # answer is a word\n",
    "    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "\n",
    "    # group all the stories together\n",
    "    stories = train_stories + test_stories\n",
    "\n",
    "    # so we can get the max length of each story, of each sentence, and of each question\n",
    "    story_maxlen = max((len(s) for x, _, _ in stories for s in x))\n",
    "    story_maxsents = max((len(x) for x, _, _ in stories))\n",
    "    query_maxlen = max(len(x) for _, x, _ in stories)\n",
    "\n",
    "    # Create vocabulary of corpus and find size, including a padding element.\n",
    "    vocab = sorted(set(flatten(stories)))\n",
    "    vocab.insert(0, '<PAD>')\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create an index mapping for the vocabulary.\n",
    "    word2idx = {c:i for i, c in enumerate(vocab)}\n",
    "\n",
    "    # convert stories from strings to lists of integers\n",
    "    inputs_train, queries_train, answers_train = vectorize_stories(\n",
    "                                                    train_stories, \n",
    "                                                    word2idx,\n",
    "                                                    story_maxlen,\n",
    "                                                    query_maxlen\n",
    "                                                    )\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(\n",
    "                                                    test_stories, \n",
    "                                                    word2idx,\n",
    "                                                    story_maxlen,\n",
    "                                                    query_maxlen\n",
    "                                                    )\n",
    "\n",
    "    # convert inputs into 3-D numpy arrays\n",
    "    inputs_train = stack_inputs(inputs_train, story_maxsents, story_maxlen)\n",
    "    inputs_test = stack_inputs(inputs_test, story_maxsents, story_maxlen)\n",
    "    print(\"inputs_train.shape, inputs_test.shape\", inputs_train.shape, inputs_test.shape)\n",
    "\n",
    "\n",
    "    # return model inputs for keras\n",
    "    return train_stories, test_stories, \\\n",
    "            inputs_train, queries_train, answers_train, \\\n",
    "            inputs_test, queries_test, answers_test, \\\n",
    "            story_maxsents, story_maxlen, query_maxlen, \\\n",
    "            vocab, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_train.shape, inputs_test.shape (1, 2, 7) (1, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "# get the single supporting fact data\n",
    "train_stories, test_stories, \\\n",
    "  inputs_train, queries_train, answers_train, \\\n",
    "  inputs_test, queries_test, answers_test, \\\n",
    "  story_maxsents, story_maxlen, query_maxlen, \\\n",
    "  vocab, vocab_size = get_data('single_supporting_fact_10k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_story_.shape,_embedded_story.shape:, embedded_story.shape: (?, 2, 7) (?, 2, 7, 15) (?, 2, 7, 15)\n",
      "inp_q.shape, _embedded_question.shape:, emb_q.shape: (?, 4) (?, 4, 15) (?, 1, 15)\n"
     ]
    }
   ],
   "source": [
    "##### create the model #####\n",
    "embedding_dim = 15\n",
    "\n",
    "\n",
    "# turn the story into a sequence of embedding vectors\n",
    "# one for each story line\n",
    "# treating each story line like a \"bag of words\"\n",
    "input_story_ = Input((story_maxsents, story_maxlen))\n",
    "embedded_story = Embedding(vocab_size, embedding_dim)(input_story_)\n",
    "_embedded_story = embedded_story\n",
    "# embedded_story = Lambda(lambda x: K.sum(x, axis=2))(embedded_story)\n",
    "print(\"input_story_.shape,_embedded_story.shape:, embedded_story.shape:\", input_story_.shape,_embedded_story.shape, embedded_story.shape)\n",
    "\n",
    "\n",
    "# turn the question into an embedding\n",
    "# also a bag of words\n",
    "input_question_ = Input((query_maxlen,))\n",
    "embedded_question = Embedding(vocab_size, embedding_dim)(input_question_)\n",
    "_embedded_question = embedded_question\n",
    "embedded_question = Lambda(lambda x: K.sum(x, axis=1))(embedded_question)\n",
    "\n",
    "# add a \"sequence length\" of 1 so that it can\n",
    "# be dotted with the story later\n",
    "embedded_question = Reshape((1, embedding_dim))(embedded_question)\n",
    "print(\"inp_q.shape, _embedded_question.shape:, emb_q.shape:\", input_question_.shape,_embedded_question.shape, embedded_question.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story_weights.shape: (?, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "# calculate the weights for each story line\n",
    "# embedded_story.shape        = (N, num sentences, embedding_dim)\n",
    "# embedded_question.shape     = (N, 1, embedding_dim)\n",
    "x = dot([embedded_story, embedded_question], 2)\n",
    "x = Reshape((story_maxsents,))(x) # flatten the vector\n",
    "x = Activation('softmax')(x)\n",
    "story_weights = Reshape((story_maxsents, 1))(x) # unflatten it again to be dotted later\n",
    "print(\"story_weights.shape:\", story_weights.shape)\n",
    "\n",
    "\n",
    "\n",
    "x = dot([story_weights, embedded_story], 1)\n",
    "x = Reshape((embedding_dim,))(x) # flatten it again\n",
    "ans = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "# make the model\n",
    "model = Model([input_story_, input_question_], ans)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "  optimizer=RMSprop(lr=1e-2),\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "r = model.fit(\n",
    "  [inputs_train, queries_train],\n",
    "  answers_train,\n",
    "  epochs=4,\n",
    "  batch_size=32,\n",
    "  validation_data=([inputs_test, queries_test], answers_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check how we weight each input sentence given a story and question\n",
    "debug_model = Model([input_story_, input_question_], story_weights)\n",
    "\n",
    "# choose a random story\n",
    "story_idx = np.random.choice(len(train_stories))\n",
    "\n",
    "# get weights from debug model\n",
    "i = inputs_train[story_idx:story_idx+1]\n",
    "q = queries_train[story_idx:story_idx+1]\n",
    "w = debug_model.predict([i, q]).flatten()\n",
    "\n",
    "story, question, ans = train_stories[story_idx]\n",
    "print(\"story:\\n\")\n",
    "for i, line in enumerate(story):\n",
    "    print(\"{:1.5f}\".format(w[i]), \"\\t\", \" \".join(line))\n",
    "\n",
    "print(\"question:\", \" \".join(question))\n",
    "print(\"answer:\", ans)\n",
    "\n",
    "\n",
    "# pause so we can see the output\n",
    "input(\"Hit enter to continue\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two supporting facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get the two supporting fact data\n",
    "train_stories, test_stories, \\\n",
    "  inputs_train, queries_train, answers_train, \\\n",
    "  inputs_test, queries_test, answers_test, \\\n",
    "  story_maxsents, story_maxlen, query_maxlen, \\\n",
    "  vocab, vocab_size = get_data('two_supporting_facts_10k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### create the model #####\n",
    "embedding_dim = 30\n",
    "\n",
    "\n",
    "# make a function for this so we can use it again\n",
    "def embed_and_sum(x, axis=2):\n",
    "    x = Embedding(vocab_size, embedding_dim)(x)\n",
    "    x = Lambda(lambda x: K.sum(x, axis))(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the inputs\n",
    "input_story_ = Input((story_maxsents, story_maxlen))\n",
    "input_question_ = Input((query_maxlen,))\n",
    "\n",
    "\n",
    "# embed the inputs\n",
    "embedded_story = embed_and_sum(input_story_)\n",
    "embedded_question = embed_and_sum(input_question_, 1)\n",
    "\n",
    "\n",
    "# final dense will be used in each hop\n",
    "dense_layer = Dense(embedding_dim, activation='elu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define one hop\n",
    "# the \"query\" can be the question, or the answer from the previous hop\n",
    "def hop(query, story):\n",
    "    # query.shape = (embedding_dim,)\n",
    "    # story.shape = (num sentences, embedding_dim)\n",
    "    x = Reshape((1, embedding_dim))(query) # make it (1, embedding_dim)\n",
    "    x = dot([story, x], 2)\n",
    "    x = Reshape((story_maxsents,))(x) # flatten it for softmax\n",
    "    x = Activation('softmax')(x)\n",
    "    story_weights = Reshape((story_maxsents, 1))(x) # unflatten for dotting\n",
    "\n",
    "    # makes a new embedding\n",
    "    story_embedding2 = embed_and_sum(input_story_)\n",
    "    x = dot([story_weights, story_embedding2], 1)\n",
    "    x = Reshape((embedding_dim,))(x)\n",
    "    x = dense_layer(x)\n",
    "    return x, story_embedding2, story_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the hops\n",
    "ans1, embedded_story, story_weights1 = hop(embedded_question, embedded_story)\n",
    "ans2, _,              story_weights2 = hop(ans1,              embedded_story)\n",
    "\n",
    "# get the final answer\n",
    "ans = Dense(vocab_size, activation='softmax')(ans2)\n",
    "\n",
    "\n",
    "# build the model\n",
    "model2 = Model([input_story_, input_question_], ans)\n",
    "\n",
    "# compile the model\n",
    "model2.compile(\n",
    "  optimizer=RMSprop(lr=5e-3),\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "r = model2.fit(\n",
    "  [inputs_train, queries_train],\n",
    "  answers_train,\n",
    "  epochs=30,\n",
    "  batch_size=32,\n",
    "  validation_data=([inputs_test, queries_test], answers_test)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print story line weights again ###\n",
    "debug_model2 = Model(\n",
    "  [input_story_, input_question_],\n",
    "  [story_weights1, story_weights2]\n",
    ")\n",
    "\n",
    "# choose a random story\n",
    "story_idx = np.random.choice(len(train_stories))\n",
    "\n",
    "# get weights from debug model\n",
    "i = inputs_train[story_idx:story_idx+1]\n",
    "q = queries_train[story_idx:story_idx+1]\n",
    "w1, w2 = debug_model2.predict([i, q])\n",
    "w1 = w1.flatten()\n",
    "w2 = w2.flatten()\n",
    "\n",
    "story, question, ans = train_stories[story_idx]\n",
    "print(\"story:\\n\")\n",
    "for j, line in enumerate(story):\n",
    "    print(\"{:1.5f}\".format(w1[j]), \"\\t\", \"{:1.5f}\".format(w2[j]), \"\\t\", \" \".join(line))\n",
    "\n",
    "print(\"question:\", \" \".join(question))\n",
    "print(\"answer:\", ans)\n",
    "print(\"prediction:\", vocab[ np.argmax(model2.predict([i, q])[0]) ])\n",
    "\n",
    "\n",
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda, Embedding, merge, RepeatVector, LSTM\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "vocab_size = 2\n",
    "\n",
    "# Inputs and embeddings \n",
    "inp = Input((4,), dtype=\"int32\")\n",
    "inp2 = Input((4,), dtype=\"int32\")\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=3, name=\"word_embeddings\")(inp)\n",
    "embed2 = Embedding(input_dim=vocab_size, output_dim=3, name=\"word_embeddings\")(inp2)\n",
    "\n",
    "# My custom lambda for computing the sum of vectors the input sequence\n",
    "embedding_sum = Lambda(lambda x: K.sum(x, axis=1))(embed)\n",
    "# If we use this LSTM layer below instead of our lambda, everything works! \n",
    "#embedding_sum = LSTM(3, return_sequences=False)(embed)\n",
    "\n",
    "# Repeat the sum to merge it with the other sequence\n",
    "# embedding_sum_repeated = RepeatVector(4)(embedding_sum)\n",
    "\n",
    "# Do the element-wise merge\n",
    "# merged = merge([embed2, embedding_sum_repeated], mode=\"dot\")\n",
    "\n",
    "# Build model\n",
    "# model = Model(inp, embedding_sum)\n",
    "model = Model(inp, embed)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# run an example\n",
    "x = np.array([\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 1, 1, 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# model.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13362293, -0.10003681, -0.00127294],\n",
       "       [ 0.10825297, -0.14170618,  0.05893242]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)\n",
    "\n",
    "# embedding_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = model.predict(x)\n",
    "embed.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
